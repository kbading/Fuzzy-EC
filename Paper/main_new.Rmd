---
title             : "US identity and US valence memory in evaluative conditioning: A multinomial modeling approach"
shorttitle        : "Memory in evaluative conditioning"
author: 
    #   - "Conceptualization"
    #   - "Writing - Original Draft Preparation"
    #   - "Writing - Review & Editing"
    #   - "Writing - Review & Editing"
    #   - "Supervision"
    #   - "Writing - Review & Editing"
    #   - "Supervision"
  - name          : "Karoline Corinna Bading"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Schleichstraße 4, 72074 Tübingen (Germany)"
    email         : "karoline.bading@uni-tuebingen.de"
  - name          : "Jérémy Béna"
    affiliation   : "2"
    # role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
  - name          : "Marius Barth"
    affiliation   : "3"
    # role:
  - name          : "Klaus Rothermund"
    affiliation   : "4"
    # role:
      
affiliation:
    
  - id            : "1"
    institution   : "University of Tübingen"
  - id            : "2"
    institution   : "CRPN, Aix-Marseille Université, CNRS, Marseille, France"
  - id            : "3"
    institution   : "University of Cologne"
  - id            : "4"
    institution   : "Friedrich Schiller University Jena"
abstract: |
  Evaluative Conditioning (EC) is a change in the evaluation of a conditioned stimulus (CS) after its pairing with a valent unconditioned stimulus (US). Accounts of EC typically assume some form of CS-US pairing memory and distinguish between US identity (memory for the specific US paired with a given CS) and US valence (memory for the valence of the US paired with a given CS) memory. Valid measures of memory involved in EC are therefore critical for advancing the understanding of EC. We present several shortcomings in common measures of CS-US pairing memory that hinder the testing of hypotheses about the role of US identity and US valence memory in EC. We then introduce a multinomial processing tree (MPT) model that solves these shortcomings. This model allows estimating CS recognition, US identity memory, US valence memory, and several guessing biases. We report three experiments showing that the proposed MPT model (1) fits the data well and (2) delivers insights into the relationships between EC, US valence memory, and US identity memory. **[JB: perhaps an additional sentence or two about more specific results and the overarching conclusion?]**
  
  
  <!-- https://tinyurl.com/ybremelq -->
authornote: |
  <!-- Karoline Bading and Jérémy Béna share first authorship. -->
  
keywords          : "keywords"
wordcount         : "X"
bibliography      : '`r file.path(rprojroot::find_rstudio_root_file(), "..", "methexp.bib")`'
floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : jou
output            : papaja::apa6_pdf
header-includes:
  - \usepackage{nicefrac}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
library(afex)
library(emmeans)
#library(ggeffects)
library(papaja)
library(dplyr)
library(MPTinR)
library(TreeBUGS)
library(HMMTreeC)

if(!requireNamespace("magick", quietly = FALSE)) install.packages("magick")
set_sum_contrasts()

#r_refs("r-references.bib")

project_root <- rprojroot::find_rstudio_root_file()

study_folder_pilot <- file.path(
  project_root
  , "pilot_data_analyses"
)

study_folder_main <- file.path(
  project_root
  , "Study 2"
)

study_folders <- list(
  paper = file.path(project_root, "Paper")
  , wsw1 = file.path(project_root, "studies", "wsw1")
  , wsw2_main = file.path(project_root, "studies", "wsw2-main")
  , wsw3_main = file.path(project_root, "studies", "wsw3-main")
  , wsw3_p2   = file.path(project_root, "studies", "wsw3-p2")
  , wsw3_joint  = file.path(project_root, "studies", "wsw3-joint-analysis")
)

source(file.path(project_root, "R", "apa_print_treebugs.R"))
source(file.path(project_root, "R", "mptinr_helper.R"))
source(file.path(project_root, "R", "treestan_helper.R"))

knitr::opts_chunk$set(
  echo = FALSE
  , warning = FALSE
  , message = FALSE
  , cache = FALSE
  , fig.env = "figure*"
  , fig.crop = TRUE
  # , fig.width  = 15
  # , fig.height =  9
)
knitr::opts_knit$set(global.par = TRUE)

cntr <- function(x, ...){x - mean(x, ...)}
```

```{r global-par}
par(las = 1, font.main = 1, cex.main = 1.1)
palette(wesanderson::wes_palette("Zissou1", n = 3, type = "c"))
```

```{r analysis-preferences}
# Seed for random number generation
# set.seed(42)
# knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Attitudes and evaluations are rarely arbitrary. Rather, they are often grounded in past experiences with the objects they concern. A food that once caused discomfort in a particular situation may later be disliked and avoided, whereas a person who was helpful in a specific encounter may be met with spontaneous liking years later. For such effects to persist beyond the original learning episode, past experiences must be represented in long-term memory, either in a detailed or in a more abstract form.
For example, later dislike for the food may be supported by relatively detailed memory for the original episode, such as recalling when and where the food was eaten and the symptoms that followed. By contrast, spontaneous liking of the person may rest on a more abstract representation, such as remembering that a positive interaction took place without retaining its specific details.

Importantly, the specificity of the underlying memory representations may have systematic consequences for key characteristics of the resulting attitudes. For example, evaluations grounded in relatively specific memories may be stronger, as access to concrete details may increase confidence in the resulting judgment. By contrast, evaluations based on more abstract representations may be more durable, as they do not depend on the continued accessibility of particular details. Finally, the specificity of the memory representation may determine how susceptible an attitude is to later intervention. For instance, new information about the original experience can only revise an evaluation when that experience is represented with sufficient detail to be updated or reinterpreted, whereas abstract representations may be comparatively resistant to such input.

Given the potential consequences of representational format for attitude strength, persistence, and resistance to change, it is not surprising that the nature of the underlying memory representations has long been of central interest in social and cognitive psychology. In both fields, questions about attitude formation and its relation to memory are commonly investigated using the evaluative conditioning (EC) paradigm. EC refers to a change in the evaluation of an initially neutral conditioned stimulus (CS) following its pairing with a positive or negative unconditioned stimulus (US). In a typical EC procedure, CSs (e.g., unfamiliar brand logos) are repeatedly paired with positive or negative USs (e.g., images of happy or sad faces) during a learning phase. After learning, the CSs are presented in the absence of the USs, and participants are asked to evaluate them. Reliable differences in evaluation are typically observed, such that CSs previously paired with positive USs are evaluated more positively than CSs paired with negative USs, indicating that evaluative information acquired during learning continues to influence judgments at test.

As a pairing-based paradigm with random assignment and tightly controlled stimulus exposure, EC allows precise control over both the content and the structure of learning experiences. This makes it a principled context for investigating the specificity of the memory representations underlying attitudes and evaluations. Specifically, evaluations of CSs may be supported either by relatively detailed memory for the particular USs with which they were paired, or by simpler representations that encode explicit knowledge of the valence of the previously paired USs, without retaining information about their specific identities. In EC research, these possibilities are reflected in different cued recall measures, which target either memory for US identity or memory for US valence. By combining direct and indirect measures of evaluative outcomes with these memory measures, the EC paradigm has become a powerful tool for investigating when and why attitudes are supported by more versus less specific forms of memory.

## The role of US identity and US valence memory in evaluative conditioning
Early EC research was strongly influenced by stimulus--stimulus (S--S) learning models, which assume that CSs become linked to representations of the specific USs with which they are paired. According to this referential view, changes in CS evaluations are mediated by memory for US identity, such that encountering a CS activates a representation of the associated US and, in turn, its evaluative properties.

A central source of evidence supporting this position came from studies employing US revaluation procedures. In a seminal contribution, Baeyens et al. (1992) demonstrated that post-conditioning changes in the valence of a US systematically altered evaluations of previously paired CSs, even when the CSs themselves were not re-presented during US revaluation. Such findings were taken to indicate that the evaluative meaning acquired by the CS depends on its reference to the US, consistent with S--S learning accounts. Importantly, these effects were shown to persist over time, further strengthening the case for US identity--based representations.

Subsequent research extended and refined these conclusions. For example, Walther et al. (2009) provided converging evidence for US revaluation effects using different evaluative measures and experimental controls, showing that changes in US valence were mirrored by corresponding changes in CS evaluations. Together, these findings reinforced the view that EC effects are sensitive to information about the US itself and supported the idea that memory for US identity plays a role in mediating evaluative learning.

Despite this evidence for US identity--based accounts, later research increasingly questioned whether memory for the specific US is in fact necessary for evaluative conditioning to occur. A key development in this regard was the proposal that EC effects may be supported by memory for the valence of the US, even when memory for its specific identity is weak or absent. From this perspective, it may be sufficient for individuals to remember that a CS was paired with something positive or negative, without being able to retrieve which particular US was involved. Importantly, this view does not deny that memory for US identity can be formed or used in EC. Rather, US revaluation effects may reflect situations in which identity-based representations are available and allow evaluations to be updated in light of new information about the US, without implying that such representations are the primary basis of EC effects.

Empirical support for this view was provided by a series of studies demonstrating that EC effects depend primarily on US valence memory rather than on US identity memory. In particular, in a seminal contribution, Stahl et al. (2009) showed that evaluative changes in CSs reliably emerged only when participants could report the valence of the associated US, whereas memory for US identity did not explain additional variance in EC effects beyond valence memory. This work has had a lasting influence on subsequent EC research, with later studies explicitly adopting US valence as the appropriate level for assessing contingency memory and awareness (e.g., Hütter et al., 2012).

## Does US valence memory exist as an independent memory representation?
The shift toward US valence memory as the primary mediator of evaluative conditioning has had important theoretical and methodological consequences. Most notably, it has led to the widespread use of direct memory measures that assess whether participants can report the valence of the US associated with a given CS, rather than its specific identity. Alternatively, indicators of US valence memory are sometimes derived by recoding responses from US identity measures that include both positive and negative response options, such that selection of a US with the correct valence is treated as evidence for the presence of valence memory. Together, these approaches are commonly taken to index more abstract memory representations that can support evaluative responding in the absence of memory for the specific identity of the US.

A notable exception to this prevailing interpretation comes from work by Walther and Nagengast (2006), who explicitly questioned whether apparent indications of US valence memory in commonly used measures reflect a genuinely abstract representation, or whether they may instead arise from inferences based on memory for US identity. To address this issue, they employed a four-picture recognition task in which participants were required to select the US associated with a given CS from a small set of alternatives that systematically varied in both nominal identity and valence. Crucially, the response set included not only the correct US, but also another US of the same valence, a US of the opposite valence, and a neutral control stimulus. This design allowed a direct test of whether participants who failed to identify the correct US nonetheless showed selective responding based on valence alone.

The results indicated that when participants were unable to identify the specific US paired with a CS, their responses did not preferentially favor stimuli sharing the correct valence. Instead, incorrect responses were distributed relatively evenly across the available alternatives. This pattern suggests that valence information was not retained independently of identity information, but rather that apparent valence memory may depend on access to identity-based representations.

At the same time, the diagnostic conclusions afforded by the recognition procedure used by Walther and Nagengast (2006) are not without limitations. In particular, their four-alternative recognition test treats selection of the correct US as unequivocal evidence for US identity memory. However, when participants retain only memory for the valence of the US, they nonetheless have a .5 probability of selecting the correct US whenever two response options share that valence. By attributing all correct US selections to identity memory, this procedure may therefore underestimate the presence of valence-based memory when it exists independently of US identity. Accordingly, although the findings of Walther and Nagengast (2006) raise important concerns about the presence of independent valence memory, the limitations of their recognition-based approach underscore the need to consider converging evidence from other methods.
Focusing on seminal empirical work on contingency memory in evaluative conditioning, the following sections therefore examine how---and how convincingly---prior studies have demonstrated the presence of valence-based memory in the absence of access to US identities.

Pleyers et al. (2007) provided an influential demonstration that evaluative conditioning depends on contingency awareness assessed at the level of individual CS--US pairings rather than at the level of participants. In two of their studies, contingency awareness was assessed using a recognition task in which participants selected the US paired with each CS from a set of alternatives that included stimuli from both valence categories. This allowed contingency awareness to be coded either in terms of US identity (selection of the correct US) or, more broadly, in terms of US valence (selection of any US with the correct valence). Across these experiments, EC effects---measured both with explicit evaluations and with an affective priming task---emerged reliably for CSs classified as contingency aware on the basis of US identity. By contrast, EC effects did not emerge for CSs for which participants failed to identify the correct US. Notably, this null effect also held when Pleyers et al. examined the specific subset of CSs for which participants selected an incorrect US that nonetheless shared the correct valence.

Crucially, as noted by Pleyers et al., the number of CSs falling into this latter category was small. As a result, the absence of EC effects for these items is difficult to interpret. It may indicate that memory for US valence without access to US identity was rare or absent, such that responses in this category largely reflected guessing rather than genuine valence memory---implying that no EC effect based on abstract valence representations should be expected. Alternatively, independent US valence memory may have been present but insufficient, on its own, to support changes in CS evaluations. Importantly, Pleyers et al. did not set out to test whether US valence memory was present independently of US identity memory. Accordingly, when their findings are considered from the perspective of the present question, the presence or absence of EC effects can only be treated as an indirect indicator, leaving open whether valence-based memory representations can be demonstrated independently of identity memory in more direct ways, as pursued by subsequent research.

Stahl and Unkelbach (2009) took an important step toward a more direct test of whether US valence memory can be accessed independently of US identity memory. To this end, they distinguished explicitly between memory for the identity of the US paired with a given CS and memory for its valence, and compared these two forms of awareness under conditions designed to differentially affect their availability. In particular, they contrasted a standard learning procedure in which each CS was paired with a single US with a multiple-US procedure in which each CS was paired with several different USs of the same valence. The latter manipulation was intended to selectively reduce memory for US identity while preserving memory for US valence.

Consistent with this reasoning, memory for US identity was substantially lower in the multiple-US condition than in the single-US condition, whereas memory for US valence remained relatively high. Stahl and Unkelbach interpreted this dissociation---specifically, higher levels of valence awareness than identity awareness in the multiple-US condition---as evidence that US valence can be retrieved even when memory for US identity is unavailable, supporting the existence of an independent form of valence memory.

However, this conclusion rests on a comparison of raw accuracy rates that is difficult to interpret given the structure of the memory tests. Critically, the US valence task involved a chance level of .50, whereas the US identity task required selecting one correct option from a larger response set and thus had a substantially lower chance level. As a consequence, whenever US identity cannot be retrieved and participants are forced to guess, correct responding is necessarily more likely on the valence test than on the identity test---even if valence information is never retrieved independently of identity. Thus, higher levels of apparent valence awareness may arise mechanically from differences in chance performance rather than from partial retrieval of abstract valence information. For this reason, the descriptive dissociation reported by Stahl and Unkelbach does not, by itself, provide a decisive test of whether US valence memory exists independently of US identity memory.

In addition to this descriptive comparison, Stahl and Unkelbach (2009) also conducted an indirect test of independent US valence memory, using EC effects as the outcome measure. Similar to Pleyers et al. (2007), they classified CSs into three categories based on participants’ memory reports: identity-aware CSs, valence-aware CSs (for which US valence was correctly reported but US identity was not), and unaware CSs (for which neither US valence nor US identity was correctly). Replicating earlier findings, EC effects reliably emerged for identity-aware CSs, whereas no EC effects were observed for unaware CSs. Crucially, and in contrast to Pleyers et al. (2007), a significant EC effect was also observed for CSs classified as valence aware, which Stahl and Unkelbach interpreted as evidence that memory for US valence can be sufficient to support EC even when memory for US identity is unavailable.

However, the evidential status of this finding with respect to whether US valence memory was present in the absence of access to US identities is difficult to evaluate, given how US identity memory was assessed in the multiple-US condition, from which almost all valence-aware CSs were drawn. In this condition, each CS was paired with several different USs of the same valence, yet identity memory was not assessed exhaustively for all USs paired with a given CS. Specifically, the analyses combined data from two experiments with different identity-memory procedures: In one experiment, identity memory was tested for all USs paired with a CS, whereas in the other experiment identity memory was tested for only a single US per CS. To maintain a comparable data structure across experiments, the joint analyses therefore included only one identity-memory response per CS, even in the experiment in which identity memory for multiple USs had been assessed. As a result, CSs classified as lacking US identity memory may nonetheless have been associated with retrievable identity memory for at least one the other USs paired with that CS. Such partial identity memory would be sufficient to infer the correct US valence, thereby producing the appearance of valence-aware but identity-unaware responding without requiring an abstract valence representation. Consequently, although the presence of EC effects for valence-aware CSs represents an important empirical observation, the design does not allow one to rule out that these effects were supported by identity-based memory for a different US paired with the same CS. As in earlier work, the test therefore remains inconclusive with respect to the question of whether US valence memory can be demonstrated independently of US identity memory.

Stahl, Unkelbach, and Corneille (2009) extended this line of work by explicitly comparing the contributions of US valence and US identity memory to EC across multiple experiments and evaluative measures. Importantly, they did not conduct a direct test of US valence memory in the absence of US identity memory; instead, the independence of US valence memory was inferred from a series of regression analyses in which US valence memory emerged as the primary predictor of EC effects. In their initial experiments, all CSs were paired with multiple USs of the same valence, and EC effects were observed only for CSs for which participants could report the associated US valence, whereas US identity memory did not explain additional variance. In subsequent experiments that included both single- and multiple-US conditions, the same general pattern emerged. On this basis, the authors concluded that memory for US valence, rather than memory for specific US identities, constitutes the critical contingency representation underlying EC.

However, the interpretation of these findings with respect to the independence of US valence memory and its role in predicting EC is complicated by several methodological issues. One important issue concerns the use of the multiple-US pairing procedure. As in Stahl and Unkelbach (2009), pairing a CS with multiple USs of the same valence complicates inferences about independent valence memory: Even partial identity memory for a single US paired with a CS may suffice to infer the correct US valence, such that valence-aware responding does not necessarily reflect access to an abstract valence representation. From this perspective, demonstrating an additional contribution of US identity memory would thus require that pairing a CS with multiple USs produces stronger EC effects than pairing it with a single US. This prerequisite, however, is contradicted by recent evidence showing that EC effects do not increase monotonically with additional USs of the same valence, but instead follow an averaging rule that limits the incremental impact of further stimuli (e.g., Ingendlahl et al., 2024).

Moreover, regardless of whether CSs were paired with single or multiple USs, the statistical separation of valence and identity memory is further complicated by their close functional relation. When US valence can be inferred from US identity, the two predictors are necessarily correlated, making it difficult to identify their unique contributions to EC effects. This problem is exacerbated by guessing processes: To the extent that participants guess consistently across evaluative and memory tasks, correct or incorrect guessing of US valence will translate directly into corresponding patterns in both EC effects and valence memory measures, whereas correct guessing of US identity additionally requires selecting the correct stimulus from several same-valence alternatives. As a result, even unsystematic or incorrect guessing can inflate the apparent association between US valence memory and EC effects relative to US identity memory. Consequently, although the findings of Stahl, Unkelbach, and Corneille (2009) show that EC effects are more closely related to responses on US valence tasks than on US identity tasks, they do not decisively establish that valence-based memory representations operate independently of identity-based information, nor that US valence functions as the primary mediator of EC.

## A multinomial processing tree model for memory involved in evaluative conditioning
The preceding sections highlight a striking gap in the literature. Although US valence memory is widely treated as the primary contingency representation underlying evaluative conditioning, convincing demonstrations that such memory exists independently of US identity remain scarce. Beyond the recognition-based approach of Walther and Nagengast (2006), more standard approaches rely on descriptive comparisons between identity and valence measures, regression-based analyses of their relative predictive power, or indirect inferences from EC effects themselves. These approaches are limited by inferential ambiguity, differences in chance levels, and statistical dependencies between memory measures, making it difficult to determine whether apparent valence memory reflects an abstract representation or is instead inferred from identity-based information.

To address these limitations, the present research adopts a multinomial processing tree (MPT) modeling approach that allows for a stricter and more principled test of independent US valence memory. The core advantage of this approach lies in its ability to disentangle different forms of memory and response processes within a single measurement framework. Most importantly, the model estimates US valence memory specifically in the subset of trials for which US identity memory is absent. By construction, this ensures that estimates of valence memory cannot be based on access to US identity information, thereby providing a direct test of whether valence-based representations exist independently.

In addition, the model explicitly accounts for guessing processes in the US memory task. This feature addresses a central concern raised in the previous section: namely, that guessing---when consistent across memory and evaluative measures---can artificially inflate associations between valence memory indicators and EC effects, while placing US identity memory at a statistical disadvantage. By modeling guessing tendencies separately, the MPT framework prevents such biases and places US identity and US valence memory on equal footing as predictors of evaluative change. As a result, the present approach not only allows for a clearer test of independent US valence memory, but also enables a more balanced reassessment of the relationship between US identity memory and EC.

Beyond these contributions, the model also provides an explicit measure of CS recognition memory---that is, the ability to discriminate previously encountered CSs from novel distractors. In standard US identity and US valence measures, CS recognition is typically confounded with contingency memory, obscuring whether apparent failures of pairing memory reflect genuine associative deficits or simply failures to recognize the CS itself. By modeling CS recognition separately, the present approach offers a more complete account of the memory processes underlying performance in EC-related memory tasks.

In the sections that follow, we first describe the theoretical and methodological background of the model, including the two-step memory task that generates the data structure required for its estimation. We then introduce the formal structure of the MPT model and provide recommendations for its estimation and interpretation. Finally, we present an overview of the experiments reported in this paper. Across these studies, the model is applied to examine the existence and functional properties of CS recognition, US valence and US identity memory and their relation to EC effects.

### Background and measurement task
The present modeling approach is based on an adaptation of the MPT model originally proposed by Klauer and Wegener (1998) for the analysis of social categorization in the “Who said what?” paradigm (Taylor et al., 1978). In this paradigm, participants are exposed to statements made by different individuals who belong to two social groups (e.g., Black vs. White speakers). At test, they are presented with the statements again and asked to assign each statement to its original speaker from a set of response options that includes members of both groups. The paradigm was designed to examine whether people process others solely as individuals, as would be indicated by predominant selection of the correct speakers, or whether social category membership is also encoded independently of speaker identity, as would be indicated by above-chance confusions within social categories relative to between-category confusions.

This logic---more within- than between-category confusions---parallels the basic idea underlying the four-picture recognition task employed by Walther and Nagengast (2006), illustrating the structural equivalence between representations of social categories independent of speaker identity and representations of US valence independent of US identity. Although they involve different contents, both phenomena rest on the same internal logic: a summary feature of a stimulus is extracted and stored separately from the details of the original stimulus, allowing for associative learning at the level of abstract stimulus dimensions.

The parallelism between the two phenomena is further strengthened by the fact that standard measures in the “Who said what?” paradigm are subject to many of the same limitations that characterize existing approaches to assessing US valence memory in evaluative conditioning. As demonstrated by Klauer and Wegener (1998), conventional analyses of social categorization confound category-level representations with item recognition, memory for individual persons, and guessing processes, rendering inferences about independent category encoding ambiguous. These limitations closely overlap with the problems previously identified for the four-picture recognition task and for other methods used to demonstrate US valence memory in the absence of US identity memory.

Based on these shared structural and methodological challenges, we adapted the MPT model developed for the “Who said what?” paradigm to the domain of evaluative conditioning. Parallel to the two-step memory task underlying the original model, we designed a two-step memory task for estimating the adapted MPT model. This memory task is administered after the learning phase and combines a CS recognition measure with a contingent measure of CS–US pairing memory.

On each trial of the memory task, participants are presented with a CS from the learning phase or with an unpaired distractor that was not included in the learning phase. They are first asked to indicate whether the presented stimulus was part of the previous learning phase. In this CS recognition task, participants choose between two response options: "old" for stimuli that had been included in the learning phase and "new" for stimuli that had not. Whenever participants select the "old" response---either for an actual CS or for a distractor---they subsequently perform an additional US memory task. In this task, participants are presented with a set of USs and are asked to select the stimulus that had previously been paired with the CS, or to guess if they cannot remember the correct option. On each trial of the US memory task, positive and negative USs are presented in equal numbers, with the correct US included whenever the old response was given for an actual CS.

Responses from the two task components—the CS recognition judgment and the contingent US selection—are combined into a joint categorical coding scheme that preserves the full structure of participants’ responses. For each stimulus, responses are coded with respect to (a) stimulus type (positively paired CSs, negatively paired CSs, or distractors), (b) the recognition response (old vs. new), and (c), for stimuli classified as old, the valence of the selected US. For conditioned stimuli, responses further distinguish whether the selected US matches the originally paired US identity; for distractors, any US selection is necessarily coded as incorrect.

Although the observable response categories partly overlap with those used in standard memory measures, the analytical approach differs in an important way. Previous studies typically recoded responses numerically and analyzed mean accuracy scores for US identity or US valence. Such approaches collapse across multiple underlying processes, including CS recognition, pairing memory, and guessing. In contrast, the present approach treats responses as categorical outcomes and analyzes them within a multinomial processing tree framework. This allows the different memory components, guessing processes, and response tendencies that jointly give rise to the observed data to be estimated separately, rather than being conflated within aggregate scores.

### Model structure and estimation

(ref:intro-model1-label) The MPT model trees for positively and negatively paired CSs.

(ref:intro-model2-label) The MPT model tree for unpaired distractor stimuli.

```{r bla, fig.cap = "(ref:intro-model1-label)", out.width="100%"}
file_name <- file.path(study_folders$paper,"mpt_wsw_model_exp3_KB2.pdf")
if(file.exists(file_name)) knitr::include_graphics(file_name)
```

```{r bla2, fig.cap = "(ref:intro-model2-label)", out.width="100%"}
file_name <- file.path(study_folders$paper,"mpt_wsw_model_exp3_KB2bb.pdf")
if(file.exists(file_name)) knitr::include_graphics(file_name)
```

As illustrated in Figures 1 and 2, the MPT model consists of three trees---one for positively paired CSs, one for negatively paired CSs, and one for unpaired distractors---and explains response data from the two-step memory task through a total of six parameters: $D$, $C$, $d$, $b$, $a$, and $g$. These parameters represent the (conditional or unconditional) probabilities of the cognitive states that give rise to observable responses in the memory task. In Figures 1 and 2, each parameter is displayed next to the branch leading to the cognitive state whose probability it represents.

CS recognition memory is captured by the $D$ parameter, which represents the unconditional probability of recognizing CSs as old and of detecting distractors as new. By estimating this parameter, the model separates CS recognition from contingency memory, thereby addressing the confounding of these processes in standard memory measures. US identity memory is measured by the $C$ parameter, which represents the conditional probability of remembering the specific US paired with a CS, given that the CS was recognized as old. US valence memory is measured by the $d$ parameter, which represents the conditional probability of remembering the correct US valence when the CS was recognized as old but the specific US identity could not be retrieved. By estimating US valence memory exclusively in this subset of trials, the model ensures that the $d$ parameter reflects an independent form of CS--US pairing memory that is not inferred from US identity memory.

The model further accounts for the possibility that remembered US valence may be used to guess the correct US identity. This process is captured by the $g$ parameter, which represents the conditional probability of guessing the correct US given knowledge of its valence. In typical applications, this parameter is not freely estimated but fixed to the inverse of the number of response options per US valence in the US memory task. In addition, the model estimates response tendencies in both task components. The $b$ parameter represents the conditional probability of guessing “old” in the absence of stimulus recognition, and the $a$ parameter represents the probability of guessing that a stimulus was paired with a positive US. For trials on which a CS is recognized or guessed as old and the correct US valence is guessed, the probability of selecting the correct US by chance is again captured by the $g$ parameter. Estimating these parameters prevents response biases from contaminating the measures of US identity and US valence memory. 

In Figures 1 and 2, each parameter is shown with subscripts corresponding to the three stimulus types (e.g., $D_{pos}$, $D_{neg}$, $D_{new}$), indicating that, in principle, parameters may differ across stimulus classes. In empirical applications, however, these parameters may be constrained to be equal across trees when cognitive states are assumed to occur with comparable probabilities. Such constraints are also required for model identification. The unconstrained model includes 15 parameters, whereas the two-step memory task yields only eight independent response categories, rendering the unconstrained model unidentifiable.

Accordingly, we applied constrained model variants with either five or seven freely estimated parameters per between-subjects condition. The 7-parameter variant corresponds to the standard model proposed by Klauer and Wegener (1998) and includes joint $D$, $b$, and $a$ parameters across stimulus types, as well as a fixed $g$ parameter set to $1/n$, where n denotes the number of US response options per valence. This model allows US identity and US valence memory to differ between positively and negatively paired CSs via separate $C_{pos}$ and $C_{neg}$ parameters and separate $d_{pos}$ and $d_{neg}$ parameters. The 5-parameter variant imposes additional constraints by estimating joint $C$ and $d$ parameters, thereby assuming comparable US identity and US valence memory across US valence. Although more restrictive, the 5-parameter model generally yields more precise parameter estimates and greater statistical power for comparisons across experimental conditions, provided that its fit to the data is comparable to that of the 7-parameter model.

## The present research
The present research has two closely related aims. First, we introduce the adapted MPT model as a new approach for measuring different forms of memory involved in evaluative conditioning. Rather than providing a formal validation, we examine whether the model yields coherent and theoretically meaningful parameter estimates across different EC procedures and experimental implementations.

Second, and more importantly, we use this modeling approach to re-examine a central assumption in evaluative conditioning research: that memory for US valence constitutes an independent form of memory and a key mediator of EC effects, rather than reflecting inferences from US identity memory or artifacts of standard measurement procedures. Across three experiments, we apply the model to EC paradigms that vary in materials, task order, and processing demands, allowing us to test whether US valence memory can be demonstrated independently of US identity memory, whether its formation is robust or context-dependent, and how it relates to evaluative change when measured without confounding.

The following experiments are designed to address these questions, and their implications are taken up in the General Discussion.

# Experiment 1

```{r}
data_list_pilot <- readRDS(file.path(study_folders$wsw1, "data", "data_exp1.rds"))
#data_list_pilot$excluded_participants
```

```{r}
demo <- read.csv(file.path(study_folders$wsw1, "data-raw", "demox1.csv"))
```

In Experiment 1, we examined whether the MPT model adapted from @klauer_who_1998 can be used to measure EC-related memory in the context of a standard EC procedure. To increase structural similarity to the “Who said what?” paradigm, we used images of human faces as conditioned stimuli (CSs). As in previous EC research [e.g., @stahl_subliminal_2016], the faces displayed neutral expressions and were paired with clearly valenced unconditioned stimuli (USs; positive vs. negative images). Following the learning phase, participants completed the two-step memory task and subsequently evaluated the CSs.

We chose this task order for two reasons. First, administering the memory task before the evaluation task ensured that the $D$ parameter would reflect CS recognition as established during learning, uncontaminated by later evaluative responding. Second, this order ensured that estimates of the $C$ and $d$ parameters---reflecting US identity and US valence memory, respectively---were not influenced by processes engaged during CS evaluation.

Due to its exploratory nature, Experiment 1 was not preregistered. Nevertheless, we held several expectations prior to data collection. First, we expected a reliable EC effect, reflected in more favorable evaluations of positively paired CSs than of negatively paired CSs. Second, because any EC effect presupposes at least some recognition of previously encountered CSs, we expected a significant D parameter in the MPT analysis of the memory task responses. Third, we anticipated a significant C parameter, indicating memory for the identity of the paired USs for at least a subset of recognized CSs. This expectation was based on prior findings of above-chance performance on US identity measures that included only USs of the correct valence (e.g., Stahl & Unkelbach, 2009).

By contrast, we did not have a strong a priori expectation regarding the $d$ parameter. As discussed above, standard US valence measures cannot rule out the possibility that valence responses are inferred from memory for specific US identities. Consequently, previous demonstrations of above-chance performance on such measures provide limited evidence for US valence memory in the absence of US identity memory. At the same time, Klauer and Wegener (1998) showed that participants can form abstract representations of concrete stimuli---such as encoding speakers in terms of social categories---and associate these representations with other co-occurring stimuli. Drawing on this work, we considered it plausible that participants might form abstract representations of CS–US pairings at the level of US valence, leading us to tentatively expect a nonzero $d$ parameter.

## Method
The data collection was conducted online. Materials, model equations, and data are publicly available at: <https://osf.io/rqkvy/>.

### Design and participants
The experiment followed a 2 (US valence: positive vs. negative) within-subjects design. Participants were recruited via Prolific and received \text{\pounds}2.25 for their participation (estimated duration: 15 minutes). 

Eligibility criteria restricted the sampling pool to native or fluent English speakers with at least 100 prior submissions and an approval rate of at least 90%. Participant sex was balanced. Prolific users who had taken part in any of our previous evaluative conditioning studies were excluded from recruitment.

A total of 50 participants were recruited (50$\%$ female; $M_{age} = 38.68$; $SD_{age} = 15.01$). Sample size was determined ad hoc and was not based on an a priori power analysis. One participant was excluded for reporting that they did not pay attention to the images presented during the experiment. A second participant was excluded for providing the same response on all trials of the evaluation task, which we interpreted as non-compliance. The final sample thus comprised 48 participants.

### Materials
The experiment was programmed using lab.js [@henninger_lab_2021] and deployed on an HTTPS-protected server via JATOS [@lange_just_2015].

The CS pool comprised 48 black-and-white photographs of human faces with neutral expressions (24 female, 24 male). These face images have been used as conditioned stimuli in previous evaluative conditioning research (e.g., Waroquier, Abadie, & Dienes, 2020).

As unconditioned stimuli (USs), we used 24 color images depicting animals (e.g., a cockroach), scenes (e.g., a rainbow), and objects (e.g., a knife). All US images were selected from the Open Affective Standardized Image Set [OASIS; @kurdi_introducing_2017]. Based on normative OASIS ratings on 7-point Likert scales, we selected 12 positive images ($M_{valence} = 5.88$; $SD_{valence}$ = 0.24; $M_{arousal} = 4.10$; $SD_{arousal} = 0.50$) and 12 negative images ($M_{valence} = 2.05$; $SD_{valence} = 0.32$; $M_{arousal} = 4.27$; $SD_{arousal} = 0.52$). The positive and negative image sets differed reliably in valence, Welch’s $t(20.23) = 33.36, p < .001$, but did not differ significantly in arousal, Welch’s $t(21.96) = 0.82, p = .419$.

For each participant, 12 randomly selected face images (50% female) served as positively paired CSs, 12 randomly selected face images (50% female) served as negatively paired CSs, and the remaining 24 face images (50% female) served as distractor stimuli in the test phase.

During the learning phase, all 24 OASIS images were used as USs for each participant. The USs were randomly assigned to the 24 CSs, such that 12 CSs (50% female) were paired with positive USs and 12 CSs (50% female) were paired with negative USs.

### Measures and procedures
All verbal materials were presented in English. After providing informed consent, participants were instructed to focus on the study and to carefully attend to all tasks and instructions. They then proceeded to the learning phase.

#### Learning Phase
Participants completed a learning task in which they viewed pairs of images consisting of a face (CS) and another picture (US). They were instructed to pay close attention to each image pair. The learning phase consisted of 72 trials, separated by blank screens shown for 1,000 ms. On each trial, a face image was presented on the left side of the screen and a positive or negative US image on the right side. CS and US appeared simultaneously for 1,000 ms. Each CS--US pair was presented once in each of three blocks of 24 trials, resulting in three repetitions per pair. Trial order was randomized within blocks for each participant. After completing the learning phase, participants proceeded to the test phase.

#### Test Phase
The test phase consisted of the two-step memory task followed by a CS evaluation task. Task order was identical for all participants. 

In the memory task, participants were shown individual face images and were asked to indicate whether each face had been presented during the learning phase (“old”) or not (“new”). The task comprised 48 trials, including all 24 CSs and 24 distractor faces. Faces were presented individually without time limits, with trials separated by 500 ms blank screens. Trial order was randomized for each participant. Whenever a participant classified a face as “old,” they completed a second step in which they were asked to identify the US that had previously been paired with that face. For this US memory task, eight US images from the learning phase were displayed in a 2 $\times$ 4 grid with randomized positions. Participants were instructed to select the paired US if they could remember it, or otherwise to guess. For correctly recognized CSs, the response set included the correct US, three USs of the same valence, and four USs of the opposite valence. For distractor faces incorrectly classified as “old,” eight randomly selected USs were shown, with equal numbers of positive and negative images.

Following the memory task, participants evaluated all face images. Each face (CSs and distractors) was presented individually without a time limit, and participants rated how positive or negative they found the depicted person using an 11-point scale ranging from very negative ($-5$) to very positive ($+5$). Responses were recorded as numerical values from 0 to 11. The task consisted of 48 trials, separated by 500 ms blank screens, with randomized trial order.

#### Control Measures and Debriefing
After completing the test phase, participants reported whether they had paid attention to the images and whether they had taken the tasks seriously. They were informed that these responses would not affect their compensation. Participants could then provide open-ended comments.

Finally, participants were debriefed about the purpose of the study, informed that it investigated how paired image valence influences face evaluations and memory, and provided with contact information for further questions. Participants were then redirected to Prolific and compensated for their participation.

### Data processing and statistical analyses
#### CS evaluations
The CS evaluations were analyzed without further preprocessing using the stats package in R. To test whether EC effects were present, we conducted a one-tailed paired-samples t test ($\alpha=.05$).

As an additional exploratory analysis, we examined whether EC effects depended on whether a CS was classified as old or new in the recognition task. This analysis was motivated by the fact that the proposed model estimates contingency memory only for CSs classified as old and we wanted to test whether this aspect of model construction is supported by reliable EC for CSs classified as old and weaker or absent EC effects for CSs classified as new.
To test this pattern, evaluative ratings for CSs were analyzed using a hierarchical linear model with US valence and recognition response (old vs. new) as categorical predictors. The model included the two main effects and the interaction. Random intercepts were specified at the participant level to account for individual differences in evaluative responding.
This analysis allowed us to directly test whether EC effects were stronger when CSs were classified as old than when they were classified as new.

#### Memory data
For the MPT model analyses, responses from the two-step memory task were recoded into a joint categorical memory index that combined the recognition response and the subsequent US selection. For positively and negatively paired CSs, responses in which the CS was incorrectly classified as new were coded as *CSPosNew* or *CSNegNew*, respectively. When a paired CS was correctly classified as “old,” responses were further differentiated based on the valence and identity of the selected US. Correct selection of the originally paired US was coded as *CSPosOldPosCor* or *CSNegOldNegCor*. Selection of an incorrect US sharing the correct valence was coded as *CSPosOldPosIncor* or *CSNegOldNegIncor*, whereas selection of a US of the opposite valence was coded as *CSPosOldNegIncor* or *CSNegOldPosIncor*. For distractor faces, correct classification as new was coded as *DistNew*. If a distractor face was incorrectly classified as old, the response was coded based on the valence of the subsequently selected US, resulting in *DistOldPosIncor* or *DistOldNegIncor*.

```{r}
baseline_restrictions <- list(
  g = "G"
  , g = .25
)

hypothesis_restrictions_8b <- list(
  baseline = list()
  , D_eq0  = list(D = 0)
  , Cpos_eq0  = list(C_positive = 0)
  , Cneg_eq0  = list(C_negative = 0)
  , dpos_eq0  = list(d_positive = 0)
  , dneg_eq0  = list(d_negative = 0)
  , a_eq.5 = list(a = .5)
  , b_eq.5 = list(b = .5)
  , C_eq  = list(C = c("C_positive", "C_negative"))
  , d_eq  = list(d = c("d_positive", "d_negative"))
  , fiveparam = list(C = c("C_positive", "C_negative"),d = c("d_positive", "d_negative"))
)

models_8b <- lapply(
  hypothesis_restrictions_8b
  , FUN = function(x) {
    fit_mpt(
      model = file.path(project_root, "model-equations", "wsw-8b.eqn")
      , data = data_list_pilot$mpt_data_hierarchical
      , restrictions = c(baseline_restrictions, x)
    )
  }
)

exp1 <- apa_print(compare(models_8b))
```

Next, the frequency distribution of the joint memory index (aggregated across participants) was analyzed using the R package *HMMTreeC*. We estimated several versions of the MPT model. In all model variants, the $g$ parameter was fixed to $.25$, corresponding to the inverse of the number of response options per US valence in the US memory task.

To allow for potential differences in pairing memory across US valence conditions, we first fitted a model variant that included separate $C$ and $d$ parameters for positively and negatively paired CSs, in addition to joint $D$, $b$, and $a$ parameters. This 7-parameter model was then compared to a more restrictive 5-parameter model in which the $C$ and $d$ parameters were constrained to be equal across US valence conditions. The two models did not differ significantly in fit, `r exp1$full_result$fiveparam`. We therefore report results from the more parsimonious 5-parameter model.

We further examined the relationship between MPT parameters and individual EC effects, computed as the difference between the mean evaluative rating of positively paired CSs and the mean evaluative rating of negatively paired CSs. To this end, we implemented a joint modeling approach using the probabilistic programming language *Stan* [@carpenter_stan_2016]. The joint model consisted of two components. The first component accounted for participant-level frequency distributions of the joint memory index and was specified as a hierarchical latent-trait version [cf., @klauer_hierarchical_2010] of the 5-parameter MPT model. The second component was a multiple linear regression model in which individual EC effects served as the outcome variable and participant-level MPT parameters served as predictors.

To our knowledge, this is the first application of such a joint modeling approach in evaluative conditioning research. Previous studies have typically relied on a two-step procedure, in which a latent-trait MPT model is estimated first and the resulting point estimates are then used as predictors in a separate regression analysis [e.g., @kukken_are_2020]. This approach is potentially problematic because it treats MPT parameter estimates as error-free predictors, thereby ignoring the uncertainty associated with their estimation. Because this assumption is unlikely to hold, regression coefficients obtained from such analyses may be biased. The joint modeling approach avoids this problem by simultaneously estimating memory parameters and their relationships with EC effects, while appropriately accounting for uncertainty in the parameter estimates.

```{r}
exp1_ratings <- data_list_pilot$rating
exp1_ratings_sd <- aggregate(exp1_ratings,evaluative_rating ~ sid, FUN = sd)
exp1_ratings_wide <- data_list_pilot$rating_wide
```

```{r}
rrr <- data_list_pilot$memory
rrr$reco_resp_num <- ifelse(rrr$reco_resp=="old",1,0)
rrr2 <- aggregate(rrr,reco_resp_num ~ sid, FUN = sd)
```

```{r}
ratingx1 <- subset(data_list_pilot$rating,us_valence%in%c("positive","negative"))
memoryx1 <- subset(data_list_pilot$memory,us_valence%in%c("positive","negative"))
rating_memory_x1 <- merge(ratingx1,memoryx1,by=c("sid","cs","us","us_valence"))
# apa_barplot(data=rating_memory_x1, id = "sid"
#             , dv = "evaluative_rating"
#             , factors = c("reco_resp","us_valence"))
lme_model_x1 <- lmer(
  formula = evaluative_rating ~ us_valence * reco_resp + (1| sid)
  , data = rating_memory_x1 |>
    within({
      us_valence <- factor(us_valence)
      reco_resp <- factor(reco_resp)
      contrasts(us_valence) <- "contr.sum"
      contrasts(reco_resp)  <- "contr.sum"
    })
)
apa_lme_model_x1 <- apa_print(lme_model_x1)
con_vec <- emmeans(lme_model_x1, specs = ~ us_valence | reco_resp) |>
  contrast(method = list("sme" = c(1, -1))) |>
  apa_print()
```

## Results

### Evaluative ratings

```{r}
exp1_ratings$cs_sex <- ifelse(exp1_ratings$cs %in% c("f1","f2","f3","f4","f5","f6","f7","f8","f9","f10","f11","f12","f13","f14","f15","f16","f17","f18","f19","f20","f21","f22","f23","f24"),"female","male")
exp1_ratings$us_valence2 <- "no pairing"
exp1_ratings$us_valence2 <- ifelse(is.na(exp1_ratings$us_valence)==TRUE, "no pairing",as.character(exp1_ratings$us_valence))

exp1_ratings_agg <- aggregate(exp1_ratings,FUN=mean,evaluative_rating~sid*us_valence2)
library(reshape2)
exp1_ratings_agg2 <- dcast(exp1_ratings_agg,value.var="evaluative_rating",sid~us_valence2)

exp1_posneg <- apa_print(t.test(exp1_ratings_agg2$positive,exp1_ratings_agg2$negative,paired=TRUE,alternative="greater"))

mean_pos <- round(mean(exp1_ratings_agg2$positive),2)
sd_pos <- round(sd(exp1_ratings_agg2$positive),2)

mean_neg <- round(mean(exp1_ratings_agg2$negative),2)
sd_neg <- round(sd(exp1_ratings_agg2$negative),2)

mean_nop<- round(mean(exp1_ratings_agg2$`no pairing`),2)
sd_nop <- round(sd(exp1_ratings_agg2$`no pairing`),2)

exp1_posdist <- apa_print(t.test(exp1_ratings_agg2$positive,exp1_ratings_agg2$`no pairing`,paired=TRUE,alternative="greater"))
exp1_negdist <- apa_print(t.test(exp1_ratings_agg2$negative,exp1_ratings_agg2$`no pairing`,paired=TRUE,alternative="less"))

#effectsize::cohens_d(t.test(exp1_ratings_agg2$positive,exp1_ratings_agg2$negative,paired=TRUE,alternative="greater"))
```

The mean evaluation of positively paired CSs was higher than that of negatively paired CSs ($M_{positive}=$ `r mean_pos`, $SD_{positive}=$ `r sd_pos`; $M_{negative}=$ `r mean_neg`, $SD_{negative}=$ `r sd_neg`). The resulting EC effect was small in magnitude, $d_{Cohen}= 0.26$, and reached statistical significance only in a one-tailed test, `r exp1_posneg$full_result`.

The 2 (US valence) $\times$ 2 (recognition response) hierarchical linear model revealed no significant main effect of US valence, `r apa_lme_model_x1$full_result$us_valence1`, a marginally significant main effect of recognition response, `r apa_lme_model_x1$full_result$reco_resp1`, and a marginally significant two-way interaction between US valence and recognition response, `r apa_lme_model_x1$full_result$us_valence1_reco_resp1`. Follow-up analyses revealed that EC effects were reliable for CS classified as old, `r con_vec$full_result$old_sme`, but not for CSs classified as new, `r con_vec$full_result$new_sme`, consistent with the model’s focus on contingency memory conditional on CS recognition.

### MPT model analyses

```{r}
baseline_restrictions <- list(
  g = "G"
  , g = .25
)

hypothesis_restrictions <- list(
  baseline = list()
  , D_eq0  = list(D = 0)
  , C_eq0  = list(C = 0)
  , d_eq0  = list(d = 0)
  , a_eq.5 = list(a = .5)
  , b_eq.5 = list(b = .5)
)

models <- lapply(
  hypothesis_restrictions
  , FUN = function(x) {
    fit_mpt(
      model = file.path(study_folders$wsw1, "WSW_pilot_hierarchical.eqn")
      , data = data_list_pilot$mpt_data_hierarchical
      , restrictions = c(baseline_restrictions, x)
    ) |>
      label_parameters(D = "$D$", C = "$C$", d = "$d$", a = "$a$", b = "$b$", g = "$g$")
  }
)
apa_wsw6 <- apa_print(models$baseline)
apa_wsw_comps <- apa_print(compare(models))
```

```{r}
apa_wsw8b <- apa_print(models_8b$baseline)
apa_wsw8b_comps <- apa_print(compare(models_8b))
```

The 5-parameter model fit the data well, `r apa_wsw6$statistic$modelfit`. Parameter estimates and confidence intervals are reported in Table 1.

(ref:exp1-model-label) Experiment 1: Parameter estimates and 95% confidence intervals from the unrestricted 5-parameter model.

```{r exp1-model, fig.cap = "(ref:exp1-model-label)"}
apa_wsw6$table$estimate[6L] <- "$\\nicefrac{1}{4}$"

apa_table(
  apa_wsw6
  , caption="Experiment 1: Parameter estimates and 95\\% confidence intervals from the unrestricted 5-parameter model."
  , escape = FALSE
)
```

The $D$ parameter estimate was significantly greater than zero, `r apa_wsw_comps$full_result$D_eq0`, indicating that old/new responses were partially driven by stimulus recognition rather than guessing alone. The magnitude of the $D$ estimate implies that participants correctly recognized CSs as old and detected distractor images as new on $45.6$% of trials. Conversely, on the remaining $54.4$% of trials ($1-D$), CSs were not recognized as old and distractor stimuli were not detected as new.

The $C$ parameter estimate was significantly greater than zero, `r apa_wsw_comps$full_result$C_eq0`, indicating that US selections for CSs recognized as old were driven in part by memory for US identity. The magnitude of the estimate suggests that participants retrieved the identity of the previously paired US for $67.4$% of recognized CSs. Conversely, for the remaining $32.6$% of recognized CSs, US identity memory was absent.

The $d$ parameter estimate was significantly greater than zero, `r apa_wsw_comps$full_result$d_eq0`, indicating the presence of US valence memory for a subset of recognized CSs without US identity memory. The magnitude of the estimate suggests that, among recognized CSs for which US identity could not be retrieved, participants retained memory for the valence of the paired US on approximately $19.9$% of trials. Conversely, for the remaining $80.1$% of such trials, US valence memory was absent.

The $b$ parameter estimate was significantly lower than .5, `r apa_wsw_comps$full_result$b_eq_5`, indicating a general response bias toward selecting “new” when a CS was not recognized as old or a distractor was not detected as new. The magnitude of the estimate implies that $14.8$% of these stimuli were guessed as old. Conversely, participants guessed “new” on the remaining $85.2$% of trials in which stimulus recognition failed.

The $a$ parameter estimate was significantly lower than .5, `r apa_wsw_comps$full_result$a_eq_5`, indicating a response tendency toward selecting a negative US when neither US identity nor US valence memory was available. The magnitude of the estimate suggests that, for CSs and distractor stimuli guessed as old---and for recognized CSs lacking both US identity and US valence memory---participants selected a positive US on $42.7$% of trials and a negative US on $57.3$% of trials.

### Relationships between EC effects and MPT parameters

(ref:exp1-regression-label) Experiment 1: Relationships between marginal EC effects and participant-level MPT parameters estimates from the unrestricted 5-parameter model. Points represent posterior means, error bars represent 95% credible intervals. Lines represent linear regression functions, dashed lines represent corresponding 95% credible bands.

```{r exp1-regression, fig.cap = "(ref:exp1-regression-label)", fig.width = 11, fig.height = 8, out.width="100%"}
model <- readRDS(file.path(study_folders$wsw1, "model-objects", "treestan.rds"))
par(mfrow = c(2, 3))
plot_regression(model, pars = c("D", "C", "d", "a", "b"))

BFs <- bayes_factors(model, prior_mean = 0, prior_sd = 2)
BF_summary <- paste0("$",
paste(apa_num(range(BFs$BF_10)), collapse = " \\geq \\mathit{BF}_{10} \\geq ")
, "$")
apa_BFs <- apa_print(BFs)
apa_model_lm <- apa_print(model, part = "lm")
```

Figure \@ref(fig:exp1-regression) illustrates the relationships between individual-level MPT parameter estimates and marginal EC effects. Evidence for these relationships was assessed by computing Bayes factors for the regression slopes in the linear-regression component of the joint model.

The regression analysis yielded anecdotal evidence for a positive association between participant-level $d$ parameter estimates and individual EC effects, `r apa_model_lm$full_result$d`. For participant-level $D$ and $C$ parameter estimates, the evidence was inconclusive with respect to a relationship with EC effects, `r apa_model_lm$full_result$D`, `r apa_model_lm$full_result$C`. Likewise, the analyses provided inconclusive evidence for negative associations between EC effects and participant-level $a$ and $b$ parameter estimates, `r apa_model_lm$full_result$a`, `r apa_model_lm$full_result$b`.

## Discussion
In Experiment 1, we applied the newly adapted MPT model to estimate EC-related memory within a standard evaluative conditioning procedure augmented by the two-step memory task. Responses from the memory task were well described by the 5-parameter model, including joint $D$, $C$, $d$, $a$, and $b$ parameters. As expected, the significant $D$ parameter indicated that participants were able to detect subsets of CSs as old and distractor images as new. The significant $C$ parameter further showed that participants retrieved US identity information from memory for a subset of recognized CSs. Importantly, the significant $d$ parameter indicated that participants also retrieved US valence information in the absence of US identity memory for a subset of recognized CSs. An above-zero $d$ parameter provides clear evidence that US valence memory cannot be reduced to inferences based on US identity memory. In light of the limitations of standard US valence measures discussed in the introduction, this finding constitutes, to our knowledge, the strongest empirical evidence to date that individuals can retrieve the valence of a previously paired US without access to its specific identity. Consistent with prior research [e.g., @stahl_respective_2009-1], the results of Experiment 1 also tentatively suggested that individual EC effects are positively related to US valence memory (as indexed by the d parameter), but not to US identity memory (as indexed by the C parameter).

Despite these encouraging findings, the conclusions that can be drawn from Experiment 1 remain limited. First, statistical evidence for or against relationships between individual EC effects and participant-level MPT parameters was generally weak. Second, the EC effect itself was small in size and reached significance only in a one-tailed test, whereas EC effects are typically medium-sized and readily detectable with comparable sample sizes [@hofmann_evaluative_2010]. This raises the possibility that statistical power was insufficient to obtain reliable evidence for a  moderation of EC effects by US valence memory. In addition, using human faces as initially neutral CSs may have attenuated the EC effect, for example due to variability in participants’ initial evaluations of the faces. Finally, the fixed task order in the test phase---administering the memory task before the evaluation task---was chosen to obtain uncontaminated memory estimates for fitting the MPT model, but may have further reduced sensitivity to EC effects. Experiment 2 was designed to address these limitations.

# Experiment 2
In Experiment 2, we aimed to replicate the core findings of Experiment 1---namely, significant $D$, $C$, and $d$ parameters---while obtaining stronger EC effects and more informative estimates of their relationships with participant-level MPT parameters. To increase the magnitude of EC effects, we used pronounceable nonwords as CSs, which are likely to be more affectively neutral and therefore more amenable to conditioning than the human faces used previously. In addition, we increased the sample size to obtain more precise estimates of both the mean EC effect and the associations between individual EC effects and MPT parameters. We also manipulated the order of the memory task and the CS evaluation task in the test phase. This manipulation served two purposes: first, to assess whether administering the memory task prior to evaluation attenuates EC effects, and second, to compare the suitability of the two task orders for measuring different forms of EC-related memory with the MPT model.

```{r}
data_list_exp2 <- readRDS(file.path(study_folders$wsw2_main, "data", "data_exp2.rds"))
#data_list_exp2$excluded_participants

n_final <- sum(length(unique(data_list_exp2$rating$sid)))
n_conditions <- split(data_list_exp2$rating, data_list_exp2$rating$task_order) |>
  lapply(function(x) {paste0("$n = ", length(unique(x$sid)), "$")})
n_conditions <- setNames(n_conditions, papaja::sanitize_terms(tolower(names(n_conditions))))

socio = read.csv(file.path(project_root, "/Paper/data/sociodemo.csv"))
socio = socio %>% filter(Status != "RETURNED")
```

## Method
Experiment 2 was preregistered on the Open Science Framework (OSF). The preregistration, materials, model equations, and data are publicly available at <https://osf.io/rqkvy/>. Data collection was conducted online.

### Design and participants
The experiment followed a 2 (US valence: positive vs. negative) $\times$ 2 (task order: evaluation first vs. memory first) mixed design, with US valence manipulated within participants and task order manipulated between participants.

Participants were recruited via Prolific and received \text{\pounds}2.25 for their participation (estimated duration: 15 minutes). As in Experiment 1, eligibility was restricted to English-speaking participants with at least 100 prior submissions and an approval rate of 90% or higher, who had not taken part in our previous evaluative conditioning studies. Sex was balanced. In total, 172 participants were recruited (50$\%$ female; $M_{age} =$ `r mean(as.numeric(socio$Age))`; $SD_{age} =$ `r sd(as.numeric(socio$Age))`). Due to an unknown technical error, data from one participant were unavailable. As preregistered, we excluded participants who reported not paying attention or not taking the study seriously ($n=5$). In addition, and not preregistered, we excluded four participants who gave the same response on all trials of the evaluation task, which we interpreted as non-compliant responding. These exclusions resulted in a final sample of `r n_final` participants (`r n_conditions$rating_first` in the evaluation-first condition and `r n_conditions$memory_first` in the memory-first condition).

The target sample size was determined by an a priori power analysis for a small EC effect of $d_{Cohen} = 0.20$. Power analyses were conducted using the R package *pwr* (Champely, 2020). Assuming a one-tailed paired-samples t-test (US valence as the independent variable, evaluative ratings as the dependent variable) with $\alpha=.05$, a total of 156 participants were required to achieve power of $1-\beta = .80$. This sample size also provides power of $.80$ to detect correlations of $r\mathrm{s} \geq |.22|$ (e.g., between individual parameter estimates and EC effects). To ensure that the final sample would remain above this threshold after applying the preregistered exclusion criteria, we recruited 172 participants, corresponding to a 10% oversampling.

### Materials
The experiment was programmed using lab.js [@henninger_lab_2021] and deployed on an HTTPS-protected server via JATOS [@lange_just_2015].

The CS pool consisted of 54 pronounceable non-words ranging from five to seven letters (e.g., *botsy*, *ikzunt*, *ampfong*; the full list is available in the online study materials). The non-words were adopted from a previous EC study (Stahl & Bading, 2020). For each participant, 12 randomly selected non-words were assigned as positively paired CSs, 12 as negatively paired CSs, and 24 as distractor stimuli presented only during the test phase.

As USs, we used 24 images from the OASIS database. Twenty-two of these images were identical to those used in Experiment 1; two negative images were replaced with less ambiguous alternatives (see the online study materials for details). For each participant, the 24 US images were randomly assigned to the 24 CSs.

### Measures and procedures
All verbal materials were presented in English. After providing informed consent, participants were instructed to concentrate on the study and to carefully follow all instructions and tasks. They were then presented with the instructions for the learning phase.

#### Learning phase
At the beginning of the learning phase, participants were informed that they would see pairs consisting of a non-word and an image, with the non-word always displayed on the left and the image on the right. They were instructed to pay close attention to each pair. Participants were then told that the learning phase would last approximately 2.5 minutes and that they could begin the task when ready.

The learning phase comprised 72 trials. Trials were separated by a blank screen presented for 1,000 ms. On each trial, a nonword CS (center-left) and a positive or negative US image (center-right) were presented simultaneously for 1,000 ms. For each participant, the 72 trials were divided into three randomized blocks of 24 trials. Across these blocks, each CS--US pair was presented once per block. After completing the final trial, participants were informed that the learning phase had ended and that they would proceed to the next part of the experiment.

#### Test phase
Following the learning phase, participants completed the CS evaluation and two-step memory tasks. Task order was manipulated between participants. In the evaluation-first condition, participants completed the CS evaluation task before the memory task; in the memory-first condition, the order was reversed. Apart from minor adjustments reflecting task order (see preregistration), task instructions closely followed those used in Experiment 1.

During the evaluation task, all CSs and distractor stimuli were presented individually and without time constraints. Participants rated each non-word on an 8-point scale ranging from very negative (1) to very positive (8). The task consisted of 48 trials, separated by 500-ms blank screens. Trial order was randomized separately for each participant.

In the two-step memory task, CSs and distractor stimuli were again presented individually and without time limits, resulting in 48 trials separated by 500-ms blank screens. Trial order was randomized anew for each participant. Each trial began with a recognition judgment in which participants indicated whether the presented non-word had appeared during the learning phase (“old”) or not (“new”). If a non-word was classified as new, the next trial began immediately. If it was classified as old, participants completed the US memory task in which the non-word was shown alongside eight images that had served as USs during learning. The images were displayed in two rows of four, with positions randomized on each trial. For correctly recognized CSs, the response set included the correct US, three distractors of the same valence, and four distractors of the opposite valence. For distractor stimuli incorrectly judged as old, the response set consisted of four positive and four negative images selected at random. Participants were instructed to select the previously paired image if possible and to guess if they could not remember the correct pairing.

#### Control measures and debriefing
Control measures and debriefing procedures closely followed those used in Experiment 1. After completing the test phase, participants reported whether they had paid attention during the experiment and whether they had taken their responses seriously, with assurances that their answers would not affect compensation. Participants were then given the opportunity to provide comments, received a brief explanation of the study’s purpose, and were redirected to Prolific for payment.

### Data processing and statistical analyses

```{r}
exp2_ratings <- data_list_exp2$rating
exp2_ratings_sd <- aggregate(exp2_ratings,evaluative_rating ~ sid, FUN = sd)
exp2_ratings_sd <- subset(exp2_ratings_sd, evaluative_rating > 0)
exp2_ratings <- subset(exp2_ratings,sid %in% exp2_ratings_sd$sid)
exp2_ratings_wide <- subset(data_list_exp2$rating_wide,sid %in% exp2_ratings_sd$sid)
```

```{r}
rrr <- data_list_exp2$memory
rrr$reco_resp_num <- ifelse(rrr$reco_resp=="old",1,0)
rrr2 <- aggregate(rrr,reco_resp_num ~ sid, FUN = sd)
```

#### CS evaluations
All analyses followed the preregistered protocol without deviation. Individual EC effects were computed by subtracting the mean evaluative rating of negatively paired CSs from the mean evaluative rating of positively paired CSs. These EC effects were analyzed using a between-subjects ANOVA with task order as the sole factor. In addition, mean EC effects within each task order condition were tested against zero using one-tailed t tests ($\alpha=.05$).

In an additional, non-preregistered analysis, we again examined whether EC effects depended on whether a CS was classified as old or new in the recognition task. Evaluative ratings for CSs were analyzed using a hierarchical linear model with US valence, recognition response (old vs. new), and task order as categorical predictors. The model included all main effects and interactions. To account for individual differences in evaluative responding, random intercepts and random slopes for US valence were specified at the participant level.

As preregistered, we also conducted complementary analyses comparing evaluative ratings for previously encountered CSs (“old”) and unpaired distractors (“new”). To streamline the main text, the results of these supplementary analyses are reported in the online supplement (see OSF repository).

#### Memory data

```{r}
models <- list(
  model7p    = readRDS(file.path(study_folders$wsw2_main, "model-objects", "trait-mpt-wsw-8b.rds"))
  , model5p = readRDS(file.path(study_folders$wsw2_main, "model-objects", "trait-mpt.rds"))
)

fit <- lapply(models, FUN = apa_fit)

individual <- models |>
  lapply(function(x) {
    individual_fits <- x$summary$fitStatistics$individual
    individual_fits$T1.obs <- colMeans(individual_fits$T1.obs)
    individual_fits$T1.pred <- colMeans(individual_fits$T1.pred)
    cross_table <- table(individual_fits$T1.p <= .05)
    cross_table
  })
```

Responses from the two-step memory task were recoded into a joint memory index using the same coding scheme as in Experiment 1. Participant-level frequency distributions of this joint index were then analyzed using the R package *TreeBUGS* [@heck_treebugs_2018], which implements hierarchical (latent-trait) MPT models that allow parameter values to vary across participants [@klauer_hierarchical_2010].

As preregistered, we fitted a hierarchical extension of the 7-parameter MPT model, estimating the parameters $D$, $C_{pos}$, $C_{neg}$, $d_{pos}$, $d_{neg}$, $a$, and $b$, with $g$ fixed at $.25$. Task order was included as a categorical predictor of individual parameter estimates, allowing population means for all parameters to differ between the two task order conditions. This modeling approach yielded individual-level parameter estimates, overall population means, and task-order effects for each parameter.

Model estimation was based on four Markov chains with 200,000 iterations each, of which the first 100,000 iterations were discarded as burn-in. We additionally used 20,000 adaptation iterations and applied a thinning rate of 40. Convergence was assessed using the Gelman--Rubin statistic, with all parameters meeting the criterion of $\hat{R}<1.02$. Model fit was evaluated using posterior predictive checks $T_1$ and $T_2$ [@klauer_hierarchical_2010]. Both criteria indicated an adequate fit of the 7-parameter model to the memory data, `r fit$model7p$T1`; `r fit$model7p$T2`.

Following the preregistration, we tested each of the seven parameters against its reference value ($0$ for $D$, $C$, and $d$ parameters; $.5$ for $a$ and $b$ parameters) by fitting restricted models and comparing them to the unrestricted model using the WAIC (Watanabe, 2010). Differences in WAIC greater than 10 were interpreted as strong evidence in favor of the model with the smaller value.

```{r}
dat_mem_1a <- data_list_exp2$memory
dat_mem_1a$resp2 <- dat_mem_1a$mpt_response_condition
level_order <- check.mpt(file.path(study_folders$wsw3_main, "WSW_exp3.eqn"))$eqn.order.categories
dat_mem_1a$resp2 <- factor(dat_mem_1a$resp2, levels = level_order)
dat_mem_1a$identity <- 1L
mpt_dat <- as.data.frame(unclass(table(dat_mem_1a$identity, dat_mem_1a$resp2)))
mpt_data <- as.data.frame(unclass(table(dat_mem_1a$sid, dat_mem_1a$resp2)))

baseline_restrictions <- list(
  g = "G_x1=G_x2"
  , g = .25
  #, d = "d_x1=d_x2"
)

hypothesis_restrictions <- list(
  baseline = list()
  , D_eq0  = list(D_x1 = 0)
  , C_eq0  = list(C_x1 = 0)
  , d_eq0  = list(d_x1 = 0,d_x2 = 0)
  , dx1_eq0  = list(d_x1 = 0)
  , dx2_eq0  = list(d_x2 = 0)
  , a_eq.5 = list(a_x1 = .5)
  , b_eq.5 = list(b_x1 = .5)
)

modelsb <- lapply(
  hypothesis_restrictions
  , FUN = function(x) {
    fit_mpt(
      model = file.path(study_folders$wsw3_main, "WSW_exp3.eqn")
      , data = mpt_dat
      , restrictions = c(baseline_restrictions, x)
    ) |> add_condition("Task order" = c("X1" = "_x1","X2" = "_x2")) |>
       label_parameters(D = "$D$", C = "$C$", d = "$d$", a = "$a$", b = "$b$", g = "$g$")
  }
)
apa_wsw6 <- apa_print(modelsb$baseline)
apa_wsw_comps <- apa_print(compare(modelsb))
```

As an additional, non-preregistered analysis targeting the presence of US valence memory in the absence of US identity memory, we also analyzed the frequency distribution of the joint memory index aggregated across participants using the 5-parameter model employed in Experiment 1. To allow for potential differences between task order conditions, the model was specified separately for the evaluation-first and memory-first groups, resulting in a 10-parameter model with distinct $D$, $C$, $d$, $a$, and $b$ parameters for each condition. This unconstrained model provided an adequate fit to the data, `r apa_wsw6$full_result$modelfit`. To assess whether independent US valence memory was present in either task order condition, we compared this model to restricted variants in which the $d$ parameter was fixed to zero in the evaluation-first condition, the memory-first condition, or in both conditions simultaneously.

The regression analyses deviated from the preregistered analysis plan in two respects. First, we employed the joint-modeling approach introduced in Experiment 1 rather than the preregistered two-step procedure. This approach jointly models memory data and EC effects and therefore accounts for uncertainty in individual MPT parameter estimates when relating them to evaluative outcomes. Second, we used the simpler 5-parameter model instead of the preregistered 7-parameter variant, which reduced multicollinearity among predictors and facilitated more stable estimation of regression coefficients. As preregistered, we examined several regression models of varying complexity. For clarity and interpretability, however, we report only the main-effects model specified in the preregistration. This regression model was estimated as part of a joint model that included task order as a categorical predictor of both MPT parameters and EC effects. All joint models were implemented in *Stan* [@carpenter_stan_2016].

```{r}
ratingx2 <- subset(data_list_exp2$rating,us_valence%in%c("positive","negative"))
memoryx2 <- subset(data_list_exp2$memory,us_valence%in%c("positive","negative"))
rating_memory_x2 <- merge(ratingx2,memoryx2,by=c("sid","cs","us","us_valence"))
# apa_barplot(data=rating_memory_x2, id = "sid"
#             , dv = "evaluative_rating"
#             , factors = c("reco_resp","us_valence","task_order.x"))
lme_model_x2 <- lmer(
  formula = evaluative_rating ~ us_valence * reco_resp * task_order.x + (us_valence| sid)
  , data = rating_memory_x2 |>
    within({
      us_valence <- factor(us_valence)
      reco_resp <- factor(reco_resp)
      task_order <- factor(task_order.x)
      contrasts(us_valence) <- "contr.sum"
      contrasts(reco_resp)  <- "contr.sum"
      contrasts(task_order)  <- "contr.sum"
    })
)
apa_lme_model_x2 <- apa_print(lme_model_x2)
con_vec <- emmeans(lme_model_x2, specs = ~ us_valence | reco_resp * task_order.x) |>
  contrast(method = list("sme" = c(1, -1))) |>
  apa_print()
```

## Results
### EC effects

```{r}
exp2_ratings <- subset(exp2_ratings,sid!=141)
exp2_ratings_wide <- subset(exp2_ratings_wide,sid!=141)

exp2_ec_anova <- apa_print(aov_ez(data = exp2_ratings_wide, id = "sid", dv = "ec_effect", between=c("task_order"),anova_table = list(intercept=TRUE)),intercept=TRUE,estimate="ges")
mean_ec <- paste0("$M_{EC}=",round(mean(exp2_ratings_wide$ec_effect),2),"$")
sd_ec <- paste0("$SD_{EC}=",round(sd(exp2_ratings_wide$ec_effect),2),"$")

memfirst <- subset(exp2_ratings_wide,task_order=="Memory first")
evalfirst <- subset(exp2_ratings_wide,task_order=="Rating first")
t.memfirst <- apa_print(t.test(memfirst$ec_effect,mu=0,alternative="greater"))
t.evalfirst <- apa_print(t.test(evalfirst$ec_effect,mu=0,alternative="greater"))

```

The between-subjects ANOVA revealed a significant intercept, `r exp2_ec_anova$full_result$Intercept`, indicating a reliable EC effect across task order conditions (`r mean_ec`, `r sd_ec`). The main effect of task order was not significant, `r exp2_ec_anova$full_result$task_order`, suggesting that the magnitude of the EC effect did not differ between the two task order conditions. Consistent with this pattern, the EC effect was significant when the memory task was administered first, `r t.memfirst$full_result`, as well as when the evaluation task was administered first, `r t.evalfirst$full_result`.

The 2 (US valence) $\times$ 2 (recognition response) $\times$ 2 (task order) hierarchical linear model revealed a significant main effect of US valence, `r apa_lme_model_x2$full_result$us_valence1`, and a significant main effect of recognition response, `r apa_lme_model_x2$full_result$reco_resp1`. These effects were qualified by significant interactions between US valence and recognition response, `r apa_lme_model_x2$full_result$us_valence1_reco_resp1`, and between US valence, recognition response, and task order, `r apa_lme_model_x2$full_result$us_valence1_reco_resp1_task_order_x1`. All remaining effects were non-significant (all *p*s$\geq.211$).

Follow-up contrasts showed that CSs classified as old exhibited reliable EC effects in both the memory-first condition, `r con_vec$full_result$old_Memory_first_sme`, and the evaluation-first condition, `r con_vec$full_result$old_Rating_first_sme`. For CSs classified as new, no EC effect emerged in the memory-first condition, `r con_vec$full_result$new_Memory_first_sme`. In contrast, a reduced but still significant EC effect was observed for CSs classified as new in the evaluation-first condition, `r con_vec$full_result$new_Rating_first_sme`.

### MPT model analyses

```{r model-performance}
# prepare_table <- function(x) {
#   data.frame(as.list(x$summary$fitStatistics$overall))
# }
# 
# fit_stats <- lapply(models, prepare_table) |> do.call(what = "rbind")
# 
# waics <- readRDS(file.path(study_folder, "waic.rds"))[names(models)]
# 
# waic_stats <- lapply(waics, FUN = function(x){
#   data.frame(
#     waic = sum(x$waic)
#     , se_waic = sqrt(length(x$waic)) * sd(x$waic)
#   )
# }) |> do.call(what = "rbind")
# table_data <- cbind(fit_stats, waic_stats)
# table_data <- within(
#   table_data
#   , {
#     p.T1 <- apa_p(p.T1)
#     p.T2 <- apa_p(p.T2)
#   }
# )
# table_data <- apa_num(table_data)
# table_data <- t(table_data)
# table_data <- data.frame(
#   " " = c(
#     "$T_1^{\\mathrm{observed}}$", "$T_1^{\\mathrm{expected}}$", "$p$"
#     , "$T_2^{\\mathrm{observed}}$", "$T_2^{\\mathrm{expected}}$", "$p$"
#     , "$\\mathrm{WAIC}$"
#     , "$\\mathit{SE}$" # _{\\mathrm{WAIC}}$"
#   )
#   , table_data
#   , check.names = F
# )
# rownames(table_data) <- NULL
# colnames(table_data) <- gsub(colnames(table_data), pattern = "_", replacement = "")
# 
# save(table_data, file ="waid_table.rdata")

study_folder2 <- file.path(
  rprojroot::find_rstudio_root_file()
  , "Paper"
  , "mpt analyses"
)

#load("waid_table.rdata")
load(file.path(study_folder2, "waid_table.rdata"))
colnames(table_data)[2] <- "unrestricted"
colnames(table_data) <- gsub(colnames(table_data), pattern = "^both|HQ$", replacement = "")
variable_labels(table_data) <- list(
  "unrestricted" = "Unrestricted"
  , a5 = "$a = .5$"
  , b5 = "$b = .5$"
  , Cneg0 = "$C_{\\mathrm{neg}} = 0$"
  , Cpos0 = "$C_{\\mathrm{pos}} = 0$"
  , D0    = "$D = 0$"
  , dpos0 = "$d_{\\mathrm{pos}} = 0$"
  , dneg0 = "$d_{\\mathrm{neg}} = 0$"
)
table_data <- table_data[, c(" ", "unrestricted", "D0", "Cpos0", "Cneg0", "dpos0", "dneg0", "b5", "a5")]

apa_table(
  table_data
  , caption = "Experiment 2: Absolute fit and WAIC for the hierarchical extensions of unrestricted and restricted variants of the who-said-what model."
  , escape = FALSE
  , landscape = FALSE
  , font_size = "scriptsize"
  , stub_indents = list(
    "Goodness of fit: Means" = 1:3
    , "Goodness of fit: Covariances" = 4:6
    , "Relative predictive accuracy" = 7:8
  )
  , align = c("l", rep("r", ncol(table_data) - 1L))
)
```

```{r exp1-param}
# MPT parameter estimates ----
df <- apa_print(summary(models$model7p), parameters = "mean", estimate = "Median")

# Create some beautiful parameter labels
parameter_labels <- c(
  A = "$a$"
  , B = "$b$"
  , a = "$a$"
  , b = "$b$"
  , "C positive" = "$C_{\\mathrm{pos}}$"
  , "C negative" = "$C_{\\mathrm{neg}}$"
  , "D positive" = "$d_{\\mathrm{pos}}$"
  , "D negative" = "$d_{\\mathrm{neg}}$"
  , "C_positive" = "$C_{\\mathrm{pos}}$"
  , "C_negative" = "$C_{\\mathrm{neg}}$"
  , "D" = "$D$"
  , d_positive = "$d_{\\mathrm{pos}}$"
  , d_negative = "$d_{\\mathrm{neg}}$"
  , Dn   = "$D$"
)
df$parameter <- parameter_labels[df$term]
df <- subset(df, select = c("parameter", "estimate", "conf.int"))

group_means <- getGroupMeans(models$model7p, probit = FALSE)
apa_groups <- data.frame(
  full_term = rownames(group_means)
  , estimate = apa_num(group_means[, "50%", drop = TRUE], gt1 = TRUE, digits = 3L)
  , conf.int = apa_interval(group_means[, c("2.5%", "97.5%")], gt1 = TRUE, digits = 3L) |> unlist()
  , p.value = apa_p(group_means[, "p(one-sided vs. overall)"])
  , row.names = NULL
)
apa_groups$term <- gsub(apa_groups$full_term, pattern = "_task_order.*$", replacement = "")
apa_groups$parameter <- parameter_labels[apa_groups$term]
apa_groups$group <- gsub(apa_groups$full_term, pattern = ".*order\\[|\\]$", replacement = "")
groups_wide <- tidyr::pivot_wider(apa_groups, names_from = "group", values_from = c("estimate", "conf.int", "p.value"), id_cols = "parameter")
groups_wide <- groups_wide[, c("parameter", "estimate_Rating first", "conf.int_Rating first", "estimate_Memory first", "conf.int_Memory first", "p.value_Rating first")]
df <- merge(df, groups_wide, by = "parameter", sort = FALSE)

variable_labels(df) <- list(
  parameter = "Parameter"
  , estimate = "$M$"
  , conf.int = "95\\% CI"
  , "estimate_Rating first" = "$M$"
  , "conf.int_Rating first" = "95\\% CI"
  , "estimate_Memory first"  = "$M$"
  , "conf.int_Memory first"  = "95\\% CI"
  , "p.value_Rating first" = "$\\:p$"
)

apa_table(
  df[c(5, 4, 3, 7, 6, 2, 1), ]
  , caption = "Experiment 2: Parameter estimates (posterior medians with 95\\% credible intervals) based on the hierarchical extension of the unrestricted 7-parameter model with task order as categorical predictor of individual parameter estimates."
  , col_spanners = list("Overall" = 2:3, "Evaluation first" = c(4, 5),"Memory first" = c(6, 7))
  , escape = FALSE
  , align = c("l", rep("c", ncol(df) - 2), "r")
  , note = "One-sided $p$ values"
)

```

Measures of absolute and relative model fit for the unrestricted 7-parameter model and its restricted variants are reported in Table \@ref(tab:model-performance). Parameter estimates and corresponding credible intervals from the unrestricted 7-parameter model, separately for the two task order conditions, are reported in Table \@ref(tab:exp1-param).

For the $D$, $C$, and $b$ parameters, the results closely mirrored those obtained in Experiment 1. As shown in Table \@ref(tab:model-performance), fixing $D$, $C_{pos}$, or $C_{neg}$ to zero resulted in inadequate model fit and substantial increases in the WAIC. This pattern indicates that, as in Experiment 1, participants showed reliable CS recognition and US identity memory for a subset of CSs. Likewise, constraining the $b$ parameter to $.5$ led to inadequate fit and a marked increase in the WAIC, indicating a systematic bias toward responding “new” when CSs were not recognized as old (or distractors were not detected as new), again replicating the findings from Experiment 1.

In contrast, the results for the $d$ parameters differed from those obtained in Experiment 1. As reported in Table \@ref(tab:model-performance), fixing $d_{pos}$ or $d_{neg}$ to zero resulted in adequate model fit and only negligible changes in the WAIC. This pattern was mirrored in the complementary analysis based on the 10-parameter model fitted to the aggregated response frequencies: fixing $d_{evaluation-first}$, $d_{memory-first}$ or both $d$ parameters to zero did not lead to substantial loss in model fit (all *p*s $\geq.210$). Thus, unlike in Experiment 1, the present data did not provide robust evidence for US valence memory in the absence of US identity memory.

Finally, constraining the $a$ parameter to $.5$ also yielded adequate model fit and a negligible change in the WAIC (see Table \@ref(tab:model-performance)). Accordingly, there was no evidence in the present experiment for a systematic response bias toward positive or negative images in the US memory task.

To compare MPT parameter estimates across task order conditions, we computed posterior differences between the group means and their associated Bayesian *p* values (see Table \@ref(tab:exp1-param)). For the $D$ parameter, the group mean was substantially higher in the memory-first condition than in the evaluation-first condition. For the $b$ parameter, the pattern was reversed, with a substantially higher group mean in the evaluation-first condition than in the memory-first condition. For all remaining MPT parameters, posterior differences between group means were negligible.

### Relationships between EC effects and MPT parameters

(ref:exp2-regression-label) Experiment 2: Relationships between marginal EC effects and participant-level MPT parameters estimates from the unrestricted 5-parameters model. Points represent posterior means, error bars represent 95% credible intervals. Lines represent linear regression functions, dashed lines represent corresponding 95% credible bands.

```{r exp2-regression, fig.cap = "(ref:exp2-regression-label)", fig.width = 11, fig.height = 8, out.width="100%"}
model <- readRDS(file.path(study_folders$wsw2_main, "model-objects", "treestan.rds"))


par(mfrow = c(2, 3))
plot_regression(model, pars = c("D"))
legend(
  x = .02
  , y = 5.98
  , legend = c("Evaluation first", "Memory first")
  , pch = 21
  , col = "black"
  , pt.bg = 1:2
  , bty = "n"
)
plot_regression(model, pars = c("C", "d", "a", "b"))

BFs <- bayes_factors(model)
BF_summary <- paste0("$",
paste(apa_num(range(subset(BFs, parameter != "d")$BF_10)), collapse = " \\geq \\mathit{BF}_{10} \\geq ")
, "$")
BFs <- apa_print(BFs)
apa_model_lm <- apa_print(model, part = "lm")
```

Figure \@ref(fig:exp2-regression) depicts the relationships between participant-level MPT parameter estimates and marginal EC effects. As in Experiment 1, we assessed these relationships by computing Bayes Factors for the regression slopes in the linear component of the joint model.

The regression analysis provided strong evidence for a positive relationship between participant-level $d$ parameter estimates and individual EC effects, `r apa_model_lm$full_result$d`. In contrast, evidence regarding the remaining parameters was inconclusive. For participant-level $D$ parameter estimates, we found inconclusive evidence against an association with EC effects, `r apa_model_lm$full_result$D`. The same was true for participant-level $C$ parameter estimates, `r apa_model_lm$full_result$C`. For the $a$ parameter, the analysis yielded inconclusive evidence for a negative relationship with EC effects, `r apa_model_lm$full_result$a`. Finally, for participant-level $b$ parameter estimates, evidence against a relationship with EC effects was again inconclusive, `r apa_model_lm$full_result$b`.

```{r}
apa_table(
  list(
    "Effects of task order manipulation on MPT parameters" = apa_print(model, "mpt")$table
    , "Standardized regression coefficients" = apa_print(model, "lm")$table
  )
  , escape = FALSE
)
```

## Discussion
Experiment 2 was designed to increase the size and reliability of the EC effect and to provide a stronger test of the adapted MPT model, its parameter estimates, and their relationships with evaluative change. Relative to Experiment 1, this preregistered study increased the sample size and used pronounceable non-words rather than human faces as CSs. In addition, we varied the order of the memory and evaluation tasks in the test phase to examine whether task order influences EC effects or the estimation of MPT parameters.

At the aggregate level, Experiment 2 yielded a robust EC effect. Task order did not significantly influence the magnitude of this effect, indicating that administering the two-step memory task before the CS evaluation task does not bias the estimation of evaluative change. This finding is methodologically important, as it suggests that EC effects can be assessed after memory measurement without systematic distortion.

Responses from the memory task were well described by the adapted MPT model. This held for both the preregistered 7-parameter version and the more parsimonious 5-parameter variant. Importantly, model fit was acceptable in both task order conditions, with no qualitative differences between them. At the group level, estimates of the $C$ and $d$ parameters were unaffected by task order, suggesting that CS--US pairing memory---both identity-based and valence-based---was not altered by whether participants evaluated CSs before or after completing the memory task. In contrast, task order did affect the $D$ and $b$ parameters: CS recognition was higher and guessing “old” was less frequent when the test phase began with the memory task rather than the evaluation task. One plausible explanation is that completing the evaluation task first introduced additional source confusion (e.g., whether a non-word had been encountered during learning or only during evaluation), effectively acting as an extra encoding episode. Although this effect is secondary for the interpretation of US identity and US valence memory, it has clear implications for parameter precision. Because estimates of the $C$ and $d$ parameters are conditional on CS recognition, lower recognition rates directly reduce the number of trials contributing to their estimation. When the evaluation task precedes the memory task, fewer responses enter the relevant model branches, resulting in noisier estimates of conditional memory parameters. For this reason, a memory-first task order may be preferable when precise estimation of $C$ and $d$ is a primary concern.

Despite these task-order effects, both conditions replicated the presence of CS recognition and US identity memory at the group level, as indicated by significant $D$ and $C$ parameters. In contrast, the significant $d$ parameter observed in Experiment 1 was not replicated. Thus, at the group level, Experiment 2 did not provide robust evidence for US valence memory in the absence of US identity memory. Crucially, however, this null result must be interpreted in light of the participant-level analyses: we found strong evidence for a positive relationship between individual $d$ parameter estimates and individual EC effects. This dissociation suggests that the reliable EC effect observed at the group level was driven primarily by a subset of participants who formed US valence memory without concomitant US identity memory during learning.

Why, then, did US valence memory emerge robustly in Experiment 1 but not in Experiment 2? One plausible explanation concerns differences in processing focus induced by the CS materials. In Experiment 1, human faces were used as CSs. Faces are socially meaningful stimuli that people are accustomed to evaluating rapidly and spontaneously in everyday life. As a result, face stimuli may elicit an evaluative processing focus even in the absence of explicit instructions to attend to valence. Such a spontaneous evaluative orientation could facilitate the abstraction and storage of US valence information, thereby increasing the likelihood of observing US valence memory independent of US identity. In contrast, pronounceable non-words lack intrinsic social or affective relevance and may not naturally invite evaluative processing. When participants encounter such stimuli without explicit cues that valence is relevant, they may encode the pairings in a more neutral or item-specific manner, resulting in weaker or less frequent formation of abstract valence representations.

This interpretation aligns with prior work by Gast and Rothermund (2011), who demonstrated that EC effects and CS--US pairing memory are enhanced when learning occurs under a valence-focused processing mode, for example through evaluative judgments during the learning phase. In Experiment 2, no such evaluative focus was induced: participants were neither instructed to attend to valence nor required to perform evaluative judgments during learning. Consequently, many participants may have processed the CS--US pairings without abstracting valence information, leading to low group-level estimates of US valence memory. At the same time, individual differences in spontaneous evaluative focus could explain why a subset of participants nevertheless formed valence-based representations and showed stronger EC effects.

Experiment 3 was designed to test this processing-focus account directly by manipulating evaluative versus non-evaluative processing during learning and examining its consequences for US identity memory, US valence memory, and their respective roles in mediating EC effects.

# Experiment 3

```{r}
data_list_exp3 <- readRDS(file.path(study_folders$wsw3_main, "data", "data_exp3.rds"))
```

```{r}
ratingx3 <- subset(data_list_exp3$rating,us_valence%in%c("positive","negative"))
memoryx3 <- subset(data_list_exp3$memory,us_valence%in%c("positive","negative"))
rating_memory_x3 <- merge(ratingx3,memoryx3,by=c("sid","cs","us","us_valence"))
# apa_barplot(data=rating_memory_x3, id = "sid"
#             , dv = "evaluative_rating"
#             , factors = c("reco_resp","us_valence","task_focus.x"))
lme_model_x3 <- lmer(
  formula = evaluative_rating ~ us_valence * reco_resp * task_focus.x + (us_valence| sid)
  , data = rating_memory_x3 |>
    within({
      us_valence <- factor(us_valence)
      reco_resp <- factor(reco_resp)
      task_focus <- factor(task_focus.x)
      contrasts(us_valence) <- "contr.sum"
      contrasts(reco_resp)  <- "contr.sum"
      contrasts(task_focus)  <- "contr.sum"
    })
)
apa_lme_model_x3 <- apa_print(lme_model_x3)

# emmeans(lme_model_x3, specs = ~ us_valence + reco_resp | task_focus.x) |>
#   contrast(method = list(
#     "us_valence" = c(1, -1, 1, -1)
#     , "reco_resp" = c(1, 1, -1, -1)
#     , "Interaction" = c(1, -1, -1, 1)
#   )
# )

con_vec_x3 <- emmeans(lme_model_x3, specs = ~ us_valence | reco_resp * task_focus.x) |>
  contrast(method = list("sme" = c(1, -1))) |>
  apa_print()
```

```{r}
rating_memory_x1$sid <- paste0(rating_memory_x1$sid,"_x1")
rating_memory_x1$condition <- "x1"
rating_memory_x1$order <- "mem-first"

rating_memory_x2$sid <- paste0(rating_memory_x2$sid,"_x2")
rating_memory_x2$condition <- ifelse(rating_memory_x2$task_order.x == "Memory first", "x2-mem", "x2-eval")
rating_memory_x2$order <- ifelse(rating_memory_x2$task_order.x == "Memory first", "mem-first", "eval-first")

rating_memory_x3$sid <- paste0(rating_memory_x3$sid,"_x3")
rating_memory_x3$condition <- ifelse(rating_memory_x3$task_focus.x == "valence", "x3-val", "x3-age")
rating_memory_x3$order <- "eval-first"

rating_memory_x1b <- rating_memory_x1[,c("sid","cs","us_valence","evaluative_rating","reco_resp","condition","order")]
rating_memory_x2b <- rating_memory_x2[,c("sid","cs","us_valence","evaluative_rating","reco_resp","condition","order")]
rating_memory_x3b <- rating_memory_x3[,c("sid","cs","us_valence","evaluative_rating","reco_resp","condition","order")]
rating_memory_all <- rbind(rating_memory_x1b,rating_memory_x2b)
rating_memory_all <- rbind(rating_memory_all,rating_memory_x3b)


lme_model_all <- lmer(
  formula = evaluative_rating ~ us_valence * reco_resp * order + (1| sid)
  , data = rating_memory_all |>
    within({
      us_valence <- factor(us_valence)
      reco_resp <- factor(reco_resp)
      order <- factor(order)
      contrasts(us_valence) <- "contr.sum"
      contrasts(reco_resp)  <- "contr.sum"
      contrasts(order)  <- "contr.sum"
    })
)
apa_lme_model_all <- apa_print(lme_model_all)
# emmeans(lme_model_all, specs = ~ us_valence + reco_resp | order) |>
#   contrast(method = list(
#     "us_valence" = c(1, -1, 1, -1)
#     , "reco_resp" = c(1, 1, -1, -1)
#     , "Interaction" = c(1, -1, -1, 1)
#   )
# )
con_vec <- emmeans(lme_model_all, specs = ~ us_valence | reco_resp * order) |>
  contrast(method = list("sme" = c(1, -1))) |>
  apa_print()
```

Experiment 3 pursued three closely related aims, all centered on the question of whether US valence memory depends on a valence-focused mode of processing during learning.

First, we aimed to replicate the core valence focus effect reported by Gast and Rothermund (2011), according to which evaluative conditioning is stronger when participants attend to stimulus valence during learning. Using materials closely matched to their Experiment 2, we compared EC effects between two groups that performed different judgment tasks while observing CS--US pairings. In the valence focus condition, participants indicated whether each pairing elicited a positive or negative impression, whereas in the age focus condition they judged whether the pairing appeared typically young or old. EC effects were assessed immediately after learning using a standard CS evaluation task. Consistent with Gast and Rothermund (2011), we expected larger EC effects in the valence focus condition than in the age focus condition.

Second, and more centrally, we examined whether US valence memory emerges as a function of processing focus. Gast and Rothermund (2011) reported that a valence-focused processing mode during learning was associated with improved performance on both US identity and US valence memory measures. US identity memory was measured using a standard CS--US identity task with response options from both valence categories, whereas US valence memory was inferred by recoding the selected USs according to their valence. Both measures showed higher performance in the valence focus condition than in the control condition. As discussed in the Introduction, however, these measures conflate CS recognition, US identity memory, US valence memory, and guessing processes, making it difficult to determine which memory representations were actually affected by valence focus. In the present experiment, we addressed these limitations by combining the two-step memory task with the adapted MPT model, which allows CS recognition, US identity memory, and US valence memory to be estimated separately and without inferential confounds. This approach enabled a stringent test of whether retrieving US valence information in the absence of US identity retrieval depends on adopting a valence-focused processing mode during learning. On this basis, we expected a larger $d$ parameter in the valence focus condition than in the age focus condition.

Third, we re-examined the roles of US identity and US valence memory in mediating the valence focus effect on evaluative conditioning. Gast and Rothermund (2011) reported evidence from multiple mediation analyses suggesting that valence focus influences EC via changes in US valence memory, but not via US identity memory. As outlined earlier, however, such conclusions are difficult to sustain when mediation analyses rely on non-independent memory measures that differ in chance level and are susceptible to guessing processes. In the present experiment, these limitations were addressed by conducting mediation analyses using participant-level MPT parameter estimates as competing mediators. By simultaneously modeling CS recognition, US identity memory, and US valence memory---while controlling for response tendencies in the memory task---we aimed to provide a clearer and less confounded test of whether US valence memory uniquely mediates the effect of valence-focused processing on evaluative conditioning.

```{r}
# from MB:
data_list <- readRDS(file.path(study_folders$wsw3_main, "data", "data.rds"))
#data_list$excluded_participants                       # Participant IDs
#lapply(data_list$excluded_participants, FUN = length) # How many?

# data_list_pilot <- readRDS(file.path(study_folders$wsw3_p2, "data", "data.rds"))

# data_list_joint <- readRDS(file.path(study_folders$wsw3_joint, "data", "data.rds"))

# data_list_pilot$rating$study <- "pilot"
# data_list_pilot$rating_wide$study <- "pilot"

data_list$rating$study <- "main"
data_list$rating_wide$study <- "main"

```

## Method
Experiment 3 was preregistered on the Open Science Framework (OSF). The preregistration, materials, model equations, and data are publicly available at <https://osf.io/rqkvy/>. Data collection was conducted online.

### Design and participants

```{r}

demographics <- read.csv(file.path(study_folders$wsw3_main, "data-raw", "demographics.csv")) |>
  subset(Status == "APPROVED") |>
  within({
    Age <- as.integer(Age)
    Sex <- factor(Sex, levels = c("Female", "Male"))
  })

n_sex <- table(demographics$Sex)
prop_sex <- as.list(proportions(n_sex)) |>
  lapply(function(x) {
    paste0("$",apa_num(x * 100), "\\%$")
  })

age <- with(demographics, {
  paste0(
    "$M_\\mathrm{age} = "
    , apa_num(mean(Age))
    , "$, $\\mathit{SD}_\\mathrm{age} = "
    , apa_num(sd(Age))
    , "$"
  )
})

n_conditions <- split(data_list$rating, data_list$rating$task_focus) |>
  lapply(function(x) {paste0("$n = ", length(unique(x$sid)), "$")})
n_conditions <- setNames(n_conditions, papaja::sanitize_terms(tolower(names(n_conditions))))



```

The experiment followed a 2 (US valence: positive vs. negative) $\times$ 2 (US age: young vs. old) $\times$ 2 (task focus: valence focus vs. age focus) mixed design. US valence and US age varied within participants, whereas task focus varied between participants.

Participants were recruited via Prolific and received \text{\pounds}3.00 for participation (estimated duration: 20 minutes). Eligibility was restricted to Prolific users whose first language was English, who resided in the USA or the United Kingdom, and who had completed at least 20 prior submissions with an approval rate of at least 90%. As in the previous experiments, participants who had taken part in earlier pretests or studies from this project were excluded. Sex was balanced across conditions.

A total of 142 participants were recruited (`r prop_sex$Female` female; `r age`). As preregistered, we excluded all participants who failed at least one control measure (i.e., reporting inattention, indicating that they had merely clicked through the study, or selecting at least one physical activity in the manipulation check). This resulted in a final sample of 105 participants, with 52 assigned to the valence focus condition and 53 to the age focus condition.

The sample size was determined by a preregistered sequential sampling plan (see OSF repository). Specifically, we preregistered a minimum sample size per task focus condition, a maximum sample size across conditions, and two stopping criteria governing whether data collection would continue within this range. The minimum sample size ($n=52$ per task focus condition, after exclusions) was based on a power analysis targeting the critical US valence $\times$ task focus interaction. Because we expected larger EC effects under valence focus than under age focus, the interaction was tested one-tailed with $\alpha=.10$ and target power $1-\beta=.90$. Based on a pilot study, the expected effect size for the interaction was set to $\eta^2_{p}\approx.079$, corresponding to approximately half of the pilot effect.

Data collection proceeded until the minimum sample size was reached. At that point, we fitted the preregistered 10-parameter MPT model, which estimated joint $D$, $C$, $d$, $a$, and $b$ parameters separately for the two task focus conditions. Using Bayes factors, we compared this unrestricted model to a restricted 9-parameter variant in which the $d$ parameters were constrained to be equal across task focus conditions (for details, see the section “Data processing and statistical analyses”). Data collection was discontinued because the first preregistered stopping criterion was met: the Bayes factor in favor of the unrestricted model exceeded 10, indicating strong evidence for a task focus effect on the $d$ parameter. The complete set of stopping criteria is documented in the preregistration.

### Materials
The experiment was programmed using lab.js [@henninger_lab_2021] and deployed on an HTTPS-protected server via JATOS [@lange_just_2015].

The CS pool consisted of 48 colored images of middle-aged human faces with neutral expressions (24 female, 24 male). The images were taken from the FACES database (Ebner, Riediger, & Lindenberger, 2010) and were optimized using an AI-based image optimization tool. For each participant, 12 randomly selected face images (50% female) served as positively paired CSs (i.e., paired with a positive US during the learning phase), 12 randomly selected face images (50% female) served as negatively paired CSs, and the remaining 24 face images served as unpaired distractor stimuli in the test phase.

The US pool comprised 24 adjectives describing human traits, arranged in a 2 (US valence: positive vs. negative) $\times$ 2 (US age: young vs. old) structure, with six adjectives in each cell. Based on a pilot study, we selected six adjectives describing traits that are positive and more typical of younger (than older) people (*energetic*, *flexible*, *lively*, *open-minded*, *optimistic*, *strong*), six adjectives describing traits that are positive and more typical of older (than younger) people (*calm*, *dignified*, *nurturing*, *patient*, *realistic*, *wise*), six adjectives describing traits that are negative and more typical of younger (than older) people (*careless*, *impulsive*, *naive*, *selfish*, *spoilt*, *self-absorbed*), and six adjectives describing traits that are negative and more typical of older (than younger) people (*demented*, *feeble*, *frail*, *rigid*, *stubborn*, *weak*).

For each participant, all 24 adjectives served as USs during the learning phase and were randomly assigned to the 24 CSs. Specifically, six CSs (50% female) were paired with positive--young USs, six CSs (50% female) with positive--old USs, six CSs (50% female) with negative--young USs, and six CSs (50% female) with negative--old USs.

### Measures and procedures
All verbal materials were presented in English. After providing informed consent, participants were instructed to focus on the study and to attend carefully to all tasks and instructions. They then proceeded to the learning phase.

#### Learning phase
At the beginning of the learning phase, all participants received general instructions explaining that they would be presented with photographs of faces paired with adjectives describing human traits. They were instructed to attend carefully to both the traits and the faces and to form an overall impression of each face--trait pair.

Subsequent instructions depended on the assigned task focus condition. In the valence-focus condition, participants were instructed to judge whether each face–trait pair conveyed a rather positive or rather negative impression. In the age-focus condition, participants were instead instructed to judge whether each face–trait pair appeared more typically young or typically old. In both conditions, the order of the category labels (positive vs. negative; typically young vs. typically old) in the instructions was randomized across participants.

Participants then received task-specific response instructions. In the valence-focus condition, they were instructed to indicate whether each face--trait pair was positive or negative by pressing designated response keys on the keyboard. In the age-focus condition, they were instructed to indicate whether each pair was typically young or typically old using the same response keys. Participants were informed that response time was limited but were encouraged to respond on every trial. The order of the category labels was consistent across instruction screens and matched the left–right mapping of the response keys (i.e., the category mentioned first was always assigned to the left-hand response key).

After initiating the task, participants completed a learning phase consisting of 72 trials. On each trial, a US adjective was first presented in green font against a black background, centered in the upper half of the screen. After 1,000 ms, the CS face appeared directly below the adjective, centered in the middle of the screen. The face--trait pair remained visible for a total of 4,000 ms.

Participants were required to provide their judgment response within 3,000 ms following the onset of the US by pressing the assigned response key (“A” or “L”). When a response was registered within this time window, visual feedback in the form of three hyphens appeared below the face--trait pair to indicate that the response had been recorded. After up to 1,000 ms, the stimuli and feedback were removed and replaced by a blank screen. Following an intertrial interval of 2,500 ms, the next trial began with the presentation of a new US.

If no response was entered within the response deadline, a “no response” message was displayed below the face--trait pair in red font for 1,000 ms. Subsequently, the stimuli were replaced by a screen instructing participants to press the spacebar to continue. This message, also presented in red font and located in the lower half of the screen, remained visible until the spacebar was pressed. Participants then viewed a blank screen before the next trial commenced after a 2,500 ms delay.

Trials were presented in three randomized blocks of 24 trials each. Across the three blocks, every CS--US pairing was presented once per block. Upon completion of the final trial, participants in both task focus conditions were informed that the learning phase had ended and were prompted to proceed to the next part of the experiment.

#### Test phase
After completing the learning phase, participants proceeded to the test phase. During this phase, they completed two tasks in a fixed order: first the CS evaluation task, followed by the two-step memory task. Task order and task instructions were identical for all participants.

At the beginning of the CS evaluation task, participants were informed that they would be presented with individual face images and asked to report their personal impressions of each face. They were instructed to indicate how positive or negative they found each face using an 8-point rating scale ranging from very negative (left) to very positive (right). Participants were asked to select the scale point that best reflected their evaluation of the displayed face and to start the task by pressing the spacebar. After initiating the task, participants completed 48 evaluation trials. The 24 conditioned stimuli and the 24 distractor faces were presented individually and without time limits. On each trial, participants rated the valence of the face using the 8-point scale (1 = very negative, 8 = very positive). Trials were separated by blank screens shown for 500 ms, and the presentation order of all faces was randomized independently for each participant.

Before beginning the memory task, participants received detailed instructions explaining the structure of the task. They were informed that, in this part of the experiment, they would again see individual face images. Some of these faces had been presented earlier as part of the face--trait pairs during the learning phase, whereas other faces had been shown only in the evaluation task and had not been paired with any trait. For each face, participants were asked to indicate whether it had been shown during the learning phase (“shown”) or not (“not shown”). Participants were further informed that classifying a face as “shown” would trigger a second task. In this second step, they would be presented with a set of traits and asked to select the trait that had been paired with the face during the learning phase. They were instructed to click on the trait if they could remember it and to make their best guess if they could not recall the paired trait. After reading these instructions, participants initiated the task by pressing the spacebar.

The two-step memory task consisted of 48 trials. Across trials, the 24 CS faces and the 24 distractor faces were presented individually and without time limits. Trials were separated by blank screens shown for 500 ms, and trial order was randomized anew for each participant. Each trial began with the CS recognition judgment. A single face was displayed, and participants indicated whether the face had been shown previously by selecting one of two response buttons labeled “Shown” or “Not shown.” These response labels were used instead of “old” and “new” to avoid confusion with the age-related response categories (“typically old” vs. “typically young”) used during the learning phase. When participants selected “Not shown,” the trial ended immediately and the next recognition trial began. When participants selected “Shown,” the trial proceeded to the second step assessing memory for the paired trait. In this step, the face image was presented together with 16 trait adjectives. All 16 adjectives had appeared as USs during the learning phase (four adjectives per US valence $\times$ US age condition). The adjectives were displayed as clickable buttons arranged in three rows, with six buttons in each of the top two rows and four buttons in the bottom row. The spatial positions of the adjectives were randomized across trials. For faces that were correctly classified as “shown,” the trait originally paired with that face was included among the 16 options and appeared at a randomly selected position. The remaining 15 buttons were filled with traits that had been paired with other faces during the learning phase. For distractor faces that were incorrectly classified as “shown,” all 16 buttons were filled with randomly selected traits from the learning phase. After selecting one of the traits, participants were shown a blank screen for 500 ms before the next recognition trial began.

#### Control measures and debriefing
After the test phase, participants completed three control measures, all of which served as exclusion criteria. First, participants were presented with a screen containing four sentences displayed as a single paragraph. The first three sentences were intentionally long and convoluted and contained general statements about attitude research, designed to discourage participants from reading the entire text. In the final sentence, participants were instructed to ignore the subsequent question about their exercise habits by not selecting any of the presented response options, thereby demonstrating that they had read the entire passage.

On the next screen, participants were shown a list of seven physical activities and were asked to indicate which of these activities they regularly engage in by selecting the corresponding checkboxes. After making their selections (or selecting none), participants proceeded by clicking a “Continue” button displayed at the bottom of the screen.

Subsequently, participants reported whether they had paid attention to the non-words and images presented throughout the experiment and whether they had taken the requested responses seriously. These questions were identical to those used in Experiments 1 and 2. Participants were then given the opportunity to provide open-ended comments about the study. Finally, participants were debriefed about the purpose of the experiment and redirected to Prolific.

### Data processing, statistical analyses and predictions
#### CS evaluations
As specified in the preregistration, participants who provided the same response on every trial of the CS evaluation task ($n=2$) were excluded from all analyses. All preregistered analyses were conducted without deviations.

Individual EC effects were computed by subtracting the mean evaluative rating of negatively paired CSs from the mean evaluative rating of positively paired CSs. To test the predicted valence-focus effect, mean EC effects in the valence focus condition were compared to those in the age focus condition using an independent-samples t test ($\alpha=.05$). Because the preregistered hypothesis specified a larger EC effect in the valence-focus condition and the data showed a difference in the predicted direction, this comparison was conducted as a one-sided test. In addition, mean EC effects within each task focus condition were tested against zero using one-sample t tests ($\alpha=.05$). In the valence-focus condition, a one-sided test was performed because a positive EC effect was predicted. In the age-focus condition, the mean EC effect was below zero; accordingly, a two-sided test was conducted.

As preregistered, we also analyzed the CS evaluations using a 2 (US valence: positive vs. negative) $\times$ 2 (task focus: valence focus vs. age focus) mixed ANOVA. To streamline the main text, results of this analysis are reported in the online supplement (see OSF repository).

Following the approach used in Experiment 2, we conducted an additional, non-preregistered analysis to examine whether EC effects depended on whether a CS was classified as old or new in the recognition task. To this end, evaluative ratings for CSs were analyzed using a hierarchical linear model with US valence, recognition response (old vs. new), and task focus as categorical predictors. The model included all main effects and interactions. To account for individual differences in evaluative responding, random intercepts and random slopes for US valence were specified at the participant level.

#### Memory data
All preregistered analyses were conducted without deviations. Responses from the two-step memory task were recoded into a joint memory index using the same coding scheme as in the previous experiments (see above). The resulting frequency distribution of the joint memory index, aggregated across participants, was analyzed using a Bayesian implementation of the MPT model with the R packages *TreeBUGS* (Heck, Arnold, & Arnold, 2018) and *MPTmultiverse* (Singmann, Heck, Barth, & Aust, 2020; see also Singmann et al., 2024). For all MPT parameters, we used default Beta priors with $\alpha = \beta = 1$, corresponding to uniform prior distributions over the unit interval. These priors were applied to the unrestricted 10-parameter model as well as to all nested model variants.

```{r}
models <- readRDS(file.path(study_folders$wsw3_main, "model-objects", "simple-mpt.rds"))
BFs    <- readRDS(file.path(study_folders$wsw3_main, "model-objects", "simple-mpt-BFs.rds")) |>
  within({
    posterior <- pmax(posterior, .Machine$double.eps)
  })

df <- apa_print(models$baseline)

parameter_labels <- c(
  a = "$a$"
  , b = "$b$"
  , C = "$C$"
  , D = "$D$"
  , d = "$d$"
)
group_labels <- c(
  x1   = "Age"
  , x2 = "Valence"
)

df$parameter <- parameter_labels[gsub(attr(df, "sanitized_term_names"), pattern = "_.*$", replacement = "")]
df$group <- group_labels[gsub(attr(df, "sanitized_term_names"), pattern = ".*_", replacement = "")]
df$term <- NULL
df_wide <- tidyr::pivot_wider(
  df
  , values_from = c("estimate", "conf.int")
  , names_from = "group"
  , names_vary = "slowest"
) |>
  label_variables(parameter = "Parameter")
df_wide <- df_wide[c(5, 3, 4, 1, 2), ]

# Compare baseline model with other models
BFs_print <- data.frame(
  model = names(BFs$posterior[-1L])
  , logBF =  log(BFs$posterior[[1L]]) - log(BFs$posterior[-1L])
  , row.names = NULL
) |> within({
  strong_evidence <- ifelse(abs(logBF) > log(10), "yes", "no")
  BF_01 <- exp(-logBF)
  BF_10 <- exp( logBF)
  BF_01 <- ifelse(BF_01 > 1000, "> 1,000", apa_num(BF_01, digits = 2L))
  BF_10 <- ifelse(BF_10 > 1000, "> 1,000", apa_num(BF_10, digits = 2L))
})

BF_10 <- split(BFs_print, BFs_print$model) |>
  lapply(function(x) {
    paste0("$\\mathit{BF}_{10} ", papaja::add_equals(x$BF_10), "$")
  })
BF_01 <- split(BFs_print, BFs_print$model) |>
  lapply(function(x) {
    paste0("$\\mathit{BF}_{01} ", papaja::add_equals(x$BF_01), "$")
  })

model_fit <- apa_fit(models$baseline)


```

We first fitted the preregistered 10-parameter model, in which joint $D$, $C$, $d$, $a$, and $b$ parameters were estimated separately for the two task focus conditions. Model fit was assessed using the posterior-predictive fit statistic $T_1$ (Klauer, 2010) together with its corresponding posterior-predictive p value. The unrestricted 10-parameter model provided an adequate fit to the data, `r model_fit$T1`.

Next, we tested each MPT parameter against its neutral value (0 for the $D$, $C$, and $d$ parameters; .5 for the $a$ and $b$ parameters). To this end, we fitted ten restricted 9-parameter models, each imposing a different parameter constraint (e.g., $D_{valence}=0$). Each restricted model was then compared to the unrestricted 10-parameter model using Bayes factors.

We subsequently tested the effect of task focus on each MPT parameter ($D$, $C$, $d$, $a$, and $b$). To do so, we fitted an additional set of five 9-parameter models, each including a different equality constraint across task focus conditions: (1) $D_{valence}=D_{age}$, (2) $C_{valence}=C_{age}$, (3) $d_{valence}=d_{age}$, (4) $a_{valence}=a_{age}$, and (5) $b_{valence}=b_{age}$. As before, each restricted model was compared to the unrestricted 10-parameter model using Bayes factors.

In line with the valence-focus account introduced above, we expected the $d$ parameter to be larger in the valence-focus condition than in the age-focus condition. Based on findings from a pilot study assessing the suitability of the present materials and procedures, we further expected larger $D$ and $C$ parameters in the valence-focus condition than in the age-focus condition, as well as a larger $b$ parameter in the age-focus condition than in the valence-focus condition. No effect of task focus was expected for the $a$ parameter.

As in Experiments 1 and 2, we examined the relationships between MPT parameters and individual EC effects using a joint-modeling approach implemented in *Stan* [@carpenter_stan_2016]. Specifically, we estimated a joint model comprising (1) participant-level frequency distributions of the joint memory index and (2) individual EC effects, with task focus included as a categorical predictor in both parts of the model. To increase statistical power, the joint model was estimated using pooled data from the present experiment and the pilot study, which employed identical materials and procedures. The combined data set included 161 participants, none of whom gave the same response on all trials of either task. Corresponding analyses conducted separately for each study are reported in the online supplement (see OSF repository).

As in the previous experiments, relationships between participant-level MPT parameter estimates and EC effects were evaluated by computing Bayes factors for the regression slopes in the linear-regression component of the joint model. To assess mediation of the valence-focus effect on evaluative conditioning, we additionally computed Bayes factors for the direct effect of task focus and for its indirect effects via the five MPT parameters ($D$, $C$, $d$, $a$, and $b$), using the independent-paths method proposed by @liu_bayesian_2023.

## Results
### EC effects

```{r}
exp3_ratings <- data_list_exp3$rating
exp3_ratings_sd <- aggregate(exp3_ratings,evaluative_rating ~ sid, FUN = sd)
exp3_ratings_sd <- subset(exp3_ratings_sd, evaluative_rating > 0)
exp3_ratings <- subset(exp3_ratings,sid %in% exp3_ratings_sd$sid)
exp3_ratings_wide <- subset(data_list_exp3$rating_wide,sid %in% exp3_ratings_sd$sid)
```

```{r}
exp3_ec_anova <- apa_print(aov_ez(data = exp3_ratings_wide, id = "sid", dv = "ec_effect", between=c("task_focus"),anova_table = list(intercept=TRUE)),intercept=TRUE,estimate="ges")

mean_ec <- paste0("$M_{EC}=",round(mean(exp3_ratings_wide$ec_effect),2),"$")
sd_ec <- paste0("$SD_{EC}=",round(sd(exp3_ratings_wide$ec_effect),2),"$")

exp3_ec_t <- apa_print(t.test(data = exp3_ratings_wide, ec_effect ~ task_focus,alternative="less"))

valfocus <- subset(exp3_ratings_wide,task_focus=="valence")
agefocus <- subset(exp3_ratings_wide,task_focus=="age")
t.val <- apa_print(t.test(valfocus$ec_effect,mu=0,alternative="greater"))
t.age <- apa_print(t.test(agefocus$ec_effect,mu=0))

mean_val <- paste0("$M_{EC}=",round(mean(valfocus$ec_effect),2),"$")
sd_val <- paste0("$SD_{EC}=",round(sd(valfocus$ec_effect),2),"$")

mean_age <- paste0("$M_{EC}=",round(mean(agefocus$ec_effect),2),"$")
sd_age <- paste0("$SD_{EC}=",round(sd(agefocus$ec_effect),2),"$")

```

The mean EC effect in the valence-focus condition was significantly greater than zero, `r t.val$full_result`, whereas the mean EC effect in the age-focus condition did not differ significantly from zero, `r t.age$full_result`. As predicted, the mean EC effect in the valence-focus condition was significantly larger than the mean EC effect in the age-focus condition, `r exp3_ec_t$full_result`.

The 2 (US valence) $\times$ 2 (recognition response) $\times$ 2 (task focus) hierarchical linear model revealed a significant main effect of US valence, `r apa_lme_model_x3$full_result$us_valence1`, and a significant main effect of recognition response, `r apa_lme_model_x3$full_result$reco_resp1`. These effects were qualified by significant interactions between US valence and recognition response, `r apa_lme_model_x3$full_result$us_valence1_reco_resp1`, between US valence and task focus, `r apa_lme_model_x3$full_result$us_valence1_task_focus_x1`, and by a significant three-way interaction between US valence, recognition response, and task focus, `r apa_lme_model_x3$full_result$us_valence1_reco_resp1_task_focus_x1`. All remaining effects were non-significant (all *p*s$\geq.522$).

Follow-up contrasts showed that CSs classified as old exhibited reliable EC effects in the valence-focus condition, `r con_vec_x3$full_result$old_valence_sme`, but not in the age-focus condition, `r con_vec_x3$full_result$old_age_sme`. For CSs classified as new, no EC effects emerged in either the valence-focus condition, `r con_vec_x3$full_result$new_valence_sme`, or the age-focus condition, `r con_vec_x3$full_result$new_age_sme`.

### MPT model analyses

```{r exp3-param}
models <- readRDS(file.path(study_folders$wsw3_main, "model-objects", "simple-mpt.rds"))
BFs    <- readRDS(file.path(study_folders$wsw3_main, "model-objects", "simple-mpt-BFs.rds")) |>
  within({
    posterior <- pmax(posterior, .Machine$double.eps)
  })


df <- apa_print(models$baseline)

parameter_labels <- c(
  a = "$a$"
  , b = "$b$"
  , C = "$C$"
  , D = "$D$"
  , d = "$d$"
)
group_labels <- c(
  x1   = "Age"
  , x2 = "Valence"
)

df$parameter <- parameter_labels[gsub(attr(df, "sanitized_term_names"), pattern = "_.*$", replacement = "")]
df$group <- group_labels[gsub(attr(df, "sanitized_term_names"), pattern = ".*_", replacement = "")]
df$term <- NULL
df_wide <- tidyr::pivot_wider(
  df
  , values_from = c("estimate", "conf.int")
  , names_from = "group"
  , names_vary = "slowest"
) |>
  label_variables(parameter = "Parameter")
df_wide <- df_wide[c(5, 3, 4, 1, 2), ]

# Compare baseline model with other models
BFs_print <- data.frame(
  model = names(BFs$posterior[-1L])
  , logBF =  log(BFs$posterior[[1L]]) - log(BFs$posterior[-1L])
  , row.names = NULL
) |> within({
  strong_evidence <- ifelse(abs(logBF) > log(10), "yes", "no")
  BF_01 <- exp(-logBF)
  BF_10 <- exp( logBF)
  BF_01 <- ifelse(BF_01 > 1000, "> 1,000", apa_num(BF_01, digits = 2L))
  BF_10 <- ifelse(BF_10 > 1000, "> 1,000", apa_num(BF_10, digits = 2L))
})

BF_10 <- split(BFs_print, BFs_print$model) |>
  lapply(function(x) {
    paste0("$\\mathit{BF}_{10} ", papaja::add_equals(x$BF_10), "$")
  })
BF_01 <- split(BFs_print, BFs_print$model) |>
  lapply(function(x) {
    paste0("$\\mathit{BF}_{01} ", papaja::add_equals(x$BF_01), "$")
  })

apa_table(
  cbind(df_wide, BF_10 = BFs_print[1:5, "BF_10"]) |>
    label_variables(BF_10 = "$\\mathit{BF}_{10}$")
  , col_spanners = list(
    "Age focus"       = c(2, 3)
    , "Valence focus" = c(4, 5)
  )
  , caption = "Parameter estimates from Experiment 3. Posterior means and 95\\% credible intervals. Bayes Factors for difference"
  , escape = FALSE
  , align = "c"
)
```

Parameter estimates and credible intervals from the unrestricted 10-parameter model are reported in Table \@ref(tab:exp3-param). We found very strong evidence for above-zero $D$ and $C$ parameters in both task focus conditions, for an above-zero $d$ parameter in the valence-focus condition, and for a departure from $.5$ for the $b$ parameter in the age-focus condition (allall $BF_{10}>1,000$). In contrast, there was strong evidence against an above-zero $d$ parameter in the age-focus condition ($BF_{10} = 0.06$) and against departures from $.5$ for the $a$ parameter in both task focus conditions (both $BF_{10}\leq0.08$). Finally, we found moderate evidence against a departure from $.5$ for the $b$ parameter in the valence-focus condition ($BF_{10} = 0.17$).

Table \@ref(tab:exp3-param) also reports Bayes Factors for the effects of task focus on the five MPT parameters. We found very strong evidence for task focus effects on the $D$, $C$, $d$, and $b$ parameters, and strong evidence against a task focus effect on the $a$ parameter. As expected, the $D$, $C$, and $d$ parameters were larger in the valence-focus condition than in the age-focus condition. Contrary to our expectation---and in contrast to the pilot study---the $b$ parameter was also larger in the valence-focus condition than in the age-focus condition.

### Relationships between EC effects and MPT parameters

(ref:exp3-regression-label) Experiment 3: Relationships between marginal EC effects and participant-level MPT parameters estimates from the unrestricted 5-parameter model. Points represent posterior means, error bars represent 95% credible intervals. Lines represent linear regression functions, dashed lines represent corresponding 95% credible bands.

```{r exp3-regression, fig.cap = "(ref:exp3-regression-label)", fig.width = 11, fig.height = 8, out.width="100%"}
model <- readRDS(file.path(study_folders$wsw3_joint, "model-objects", "treestan.rds"))

par(mfrow = c(2, 3))
plot_regression(model, pars = c("D"))
legend(
  x = .02
  , y = 5.98
  , legend = c("Age focus", "Valence focus")
  , pch = 21
  , col = "black"
  , pt.bg = 1:2
  , bty = "n"
)
plot_regression(model, pars = c("C", "d", "a", "b"))

# Bayes factors for slope parameters
BFs <- apa_print(bayes_factors(model, pars = "lm_beta", prior_mean = 0, prior_sd = 2))
apa_model_lm <- apa_print(model, part = "lm")
```

Figure \@ref(fig:exp3-regression) depicts the relationships between participant-level MPT parameter estimates and marginal EC effects. The linear regression component of the joint model provided very strong evidence for a positive association between participant-level $d$ parameter estimates and individual EC effects (`r apa_model_lm$full_result$d`). For the $D$ and $b$ parameters, the analyses yielded moderate evidence against associations with individual EC effects ($D$: `r apa_model_lm$full_result$D`; $b$: `r apa_model_lm$full_result$b`). For the $C$ and $a$ parameters, the evidence was inconclusive, providing neither clear support for nor against relationships with individual EC effects ($C$: `r apa_model_lm$full_result$C`; $a$: `r apa_model_lm$full_result$a`).

```{r}
BF_a <- subset(bayes_factors(model, pars = "beta", prior_sd = 1), subset = term == "task_focus1")
BF_b <- bayes_factors(model, pars = "lm_beta", prior_sd = 2)
nm <- BF_b$parameter

source(file.path(project_root, "R", "liu_et_al_2022.R"))
indirect_effects <- mapply(
  BF.a   = setNames(BF_a$BF_10, nm)
  , BF.b = setNames(BF_b$BF_10, nm)
  , FUN = pathb.a
  , MoreArgs = list(PriorOdds.a = 1, PriorOdds.b = 1)
  , SIMPLIFY = FALSE
) |>
  lapply(function(x){
    BF_10 <- x["Mediation", "Bayes Factor"]
    
    BF    <- ifelse(BF_10 > 1, BF_10, 1 / BF_10)
    label <- ifelse(BF_10 > 1, "\\mathit{BF}_{10} ", "\\mathit{BF}_{01} ")
    
    BF <- ifelse(
      BF > 1000
      , "> 1,000"
      , papaja::apa_num(BF, add_equals = TRUE)
    )
    paste0("$", label, BF, "$")
  })

direct_effects <- paste0(
  "$\\mathit{BF}_{01} = "
  , apa_num(bayes_factors(model, pars = "lm_alpha_tilde", prior_sd = 1)$BF_01[2])
  , "$"
)

# rstan::extract(model, pars = "lm_beta_star")[[1]] |> apply(MARGIN = 2, quantile, probs = c(.025, .975))
```

n the multiple mediation analysis, we obtained very strong evidence for an indirect effect of task focus on EC via the $d$ parameter (`r indirect_effects$d`). In contrast, there was moderate evidence against an indirect effect via the $b$ parameter (`r indirect_effects$b`). For indirect effects via the remaining MPT parameters, the evidence was inconclusive, with $BF_{10}$ ranging from $0.59$ to $2.26$. Finally, we found weak evidence against a direct effect of task focus on EC (`r direct_effects`).

```{r}
apa_table(
  list(
    "Effects of task focus manipulation on MPT parameters" = apa_print(model, "mpt")$table
    , "Standardized regression coefficients" = apa_print(model, "lm")$table
  )
  , escape = FALSE
)

```

## Discussion
Experiment 3 was designed to address a focused theoretical question: Does US valence memory constitute an independent form of CS--US pairing memory that depends on a valence-focused mode of processing during learning? To answer this question, we combined a close replication of Gast and Rothermund’s (2011) task focus manipulation with a two-step memory task and an adapted MPT model that disentangles CS recognition, US identity memory, US valence memory, and guessing processes. Replicating the core finding reported by Gast and Rothermund (2011), we observed a reliable EC effect in the valence-focus condition, whereas no such effect emerged in the age-focus condition. This result confirms that directing attention to evaluative aspects of CS--US pairings during learning strengthens subsequent evaluative responses to the CSs.

More centrally, the present experiment extends this finding to the level of memory representations. Using the adapted MPT model, we found that the $d$ parameter---measuring US valence memory in the absence of US identity memory---was substantially larger in the valence-focus condition than in the age-focus condition. Importantly, going beyond previous work, the present analyses provided strong evidence for the absence of US valence memory in the age-focus condition, as indicated by Bayes Factors favoring a zero $d$ parameter. This pattern shows not merely that US valence memory is enhanced by valence-focused processing, but that it fails to reliably emerge when attention is directed toward a non-evaluative stimulus dimension. Taken together, these results provide strong support for the view that US valence memory is not an automatic by-product of CS--US pairing, but instead depends on how the pairings are processed during learning. When CS--US pairings are encoded in terms of valence, participants form abstract valence-based representations that can later be retrieved independently of US identity. When learning is guided by a non-evaluative processing goal, such representations appear to be largely absent.

The present findings also clarify the role of different memory representations in mediating the effect of valence focus on evaluative conditioning. Using participant-level MPT parameters as competing mediators, we found strong evidence that the effect of task focus on EC was mediated by US valence memory, as captured by the $d$ parameter. In contrast, there was no corresponding evidence for mediation via US identity memory or CS recognition. This result conceptually replicates the mediation pattern reported by Gast and Rothermund (2011), but does so under substantially stricter measurement conditions. By separating US valence memory from US identity memory and from guessing tendencies, the present analyses provide a more compelling test of whether US valence memory plays a unique functional role in evaluative change. The findings suggest that valence-focused processing influences EC primarily by enabling the formation and retrieval of valence-based memory representations, rather than by strengthening memory for specific US identities or by improving stimulus recognition more generally.

In addition to its effects on US valence memory, task focus also influenced other components of memory. Both the $D$ parameter (CS recognition) and the $C$ parameter (US identity memory conditional on CS recognition) were substantially larger in the valence-focus condition than in the age-focus condition. Because the same CSs and USs were used in both conditions, these differences must be attributed to differences in processing during learning rather than to stimulus properties. One possible explanation is that the age-focus task imposed higher cognitive demands than the evaluative judgment task, thereby leaving fewer resources available for encoding stimulus details. Another possibility is that valence-focused processing involves deeper or more self-relevant processing, consistent with classic levels-of-processing accounts (Craik & Lockhart, 1972; Craik, 2002). From this perspective, the valence-focus manipulation may have increased memory performance more generally, affecting CS recognition and US identity memory in addition to enabling US valence memory. Regardless of the precise mechanism, these findings underscore the importance of separately measuring different memory components, as changes in evaluative learning can be accompanied by changes in multiple forms of memory.

Before we discuss the broader implications of the present research in the General Discussion, we address the inconsistent effects of task focus on the $b$ parameter, which captures a bias toward responding “old” when recognition memory is absent. In the pilot study, we observed a larger $b$ parameter in the age-focus condition than in the valence-focus condition, consistent with the idea that participants may compensate for lower recognition memory by guessing “old” more frequently. Based on this rationale, we had expected a similar pattern in the present experiment. Contrary to this expectation, the aggregated data in Experiment 3 showed the opposite pattern, with a larger $b$ parameter in the valence-focus condition. Given that the same materials, instructions, and procedures were used across studies, we considered the possibility that these discrepancies reflect statistical artefacts arising from the aggregation of response frequencies across participants. To evaluate this possibility, we estimated joint models at the participant level separately for the pilot study and the present experiment. These analyses did not provide evidence for substantial or reliable task focus effects on the $b$ parameter. On this basis, we tentatively conclude that the inconsistent $b$ parameter effects observed in analyses of aggregated data are likely artefactual and do not reflect stable differences in response tendencies across task focus conditions. This interpretation remains provisional and highlights the value of hierarchical modeling approaches for distinguishing genuine psychological effects from aggregation-induced distortions.

# General discussion
Attitudes and evaluations are typically shaped by past experiences with their objects. Encounters that are positive or negative can leave traces in memory that later inform how objects are judged, even when the original context is no longer present. Importantly, such evaluative judgments need not rely on a single type of memory representation: they may be supported by relatively detailed recollections of specific episodes, or by more abstract representations that capture only the affective meaning of prior experiences.

The nature of these underlying memory representations is likely to matter for how evaluations are expressed, maintained, and changed. More detailed representations may allow evaluations to be revised in light of new information, whereas more abstract representations may support stable judgments that are less dependent on the accessibility of specific details. Understanding which forms of memory are involved in evaluative judgments is therefore central to explaining when and how evaluations arise from experience.

Evaluative conditioning provides a useful paradigm for addressing these questions. In EC procedures, initially neutral stimuli are paired with positive or negative stimuli during learning, and are subsequently evaluated when presented alone at test. Although reliable differences in evaluation are commonly observed, there is ongoing debate about what information is retrieved from memory to support these judgments. In particular, many accounts assume that memory for the valence of the unconditioned stimulus is sufficient to drive evaluative conditioning effects, even in the absence of memory for the specific stimulus with which it was paired. However, direct evidence for this assumption has been difficult to obtain, largely because standard memory measures confound different forms of memory with guessing and response biases. As a result, it has remained unclear whether US valence memory constitutes an independent form of memory, whether it reliably predicts EC effects when measured without confounds, and under which conditions such abstract valence representations are formed in the first place.

The present research addressed these questions by introducing an adapted multinomial processing tree (MPT) model as a tool for disentangling CS recognition memory, US identity memory, US valence memory, and guessing processes in EC paradigms. Across three experiments, we applied this model to EC procedures that varied in materials, task order, and processing demands, allowing us to examine both the measurement properties of the model and the substantive assumptions about memory representations underlying evaluative conditioning.

In Experiment 1, we provided an initial test of whether the adapted MPT model yields a coherent and interpretable decomposition of EC-related memory processes and whether US valence memory can be demonstrated independently of US identity memory. Using faces as CSs, we found evidence for US valence memory in the absence of US identity memory, as reflected in an above-zero $d$ parameter. Although the EC effect itself was small and statistical evidence for parameter–EC relationships was limited, the results suggested that US valence memory can exist as a distinct memory representation and may be more closely related to evaluative change than US identity memory.

Experiment 2 subjected this pattern to a more stringent test by increasing statistical power, using nonwords as CSs, and varying the order of the memory and evaluation tasks. At the individual level, EC effects were again clearly related to US valence memory but not to US identity memory, providing stronger evidence for the predictive role of the $d$ parameter than in Experiment 1. At the same time, US valence memory did not reliably differ from zero at the group level, indicating that such abstract valence representations are not formed uniformly across participants or experimental contexts. This dissociation pointed to the possibility that additional factors determine whether US valence memory emerges during learning.

Experiment 3 directly addressed this issue by manipulating processing focus during learning. Replicating and extending prior work by Gast and Rothermund (2011), we found that evaluative conditioning was present only when participants processed the CS--US pairings with an explicit focus on valence. Crucially, using the adapted MPT model, we showed that this valence focus increased US valence memory in the absence of US identity memory and that only this form of memory mediated the effect of processing focus on evaluative change. In contrast, when participants focused on a non-evaluative dimension, we found strong evidence for the absence of US valence memory, despite reduced, but intact CS recognition and US identity memory.

In the remainder of the General Discussion, we elaborate on the theoretical implications of these findings for accounts of evaluative conditioning, discuss the methodological contributions and limitations of the present modeling approach, and outline directions for future research on memory representations in attitude formation.

## Theoretical implications
### US valence memory as an independent memory representation
Across experiments, the results provide strong support for the idea that memory for US valence can exist as an independent form of memory, separable from memory for US identity. Using the adapted MPT model that disentangles CS recognition, US identity memory, US valence memory, and guessing processes, we found clear evidence that participants sometimes retrieved evaluative information about the US in the absence of memory for its specific identity. This pattern was observed under measurement conditions that avoid the inferential confounds inherent in standard US identity tasks and valence recoding procedures. Taken together, these findings strengthen the claim that US valence memory is not merely a byproduct of US identity retrieval or post-hoc inference, but constitutes a distinct representational format that can support evaluative judgments.

### Differential robustness of US identity and US valence memory
At the same time, the present experiments reveal an important asymmetry in the robustness of different memory components. US identity memory (and CS recognition memory) was consistently observed across materials, task orders, and processing conditions. By contrast, US valence memory was less robust and varied across experimental conditions,despite comparable levels of CS recognition and US identity memory. Crucially, Experiment 3 provides direct evidence that an evaluative mode of processing during learning plays a central role in its formation: When participants were explicitly instructed to evaluate CS--US pairings in terms of valence, US valence memory reliably emerged and mediated the corresponding increase in EC effects, whereas no such memory was observed under a non-evaluative processing focus.

Interpreted in light of this finding, the presence of US valence memory in Experiment 1---despite the absence of explicit evaluative instructions---suggests that evaluative processing may also arise spontaneously when stimulus materials invite impression formation, such as human faces. By contrast, the mixed pattern observed in Experiment 2, in which US valence memory predicted individual EC effects without reliably emerging at the group level, is consistent with the idea that evaluative processing occurred only for a subset of participants. Together, these results indicate that US valence memory is not an obligatory consequence of CS--US pairing, but emerges preferentially when learning engages evaluative processing, whether induced by task demands, stimulus characteristics, or spontaneous processing strategies.

### Relations between memory components and evaluative conditioning
Another central implication concerns the mechanisms through which evaluative conditioning effects are mediated. Across experiments, US valence memory emerged as the only reliable predictor and mediator of EC effects when assessed with a measurement approach designed to avoid the inferential ambiguities of standard memory measures. This pattern conceptually replicates prior claims that US valence memory plays a privileged role in evaluative conditioning, while placing those claims on firmer measurement grounds.

By contrast, several findings argue against a causal role of US identity memory in driving EC effects. First, US identity memory did not reliably predict individual EC effects when US valence memory was modeled simultaneously. Crucially, this pattern cannot be explained by shared variance arising from valence inferences based on remembered US identities or by guessing processes that disproportionately inflate associations with valence-based measures. Because the adapted MPT model estimates US identity memory, US valence memory, and response tendencies as separable latent components, the absence of a relationship between US identity memory and EC reflects a genuine lack of predictive power rather than a measurement artifact.

A second line of evidence comes from dissociations between memory and evaluation: EC effects were often absent despite clear evidence for US identity memory, as observed in Experiment 1 and in the age-focus condition of Experiment 3. Together, these findings suggest that evaluative judgments are not typically formed by retrieving specific US identities and subsequently inferring their valence at test. Instead, evaluative information appears to be retrieved in a more direct and abstract form.

Importantly, this conclusion should not be misunderstood as implying that evaluative conditioning generally occurs in the absence of US identity memory. By design, the $d$ parameter captures instances in which US valence is retrieved without concurrent US identity retrieval, but this does not imply that valence-based representations are absent when US identity information is available. Rather, evaluative information may often be encoded and retrieved alongside identity information, with the MPT model isolating the subset of cases in which valence retrieval occurs independently of identity retrieval. Accordingly, the present results should be taken to speak to the causal role of US valence memory in evaluative conditioning, not to the exclusivity of particular representational formats.

### Implications for theories of evaluative conditioning and attitude formation
The present findings have direct implications for theoretical accounts of evaluative conditioning and, more broadly, for theories of attitude formation. Across experiments, EC effects were systematically linked to the retrieval of evaluative information from memory and depended on how CS--US pairings were processed during learning. This pattern supports views of EC as a memory-based and goal-dependent phenomenon rather than as the inevitable outcome of automatic associative processes.

Most importantly, the results challenge accounts that assume that evaluative conditioning arises automatically from the repeated co-occurrence of CSs and valenced USs, independent of how the pairings are processed. If EC were driven primarily by automatic associative mechanisms, then EC effects should emerge reliably across learning conditions. Instead, US valence memory proved sensitive to processing demands: It was robustly observed when learning involved an evaluative focus (Experiment 3), appeared only for a subset of participants under neutral instructions (Experiment 2), and may have been facilitated by stimulus materials that naturally invite evaluative processing (faces in Experiment 1). This context dependence is difficult to reconcile with strong automaticity assumptions but follows naturally from accounts that emphasize encoding goals, task demands, and representational formats.

Accordingly, the present findings are more compatible with propositional and memory-based theories of evaluative conditioning. These accounts assume that EC reflects the formation and later retrieval of evaluative knowledge about CS--US relations and allow learning outcomes to depend on how stimuli are interpreted and encoded during learning. From this perspective, evaluative conditioning does not require detailed contingency memory or deliberate inference at test, but it does presuppose that evaluative information was extracted and stored in memory during learning. The present results suggest that such extraction is facilitated by evaluative processing goals.

## Methodological contributions
A central contribution of the present research lies in overcoming several long-standing limitations of standard memory measures used in evaluative conditioning research. Traditional approaches have made it difficult to draw clear conclusions about the presence, absence, and functional relevance of different memory representations because multiple processes---recognition, identity memory, valence memory, and guessing---are confounded at the level of observed responses. The adapted MPT model provides a principled solution by disentangling these components at the level of latent processes.

First, the MPT modeling approach ensures that indications of US valence memory cannot be driven by inferences from remembered US identities. In standard procedures, US valence memory is often inferred by recoding identity responses according to their valence, making it impossible to determine whether apparent valence memory reflects an abstract valence representation or simply the evaluative interpretation of retrieved identity information. In the present approach, US valence memory is estimated independently of US identity memory, such that evidence for valence memory cannot be explained by successful identity retrieval.

A second advantage of the MPT modeling approach is that estimates of US identity memory are not inflated by valence-based response strategies. In standard identity memory tasks, participants who remember only the valence of a US can restrict their choice to valence-consistent response options, increasing the probability of selecting the correct US identity even in the absence of true identity memory. In the adapted MPT model, such valence-guided guessing is explicitly separated from US identity memory, such that the corresponding parameter reflects retrieval of specific US information rather than performance gains driven by US valence memory.

Third, the present approach ensures that indications of both US identity memory and US valence memory are not confounded with CS recognition memory. Standard EC memory measures implicitly assume that all CSs are recognized as previously encountered, even though recognition may vary across stimuli and experimental conditions. By explicitly modeling CS recognition as a distinct component, the present approach allows researchers to determine whether apparent differences in contingency memory reflect changes in CS--US associations or simply differences in recognizing the CSs themselves.

A final advantage of the present approach is that all memory components are separated from guessing processes. Guessing “old” responses is modeled independently of CS recognition; guessing US valence is modeled independently of US valence memory; and correct US guesses are modeled independently of US identity memory. As a result, correct responses are not automatically treated as evidence for memory unless they systematically exceed what would be expected from guessing alone.

Together, these features substantially improve the interpretability of memory measures in evaluative conditioning. They allow researchers to demonstrate unambiguously whether US identity memory and US valence memory are present or absent, to determine whether moderators of EC operate via changes in CS recognition, US identity memory, or US valence memory, and to examine the relative predictive power of these components in regression or mediation analyses without bias. Crucially, because guessing processes are explicitly modeled and separated from memory, associations between US valence memory and EC effects are not artificially favored by shared response tendencies or correlated guessing across tasks. By resolving these methodological ambiguities, the present approach provides a stronger foundation for future research on the memory mechanisms underlying evaluative conditioning and attitude formation.

## Limitations and model extensions
### Participant-level modeling rather than item-level analysis
The present research relied on participant-level MPT modeling to estimate memory performance. That is, the model parameters reflect probabilities defined across stimuli---for example, the $D$ parameter corresponds to the probability that a participant recognizes a CS as old. This participant-level approach is well suited for addressing questions about the relationship between the likelihood of retrieving particular types of information from memory and the magnitude of evaluative conditioning effects.

At the same time, the present approach does not permit inferences at the item level. Specifically, it does not allow conclusions about whether the evaluative response to a particular CS on a given trial was based on retrieval of US identity, retrieval of US valence in the absence of identity, or no memory retrieval at all. The inability to make stimulus-specific inferences may be viewed as a limitation, particularly in light of arguments that item-level measures of awareness or memory can be especially informative in the context of evaluative conditioning (e.g., Pleyers et al., 2007; Stahl et al., 2023).

Importantly, however, the participant-level MPT approach adopted here was deliberately chosen to address interpretative problems inherent to item-level analyses. In item-level memory tasks, responses on individual trials are typically classified dichotomously, such that correct responses are treated as evidence for the presence of memory, whereas incorrect responses are treated as evidence for its absence. This practice offers no safeguards against guessing: a correct response may reflect genuine memory retrieval, but it may equally result from chance or informed guessing, whereas an incorrect response may occur despite partial or degraded memory. As a result, item-level classifications usually provide ambiguous indicators of whether a specific type of information (e.g., US identity or US valence) was actually retrieved on a given trial.

By modeling memory responses across items, the MPT framework yields interpretable estimates of the probabilities with which different types of information are retrieved from memory, while explicitly separating memory processes from guessing tendencies. Although this comes at the cost of stimulus-specific inference, it provides a principled way to quantify distinct memory components and to relate them to evaluative outcomes without conflating retrieval with response strategies.

### Contingency memory conditional on CS recognition
A second limitation concerns the fact that the adapted MPT model estimates CS--US contingency memory only for CSs that are classified as old in the recognition task. In principle, participants might sometimes retrieve information about prior CS--US pairings even when they fail to classify a CS as old. From a modeling perspective, this raises the possibility that some contingency memory could go unmeasured when analyses are restricted to recognized CSs.

This concern is mitigated by the results of the hierarchical linear model analyses conducted in all three experiments. Across most conditions, reliable EC effects were observed only for CSs classified as old, whereas EC effects for CSs classified as new were not reliable. This pattern suggests a close empirical association between CS recognition and the retrieval of contingency information, lending support to the model assumption that contingency memory is primarily expressed when a CS is recognized as having occurred during learning.

One exception to this pattern was observed in Experiment 2 in the evaluation-first condition, where a small but reliable EC effect also emerged for CSs classified as new. Importantly, this effect was absent in the memory-first condition of the same experiment. This pattern can be understood by considering how weak memory signals are used under different task orders. In the memory-first condition, stimulus familiarity can only arise from prior exposure during the learning phase. As a result, all available memory signals---both strong and weak---can be exhaustively attributed to learning-phase exposure and are therefore informative for the old/new decision. Any contingency memory present can then be expressed both in the US memory task and in the subsequent evaluation task.

By contrast, in the evaluation-first condition, familiarity may arise from prior exposure during the learning or evaluation phase. When participants later perform the recognition task, weak memory signals can thus not be uniquely attributed to learning-phase exposure, because they may also stem from the prior evaluation phase. Under these conditions, participants may adopt a more conservative criterion for classifying a CS as old, relying primarily on stronger memory signals that can be clearly linked to the learning episode. As a consequence, weak memory traces may still influence evaluative judgments, while failing to support an “old” response in the recognition task. This provides a plausible explanation for the observation that, in the evaluation-first condition of Experiment 2, some CSs showed EC effects despite being classified as new. Notably, Experiment 3 also employed an evaluation-first test phase but did not show reliable EC effects for CSs classified as new, indicating that the finding from Experiment 2 is not robust across procedures.

Taken together, these results suggest that restricting the estimation of contingency memory to CSs recognized as old is largely consistent with the observed data. The current model construction therefore appears well aligned with the empirical relationship between recognition and evaluative effects. That said, future research could also extend the two-step memory task and the MPT model to explicitly assess and model contingency memory for CSs classified as new, allowing a more direct test of whether and under which conditions such memory contributes to evaluative conditioning.

### Temporal separation between evaluation and memory assessment
A further limitation of the present approach concerns the temporal separation between the evaluation task and the memory task. Depending on the experiment and task order condition, evaluative responses to the CSs were collected either before or after the memory task. As a result, it is in principle possible that the specific information retrieved from memory differs across tasks. For example, a participant might fail to retrieve US identity or valence information when evaluating a CS, but successfully retrieve this information later in the separate memory task. More generally, if memory is causally involved in evaluative conditioning, the information accessible at the moment of CS evaluation---rather than at a later time point---should be most diagnostic (e.g., Shanks & St. John, 1994; Stahl et al., 2023).

This concern is partly alleviated by the results of the hierarchical linear model analyses: EC effects were reliably stronger for CSs that were classified as old than for CSs classified as new, which often showed no reliable EC effects. This pattern suggests a close correspondence between the information that supports evaluative judgments and the information that supports memory responses, even when the two are assessed in separate tasks.

Nevertheless, future research could further strengthen the link between memory and evaluation by reducing or eliminating the temporal gap between the two measures. Building on conditional judgment procedures that assess evaluative responses and memory within a single task (e.g., Ingendahl et al., 2025; Stahl et al., 2023), participants could be asked to evaluate each presented stimulus (CS or distractor) while simultaneously indicating whether they (a) recognize the CS and remember the specific US it was paired with, (b) recognize the CS and remember only the US valence, or (c) consider the stimulus new. When participants select options (a) or (b), they could then be prompted to select the corresponding US from a set of alternatives. Such a procedure would allow evaluative judgments and memory reports to be collected at the same time while preserving the logic of the present MPT modeling approach. Extending the current model to such integrated tasks would provide an even more stringent test of the role of different memory representations in evaluative conditioning.

### Conditioned attititudes as a potential source of above-zero *d* parameters

[...]

## Future applications and outlook

[...]

# References

::: {#refs custom-style="Bibliography"}
:::

<!-- # (APPENDIX) Appendix {.unnumbered} -->

<!-- ```{r child = file.path(project_root, "Paper/appendix.rmd"), eval = TRUE} -->
<!-- ``` -->
