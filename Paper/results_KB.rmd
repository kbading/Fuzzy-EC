---
title: "Results"
date: "`r Sys.Date()`"
bibliography: '`r file.path(rprojroot::find_rstudio_root_file(), "..", "methexp.bib")`'
output: papaja::apa6_pdf
classoption: doc
editor_options:
  chunk_output_type: console
---

```{r set, include=FALSE}
#Load (or install and load) packages
require(pacman)
library(interactions)
p_load('tidyverse', 'psych', 'effectsize', 'reshape2', 'afex', 'papaja', 'kableExtra', 'MPTinR', 'TreeBUGS', 'emmeans', 'lme4', 'ggeffects') 
set_sum_contrasts()


#read the dataset we created in a previous R script
# dat = readRDS("C:/Users/benaj/OneDrive - UCL/Postdoctorat/projects_Karoline/exp2/write_manuscript/Fuzzy-EC/Paper/data/data_wsw2.RDS")

dat = readRDS("~/R/Fuzzy-EC/Paper/data/data_wsw2.RDS")

#exclude participants declaring they did not take their responses seriously
##or did not pay attention
dat = dat %>% filter(pay_attention != 0 & serious != 0) %>% droplevels() #drop level to remove excluded ppts

#some factors are integer or character variables in the dataset; make them factors
dat$url.srid = as.factor(dat$url.srid)
#dat$cs_category = as.factor(dat$cs_category)
dat$us_valence = as.factor(dat$us_valence)
dat$reco_resp = as.factor(dat$reco_resp)
```

```{r eval_change}
####
#EVALUATIVE CONDITIONING
####

#compute mean evaluative change scores for each participant as a function of US Valence 
dat_ev = dat %>%
  group_by(url.srid, us_valence) %>%
  summarize(eval_rating = mean(as.numeric(cs_rating)))

#check data distribution
# plot(density(dat_ev$eval_rating[dat_ev$us_valence=="positive"]))
# plot(density(dat_ev$eval_rating[dat_ev$us_valence=="dist"]))
# plot(density(dat_ev$eval_rating[dat_ev$us_valence=="negative"]))

#include order
dat_ev_order = dat %>%
  group_by(url.srid, us_valence, order) %>%
  summarize(eval_rating = mean(as.numeric(cs_rating)))

# knitr::kable(dat_ev_order, format = "html")

dat_ev_order$us_valence = as.factor(dat_ev_order$us_valence)
dat_ev_order$order = as.factor(dat_ev_order$order)
```

```{r eval_ratings}
##calculate ec effects (ratings pos-ratings neg)
dat_ec = dat_ev_order %>% filter(us_valence != "dist") 

dat_o_wide = dat_ec %>% pivot_wider(names_from = us_valence
                                          ,values_from = eval_rating)

dat_o_wide$ec = dat_o_wide$positive-dat_o_wide$negative

mod_ec = aov_ez(dat_o_wide
              ,id = "url.srid"
              ,dv = "ec"
              ,between = "order"
)

mod_ec_print = apa_print(mod_ec, intercept = TRUE) #significant effect

# anova(mod_ec, intercept=TRUE)
# 
# describe(dat_o_wide$ec)
# describeBy(dat_o_wide$ec, dat_o_wide$order)

dat_o_wide_mem = dat_o_wide %>% filter(order=="mem_first")
t_ec_mem = apa_print(t.test(dat_o_wide_mem$ec, mu = 0))
d_ec_mem = cohens_d(dat_o_wide_mem$ec, mu=0, ci = .9)

dat_o_wide_ev = dat_o_wide %>% filter(order=="eval_first")
t_ec_ev = apa_print(t.test(dat_o_wide_ev$ec, mu = 0))
d_ec_ev = cohens_d(dat_o_wide_ev$ec, mu=0, ci = .9)

####
mod1 = aov_ez(dat_ev
              ,id = "url.srid"
              ,dv = "eval_rating"
              ,within = "us_valence"
              ,anova_table=list(correction = "none")
)

mod1_print = apa_print(mod1) #significant effect

# apa_table(
#   mod1_print$table
#   ,caption = "Repeated-measures ANOVA: Ratings as a function of US"
# )

# Descriptive statistics: evaluative change scores as a function of US Valence
# knitr::kable(describeBy(dat_ev$eval_rating, dat_ev$us_valence, mat = TRUE), digits = 2)

#multiple comparisons
em_model = emmeans(mod1,pairwise~us_valence, adjust="bonf")

#with order
mod1_order = aov_ez(dat_ev_order
              ,id = "url.srid"
              ,dv = "eval_rating"
              ,within = "us_valence"
              ,between="order"
              ,anova_table=list(correction = "none")
              ,include_aov = TRUE
)

# mod1_order
mod1_print = apa_print(mod1_order, correction = "none") #significant effect

# anova(mod1_order, intercept = TRUE)

#no significant effect including order

#multiple comparisons
em_model_val = emmeans(mod1_order,pairwise~us_valence)
# em_model_val

d_pos_dist = cohens_d(dat_ev_order$eval_rating[dat_ev_order$us_valence=="positive"], dat_ev_order$eval_rating[dat_ev_order$us_valence=="dist"], paired = TRUE, ci = .95)

d_pos_neg = cohens_d(dat_ev_order$eval_rating[dat_ev_order$us_valence=="positive"], dat_ev_order$eval_rating[dat_ev_order$us_valence=="negative"], paired = TRUE, ci = .95)

d_neg_pos = cohens_d(dat_ev_order$eval_rating[dat_ev_order$us_valence=="negative"], dat_ev_order$eval_rating[dat_ev_order$us_valence=="dist"], paired = TRUE, ci = .95)
```

## Preregistered analyses on evaluative ratings

We report the analyses we conducted on evaluative ratings. We averaged evaluative ratings as a function of US valence (Positive, Negative, None) and Task order (Evaluation first, Memory first). For each participant, we calculated an Evaluative Conditioning (EC) score, which is their mean evaluative rating on CSs paired with positive USs minus their mean evaluative rating on CSs paired with negative USs (negative scores indicate higher evaluations on negatively paired vs. positively paired CSs; positive scores indicate higher evaluations on positively vs. negatively paired CSs).

First, we conducted a between-participants ANOVA on EC scores with Task order as the only factor. We tested whether the grand mean was above 0 by calculating the F-test of the intercept. A grand mean above 0 would indicate that, overall, we replicated the EC effect. In line with the preregistration, we divided the *p*-value of this test by two to perform a one-tailed test, as the grand mean of EC scores was above 0 ($M =$ `r round(mean(dat_o_wide$ec), 2)`; $SD =$ `r round(sd(dat_o_wide$ec), 2)`). The *F*-test was significant, `r mod_ec_print$full_result$Intercept`, showing that we replicated the EC effect. The effect of Task order was not significant^[As preregistered, we divided the *p*-value by two to perform a one-tailed version of the test (similar to *t*-tests) because EC scores are descriptively larger in the Evaluation first ($M =$ `r round(mean(dat_o_wide_ev$ec), 2)`; $SD =$ `r round(sd(dat_o_wide_ev$ec), 2)`) than Memory first ($M =$ `r round(mean(dat_o_wide_mem$ec), 2)`; $SD =$ `r round(sd(dat_o_wide_mem$ec), 2)`) condition. EC scores were above 0 both in the Evaluation first condition, `r t_ec_ev$statistic`, $d =$ `r d_ec_ev$Cohens_d`, 90\% CI = [`r d_ec_ev$CI_low`, `r d_ec_ev$CI_high`] and in the Memory first condition, `r t_ec_mem$statistic`, $d =$ `r d_ec_mem$Cohens_d`, 90\% CI = [`r d_ec_mem$CI_low`, `r d_ec_mem$CI_high`].], `r mod_ec_print$full_result$order`, which means that performing the evaluative rating task before or after the memory task did not significantly change the EC effect.

```{r plot_us_valence_order, fig.cap="Mean evaluative ratings (and 95% error bars) as a function of US valence and Task order."}
dat_ev_order$`US Valence` = dat_ev_order$us_valence

#reorder US valence
dat_ev_order$`US Valence` = factor(dat_ev_order$`US Valence`, levels = c("negative", "dist", "positive"))

#rename US valence and order levels
dat_ev_order$`US Valence` = plyr::revalue(dat_ev_order$`US Valence`, c("negative"="Negative", "dist"="None", "positive"="Positive"))

dat_ev_order$order = plyr::revalue(dat_ev_order$order, c("eval_first"="Evaluation first", "mem_first"="Memory first"))

dat_ev_order$`Task order` = dat_ev_order$order 

#visualize the data
apa_beeplot(data=dat_ev_order, id="url.srid", dv="eval_rating", factors=c("US Valence", "Task order"), use = "all.obs", ylim=c(0,11),
            xlab = "US"
            ,ylab = "Mean evaluative ratings"
            ,ylim=c(0,8))
```

Complementarily, we also conducted a 3 (US valence) x 2 (Task order) mixed ANOVA on evaluative ratings (see Figure \@ref(fig:plot_us_valence_order)). Different from the ANOVA above, evaluative ratings on unpaired nonwords can be compared with evaluations in other conditions. The main effect of US valence was significant, `r mod1_print$full_result$us_valence`. We followed-up on the ANOVA by conducting multiple comparisons (Bonferroni-corrected) based on the full model: evaluative ratings were higher for positively paired CSs compared with new non-words, $t(164) = -9.94, p < .001, d = 0.75, 95\%$ CI $= [0.58, 0.92]$, and compared with negatively-paired CSs, $t(164) = -9.61, p < .001, d = 0.74, 95\%$ CI $= [0.57, 0.91]$. Evaluative ratings were not significantly different for negatively-paired CSs and for nonwords, $t(164) = -0.39, p = .922, d = 0.02, 95\%$ CI $= [-0.13, 0.17]$. The main effect of Task order was not significant, `r mod1_print$full_result$order`, nor was the interaction between US valence and Task order, `r mod1_print$full_result$order_us_valence`.

## Preregistered analyses on memory performance

```{r}

library(TreeBUGS)
library(papaja)
library(afex)
library(ggeffects)

source(file.path(rprojroot::find_rstudio_root_file(), "R", "apa_print_treebugs.R"))

study_folder <- file.path(
  rprojroot::find_rstudio_root_file()
  , "Paper"
  , "mpt analyses"
)

models <- list(
  both_HQ    = readRDS(file.path(study_folder, "study2_both_HQ.rds"))
  , botha5_HQ = readRDS(file.path(study_folder, "study2_botha5_HQ.rds"))
  , bothb5_HQ = readRDS(file.path(study_folder, "study2_bothb5_HQ.rds"))
  , bothCneg0_HQ = readRDS(file.path(study_folder, "study2_bothCneg0_HQ.rds"))
  , bothCpos0_HQ = readRDS(file.path(study_folder, "study2_bothCpos0_HQ.rds"))
  , bothD0_HQ = readRDS(file.path(study_folder, "study2_bothD0_HQ.rds"))
  , bothdneg0_HQ = readRDS(file.path(study_folder, "study2_bothdneg0_HQ.rds"))
  , bothdpos0_HQ = readRDS(file.path(study_folder, "study2_bothdpos0_HQ.rds"))
)

fit <- lapply(models, FUN = apa_fit)

individual <- models |>
  lapply(function(x) {
    individual_fits <- x$summary$fitStatistics$individual
    individual_fits$T1.obs <- colMeans(individual_fits$T1.obs)
    individual_fits$T1.pred <- colMeans(individual_fits$T1.pred)
    cross_table <- table(individual_fits$T1.p <= .05)
    cross_table
  })

```

### Model fit and parameter estimates

```{r model-performance}
# prepare_table <- function(x) {
#   data.frame(as.list(x$summary$fitStatistics$overall))
# }
# 
# fit_stats <- lapply(models, prepare_table) |> do.call(what = "rbind")
# 
# waics <- readRDS(file.path(study_folder, "waic.rds"))[names(models)]
# 
# waic_stats <- lapply(waics, FUN = function(x){
#   data.frame(
#     waic = sum(x$waic)
#     , se_waic = sqrt(length(x$waic)) * sd(x$waic)
#   )
# }) |> do.call(what = "rbind")
# table_data <- cbind(fit_stats, waic_stats)
# table_data <- within(
#   table_data
#   , {
#     p.T1 <- apa_p(p.T1)
#     p.T2 <- apa_p(p.T2)
#   }
# )
# table_data <- apa_num(table_data)
# table_data <- t(table_data)
# table_data <- data.frame(
#   " " = c(
#     "$T_1^{\\mathrm{observed}}$", "$T_1^{\\mathrm{expected}}$", "$p$"
#     , "$T_2^{\\mathrm{observed}}$", "$T_2^{\\mathrm{expected}}$", "$p$"
#     , "$\\mathrm{WAIC}$"
#     , "$\\mathit{SE}$" # _{\\mathrm{WAIC}}$"
#   )
#   , table_data
#   , check.names = F
# )
# rownames(table_data) <- NULL
# colnames(table_data) <- gsub(colnames(table_data), pattern = "_", replacement = "")
# 
# save(table_data, file ="waid_table.rdata")

#load("waid_table.rdata")
load(file.path(study_folder, "waid_table.rdata"))

apa_table(
  table_data
  , caption = "Experiment 1: Absolute fit and WAIC for the hierarchical extensions of unrestricted and restricted variants of the who-said-what model."
  , escape = FALSE,
  landscape = TRUE,
  font_size = "scriptsize",
  stub_indents = list(
    "Goodness of fit: Means" = 1:3
    , "Goodness of fit: Covariances" = 4:6
    , "Relative predictive accuracy" = 7:8
  )
)
```

```{r exp1-param}
# MPT parameter estimates ----
df <- apa_print(summary(models$both_HQ))
# pars_list <- strsplit(df$term, split = " ", fixed = TRUE)
# df$term[] <- vapply(pars_list, FUN = `[[`, i = 1L,  FUN.VALUE = character(1L))
# df$relational_pair <- vapply(pars_list, FUN = `[[`, i = 2L, FUN.VALUE = character(1L))
# df_wide <- tidyr::pivot_wider(df, values_from = c("estimate", "conf.int"), id_cols = "term", names_from = "relational_pair")
# df_wide <- df_wide[, c("term", "estimate_asymmetrical", "conf.int_asymmetrical", "estimate_symmetrical", "conf.int_symmetrical")]
# df_wide$term[] <- c(M1 = "$R_{\\mathrm{positive}}$", M3 = "$R_{\\mathrm{negative}}$", P1 = "$C$", G = "$B$")[df_wide$term]

df <- df[c(5,4,3,7,6,2,1),]
df$term[1] <- "$D$"
df$term[2] <- "$C_{\\mathrm{positive}}$"
df$term[3] <- "$C_{\\mathrm{negative}}$"
df$term[4] <- "$d_{\\mathrm{positive}}$"
df$term[5] <- "$d_{\\mathrm{negative}}$"
df$term[6] <- "$b$"
df$term[7] <- "$a$"

xxx <- getGroupMeans(models$both_HQ,probit = FALSE)

df$est.EF <- rep(df$estimate[1],7)
df$ci.EF <- rep(df$conf.int[1],7)
df$est.MF <- rep(df$estimate[1],7)
df$ci.MF <- rep(df$conf.int[1],7)
df$p.val <- rep("$.460$",7)

variable_labels(df) <- list("term" = " ", "p.val" = "$p$ (one-sided)")

apa_table(
  df#[c(2:4, 1), ]
  , caption = "Parameter estimates (with 95\\% credible intervals) based on a hierarchical extension of the unrestricted who-said-what model with task order as categorical predictor of individual parameter estimates."
  , col_spanners = list("overall" = 2:3, "evaluation first" = 4:5,"memory first" = 6:7)
  , escape = FALSE
)

```

The who-said-what MPT model (with $D_{positive}=D_{negative}=D_{new}$ and $1/n=.25$) was fit to the whole sample.
Task order (memory first vs. evaluation first) was included as a categorical predictor of individual parameter estimates.
The model fit the data well, `r fit$both_HQ$T1`, `r fit$both_HQ$T2`, and was therefore used as the baseline model to assess loss of model fit due to parameter restrictions.
Measures of (absolute and relative) model fit for the baseline model (and restricted model variants) are reported in Table\ \@ref(tab:model-performance).
Parameter estimates based on the baseline model can be found in Table\ \@ref(tab:exp1-param).

The discrimination parameter $D$ was $.454$ on average.
A restricted who-said-what model setting the $D$ parameter to zero produced inadequate fit, `r fit$bothD0_HQ$T1`, `r fit$bothD0_HQ$T2`, and a WAIC of $11,200.27$.
The WAIC of the restricted model was more than 10 points higher than the WAIC of the baseline model ($\text{WAIC}_{baseline}=3,695.99$), indicating that the $D$ parameter cannot be set to zero without loss of model adequacy.

The $C$ parameter for positively (negatively) paired CSs was $.666$ ($.864$) on average.
A restricted who-said-what model setting $C_{positive}$ ($C_{negative}$) to zero produced inadequate fit and a WAIC of $5,094.10$ ($5,288.79$).
The WAIC values were again more than 10 points higher than the WAIC of the baseline model, indicating that neither $C_{positive}$ nor $C_{negative}$ parameter can be set to zero without loss of model adequacy.

The $d$ parameter for positively (negatively) paired CSs was $.049$ ($.118$) on average.
A restricted who-said-what model setting $d_{positive}$ ($d_{negative}$) to zero produced adequate fit and a WAIC of $3,695.01$ ($3,695.48$).
The WAIC values were almost identical to the WAIC of the baseline model, indicating that both $d_{positive}$ and $d_{negative}$ can be set to zero without loss of model adequacy.

The $b$ parameter (indicating a bias for responding "old" in the recognition task) was $.135$ on average.
A restricted who-said-what model setting the $b$ parameter to $.5$ produced inadequate fit and a $WAIC$ of $5,632.61$.
The WAIC difference between the models (restricted vs. baseline) was again larger than 10, indicating that the $b$ parameter cannot be set to $.5$ without loss of model adequacy.

Finally, the $a$ parameter (indicating a bias for selecting a positive US in the recollection task) was $.482$ on average.
A restricted who-said-what model setting the $a$ parameter to $.5$ produced adequate fit and a WAIC of $3,696.15$.
The WAIC was almost identical to the WAIC of the baseline model, indicating that the $a$ parameter can be set to $.5$ without loss of model adequacy.

### Effects of task order on parameter estimates

## Preregistered analyses on evaluations as a function of MPT parameter estimates

```{r}
load(file.path(study_folder, "HQstudy2evalfirst_x_ratings.rdata"))
load(file.path(study_folder, "HQstudy2memfirst_x_ratings.rdata"))
load(file.path(study_folder, "HQstudy2both_x_ratings.rdata"))
load(file.path(study_folder, "HQstudy2bothPN_x_ratings.rdata"))

para_ratings <- para_ratings_both
para_ratings$Cpn <- (para_ratings$Cneg + para_ratings$Cpos)/2

```

Individual EC effects will be calculated by subtracting the mean rating of negatively paired CSs from the mean rating of positively paired CSs.
The individual EC effects will then be predicted from individual MPT parameters (D [discrimination old vs. new], b [guessing "old"], a [guessing "positive US"], Cpos [recollection of paired US for positively paired CSs], Cneg [recollection of paired US for negatively paired CSs], dpos [recollection of paired US valence for positively paired CSs], dneg [recollection of paired US valence for negatively paired CSs]).

```{r}

cntr <- center <- function(x) { x - mean(x, na.rm = TRUE) } # = function for centering

para_ratings$ec <- para_ratings$positive-para_ratings$negative
#para_ratings$ec.2 <- para_ratings$positive_bc-para_ratings$negative_bc
#para_ratings <- subset(para_ratings, ec >= 0)
#contrasts(data$us_valence) <- "contr.sum"                   # = effects coding (i.e. sum-to-zero contrasts)

para_ratings$Cpn.c <- cntr(para_ratings$Cpn)

#para_ratings$Cneg.c <- cntr(para_ratings$Cneg)

para_ratings$dpos.c <- cntr(para_ratings$dpos)

para_ratings$dneg.c <- cntr(para_ratings$dneg)
para_ratings$b.c <- cntr(para_ratings$b)
para_ratings$a.c <- cntr(para_ratings$a)
para_ratings$D.c <- cntr(para_ratings$Dn)


model3a <- lm(
  formula = ec ~ (Cpn.c+dneg.c+dpos.c)*D.c
  
  , data = para_ratings
)

summary(model3a)
library(car)
vif(model3a)

ggpredict(model3a, terms = ~ D.c*Cpn.c) |>
  plot(rawdata = TRUE)

johnson_neyman(model = model3a
               , pred = "D.c"
               , modx = "Cpn.c"
               , plot = TRUE)

ggpredict(model3a, terms = ~ Cpn.c*D.c) |>
  plot(rawdata = TRUE)

johnson_neyman(model = model3a
               , pred = "Cpn.c"
               , modx = "D.c"
               , plot = TRUE)

ggpredict(model3a, terms = ~ dneg.c*D.c) |>
  plot(rawdata = TRUE)

johnson_neyman(model = model3a
               , pred = "dneg.c"
               , modx = "D.c"
               , plot = TRUE)

ggpredict(model3a, terms = ~ dpos.c*D.c) |>
  plot(rawdata = TRUE)

johnson_neyman(model = model3a
               , pred = "dpos.c"
               , modx = "D.c"
               , plot = TRUE)




xxx <- subset(para_ratings, order == "mem_first")
cor.test(xxx$a.c,xxx$b.c)
xxx <- subset(para_ratings, order == "eval_first")
cor.test(xxx$a.c,xxx$b.c)

model3b <- lm(
  formula = ec ~ (a.c)*D.c
  
  , data = para_ratings
)

summary(model3b)

ggpredict(model3b, terms = ~ a.c*D.c) |>
  plot(rawdata = TRUE)

johnson_neyman(model = model3b
               , pred = "a.c"
               , modx = "D.c"
               , plot = TRUE)


model3c <- lm(
  formula = ec ~ b.c*D.c
  
  , data = para_ratings
)

summary(model3c)

ggpredict(model3c, terms = ~ b.c*D.c) |>
  plot(rawdata = TRUE)

johnson_neyman(model = model3c
               , pred = "b.c"
               , modx = "D.c"
               , plot = TRUE)


```

```{r}

model3d <- lm(
  formula = dist ~ (a.c*b.c*D.c)
  
  , data = para_ratings
)

summary(model3d)

ggpredict(model3d, terms = ~ a.c*D.c) |>
  plot(rawdata = TRUE)

johnson_neyman(model = model3d
               , pred = "a.c"
               , modx = "D.c"
               , plot = TRUE)

```

## Additional analyses
