---
title             : "Memory specificity in evaluation conditioning: a 'Who said what' approach"
shorttitle        : "Memory specificity in EC"

author: 
  - name          : "Karoline Bading"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Schleichstraße 4, 72074 Tübingen (Germany)"
    email         : "karoline.bading@uni-tuebingen.de"
  - name          : "Jérémy Béna"
    affiliation   : "2"
    # role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
    #   - "Conceptualization"
    #   - "Writing - Original Draft Preparation"
    #   - "Writing - Review & Editing"
  - name          : "Marius Barth"
    affiliation   : "3"
    # role:
    #   - "Writing - Review & Editing"
    #   - "Supervision"
  - name          : "Klaus Rothermund"
    affiliation   : "4"
    # role:
    #   - "Writing - Review & Editing"
    #   - "Supervision"
      
affiliation:
  - id            : "1"
    institution   : "University of Tübingen"
  - id            : "2"
    institution   : "Aix-Marseille University"
  - id            : "3"
    institution   : "University of Cologne"
  - id            : "4"
    institution   : "Friedrich Schiller University Jena"
    


abstract: |
  BLABLABLABLABLA

  <!-- https://tinyurl.com/ybremelq -->
  
  
# authornote: |
#   Karoline Bading, University of Tübingen,
#   Jérémy Béna, Aix-Marseille University,
#   Marius Barth, University of Cologne, 
#   Klaus Rothermund, University of Jena.
#   Karoline Bading and Jérémy Béna share first authorship.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib", "`r methexp_bib()`"]
floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : jou # man
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(afex)
library(emmeans)
library(ggeffects)
library(papaja)
library(dplyr)
library(MPTinR)
library(TreeBUGS)

r_refs("r-references.bib")

project_root <- rprojroot::find_rstudio_root_file()

study_folder_pilot <- file.path(
  project_root
  , "pilot_data_analyses"
)

study_folder_main <- file.path(
  project_root
  , "Study 2"
)

study_folders <- list(
  wsw2_main = file.path(project_root, "studies", "wsw2-main")
  , wsw3_main = file.path(project_root, "studies", "wsw3-main")
)

source(file.path(project_root, "R", "apa_print_treebugs.R"))
source(file.path(project_root, "R", "mptinr_helper.R"))

knitr::opts_chunk$set(
  echo = FALSE
  , warning = FALSE
  , message = FALSE
  , cache = FALSE
  , fig.env = "figure*"
)
```

```{r analysis-preferences}
# Seed for random number generation
# set.seed(42)
# knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Experiment 1
## Methods
## Results
## Discussion

# Experiment 2

```{r}
# from MB:
data_list <- readRDS(file.path(study_folders$wsw2_main, "data", "data.rds"))

n_final <- sum(length(unique(data_list$rating$sid)))
n_conditions <- split(data_list$rating, data_list$rating$task_order) |>
  lapply(function(x) {paste0("$n = ", length(unique(x$sid)), "$")})
n_conditions <- setNames(n_conditions, papaja::sanitize_terms(tolower(names(n_conditions))))


# ----
dat <- readRDS(file.path(project_root, "Paper", "/data/data_wsw2.RDS"))
dat$url.srid <- as.factor(dat$url.srid)
n_total <- length(unique(dat$url.srid))
dat <- dat %>% filter(pay_attention != 0 & serious != 0) %>% droplevels() 
n_final <- length(unique(dat$url.srid))
socio = read.csv(file.path(project_root, "/Paper/data/sociodemo.csv"))
socio = socio %>% filter(Status != "RETURNED")



```

## Method
The pre-registration, materials and data are publicly available on the Open Science Framework at: https://osf.io/rqkvy/.

### Participants
Participants were recruited through Prolific and received monetary compensation for their participation.
The sampling pool was restricted to English speakers with at least 100 previous submissions and an approval rating of at least 90%.
Prolific users who had participated in our previous evaluative conditioning studies were excluded from the sampling pool.
We recruited 172 participants (50$\%$ female; $M_{age} =$ `r mean(as.numeric(socio$Age))`; $SD_{age} =$ `r sd(as.numeric(socio$Age))`).
Based on pre-registered criteria, we excluded five participants who declared that they did not pay attention or that they did not take their responses seriously.
We excluded another participant whose data was unavailable due to an unknown technical error.
Taken together, this resulted in a final sample size of `r n_final` participants (`r n_conditions$rating_first` in the evaluation first condition and `r n_conditions$memory_first` in the memory first condition).

The sample size was based on a power analysis for an EC effect as small as Cohen's $d = 0.2$. 
The power analysis was conducted with the R package *pwr* (version 1.3-0; Champely, 2020).
The test of the EC effect was implemented as a one-tailed paired-samples *t*-test with $\alpha=.05$ (IV: US valence; DV: evaluative ratings).
We found that 156 participants were required to achieve a statistical power of $1-\beta = .8$ to detect a signicant EC effect.
We also found that a sample size of $N = 156$ provided statistical power of $1-\beta = .8$ to detect correlations $r\mathrm{s} \geq |.22|$ (e.g., between parameter estimates and evaluative conditioning scores).
To avoid a final sample smaller than $N = 156$ after applying the pre-registered exclusion criteria, we chose to recruit 172 participants (the required sample size increased by 10%)

### Design
The experiment followed a 2 (US valence: positive vs. negative vs. unpaired) $\times$ 2 (task order: evaluation task first vs. memory task first) mixed design.
The first factor varied within participants and the second factor varied between participants.

### Materials
The experiment was programmed in *lab.js* [@henninger_labjs_2022].

The CS pool comprised 54 5- to 7-letter nonwords (e.g., "botsy", "ikzunt", "ampfong").
The nonwords were taken from a previous EC study (Stahl & Bading, 2020).
For each participant, 24 nonwords were randomly selected to serve as CSs during the learning phase.
Another 24 nonwords were randomly selected to serve as distractor stimuli in the test phase.

As USs, we used 24 colored images animals (e.g., a cockroach), scenes (e.g., a rainbow) and objects (e.g., a knife).
The images were taken from the Open Affective Standardized Image Set (OASIS; Kurdi et al., 2017). Based on OASIS ratings (on a 7-point Likert scale), 12 images were positive ($M_{valence} = 5.88$; $SD_{valence}$ = 0.24; $M_{arousal} = 4.10$; $SD_{arousal} = 0.50$) were therefore used as positive USs, while 12 were negative ($M_{valence} = 2.05$; $SD_{valence} = 0.32$; $M_{arousal} = 4.27$; $SD_{arousal} = 0.52$) and were thus used as negative USs. Positive and negative USs differed with regard valence, Welch’s $t(20.23) = 33.36, p < .001$, but not with regard to arousal, Welch’s $t(21.96) = 0.82, p = .419$. 
For each participant, the 24 USs were combined one-to-one with the 24 CSs (via random assignment).

### Measures and procedures

#### Learning phase

#### Test phase
After the learning phase, participants entered the test phase. In the test phase, participants performed two tasks: an evaluation task and a memory task. Participants were randomly assigned to one of the two task order conditions. In the evaluation task first condition, participants performed the evaluation task and then the memory task. The task order was reversed in the memory first condition. The wording of the task instructions differed slightly between task order conditions. 
The instructions can be found in the pre-registration (see OSF repository).

##### Evaluation task
In the evaluation task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit. 
Participants rated how positive or negative they found each nonword on a 8-point Likert scale ranging from "very negative" (1) to "very positive" (8).
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

##### Memory task
In the memory task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit.
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

Each trial began with with the recognition memory task: participants were asked whether the nonword had been part of the nonword-image pairs presented in the learning phase.
To indicate their response, participants were presented with two buttons labeled "Yes (old)" and "No (new)".
If participants responded "No (new)", they proceeded to the next recognition memory trial.
If participants responded "Yes (old)", they proceeded to the associative memory task.
In this task, the nonword was presented together with eight images (all of which had been shown as USs during the learning phase).
The eight images were displayed in two rows of four images (with random assignment of images to positions).
For CSs that were correctly recognized as "old", the correct US was presented together with seven randomly selected distractors (3 images of the same valence as the correct US; 4 images of the opposite valence). 
For distractor stimuli incorrectly classified as "old", eight randomly selected images (4 $\times$ positive, 4 $\times$ negative) were presented.
Participants were instructed to guess the correct option if they could not remember the previously paired image for a given nonword.

### Data processing and analysis

## Results

## Discussion



# Experiment 2

## Method

## Results



## Discussion 



# General discussion



# References

::: {#refs custom-style="Bibliography"}
:::


# (APPENDIX) Appendix {-}

```{r child = file.path(project_root, "Paper/appendix.rmd"), eval = TRUE}
```



