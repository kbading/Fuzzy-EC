---
title             : "Memory specificity in evaluative conditioning: a multinomial modeling approach"
shorttitle        : "Memory specificity in EC"

author: 
  - name          : "Karoline Bading"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Schleichstraße 4, 72074 Tübingen (Germany)"
    email         : "karoline.bading@uni-tuebingen.de"
  - name          : "Jérémy Béna"
    affiliation   : "2"
    # role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
    #   - "Conceptualization"
    #   - "Writing - Original Draft Preparation"
    #   - "Writing - Review & Editing"
  - name          : "Marius Barth"
    affiliation   : "3"
    # role:
    #   - "Writing - Review & Editing"
    #   - "Supervision"
  - name          : "Klaus Rothermund"
    affiliation   : "4"
    # role:
    #   - "Writing - Review & Editing"
    #   - "Supervision"
      
affiliation:
  - id            : "1"
    institution   : "University of Tübingen"
  - id            : "2"
    institution   : "Aix-Marseille University"
  - id            : "3"
    institution   : "University of Cologne"
  - id            : "4"
    institution   : "Friedrich Schiller University Jena"
    


abstract: |
  BLABLABLABLABLA

  <!-- https://tinyurl.com/ybremelq -->
  
  
authornote: |
  Karoline Bading and Jérémy Béna share first authorship.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib", "`r methexp_bib()`"]
floatsintext      : yes
figsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : man
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(afex)
library(emmeans)
library(ggeffects)
library(papaja)
library(dplyr)
library(MPTinR)
library(TreeBUGS)
library(HMMTreeC)

set_sum_contrasts()

r_refs("r-references.bib")

project_root <- rprojroot::find_rstudio_root_file()

study_folder_pilot <- file.path(
  project_root
  , "pilot_data_analyses"
)

study_folder_main <- file.path(
  project_root
  , "Study 2"
)

study_folders <- list(
    wsw1 = file.path(project_root, "studies", "wsw1")
  , wsw2_main = file.path(project_root, "studies", "wsw2-main")
  , wsw3_main = file.path(project_root, "studies", "wsw3-main")
  , wsw3_p2   = file.path(project_root, "studies", "wsw3-p2")
  , wsw3_joint  = file.path(project_root, "studies", "wsw3-joint-analysis")
)

source(file.path(project_root, "R", "apa_print_treebugs.R"))
source(file.path(project_root, "R", "mptinr_helper.R"))

knitr::opts_chunk$set(
  echo = FALSE
  , warning = FALSE
  , message = FALSE
  , cache = FALSE
  , fig.env = "figure*"
)

cntr <- function(x, ...){x - mean(x, ...)}
```

```{r analysis-preferences}
# Seed for random number generation
# set.seed(42)
# knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Evaluative conditioning (EC) refers to a change in the evaluation of an initially neutral "conditioned" stimulus (CS) after its pairing with a positive or negative "unconditioned" stimulus (US).

- memory for CS-US pairings is most important moderator

- US identity memory vs. US valence memory: definitions and measurement tasks

- distinction is important for methodological vs. theoretical reasons: (1) EC without awareness/memory, (2) different memory-based accounts imply different relationships between EC and two types of pairing memory

- present research is concerned with (2): specifically, improving methodology for testing different memory-based accounts

- we identify fours shortcomings in current measurement tasks and explain their problematic implications for testing memory-based accounts: (1) US valence scores are likely contaminated by US identity memory, (2) US identity scores may be contaminated by US valence memory, (3) US identity/valence scores can be contaminated by bias for positive vs. negative response options, (4) US identity/valence scores are confounded with CS recognition memory

- we propose a multinomial model as a solution for these shortcomings: adaptation of the "who-said-what" model by Klauer and Wegener (1998)

- we present three experiments: (1) shows fit to pairing memory data based on standard EC procedure, (2) explores effects of task order (memory first vs. rating first) as important procedural feature for successful applications in future applications, (3) reports an experimental application re-investigating a widely cited finding in EC research (Gast & Rothermund, 2011)

# Experiment 1

```{r}
data_list_pilot <- readRDS(file.path(study_folders$wsw1, "data", "data.rds"))
#data_list_pilot$excluded_participants
```

## Methods
Experiment 1 was not pre-registered.
The materials and data are publicly available at: https://osf.io/rqkvy/.
The data collection was conducted online.

### Participants
Participants were recruited through Prolific and received monetary compensation for their participation.
The sampling pool was restricted to English speakers with at least 100 previous submissions and an approval rating of at least 90%.
Prolific users who had participated in our previous evaluative conditioning studies were excluded from the sampling pool.
We recruited 50 participants (xxx$\%$ female; $M_{age} = xxx$; $SD_{age} = xxx$).
The number of recruited participants was determined *ad hoc* and was not based on a power analysis.
We excluded one participant who declared that they did not pay attention to the images presented throughout the experiment.
We excluded another participant who gave the same response on all trials of the evaluation task.
We interpreted this response behavior as an indicator of non-compliance and decided to exclude the participant from all analyses.
Taken together, this resulted in a final sample size of 48 participants.

### Design
The experiment followed a 2 (US valence: positive vs. negative) within-subjects design.

### Materials
The experiment was programmed with *lab.js* [@henninger_labjs_2022] and exported to an HTTPS-protected website with JATOS [@lange_just_2015].

The CS pool comprised 48 colored images of human faces with neutral expressions (24 female faces, 24 male faces).
The images were taken from xxx.

As USs, we used 24 colored images animals (e.g., a cockroach), scenes (e.g., a rainbow) and objects (e.g., a knife).
The images were taken from the Open Affective Standardized Image Set (OASIS; Kurdi et al., 2017). Based on OASIS ratings (on a 7-point Likert scale), we selected 12 positive images ($M_{valence} = 5.88$; $SD_{valence}$ = 0.24; $M_{arousal} = 4.10$; $SD_{arousal} = 0.50$) and 12 negative images ($M_{valence} = 2.05$; $SD_{valence} = 0.32$; $M_{arousal} = 4.27$; $SD_{arousal} = 0.52$).
The positive and negative images differed with regard valence, Welch’s $t(20.23) = 33.36, p < .001$, but not with regard to arousal, Welch’s $t(21.96) = 0.82, p = .419$. 

For each participant, 12 randomly selected face images (50% female) served as positively paired CSs (i.e., they were paired with a positive US during the learning phase), 12 randomly selected face images (50% female) served as negatively paired CSs, and the remaining 24 face images served as distractor stimuli in the test phase.

For each participant, all 24 images served as USs during the learning phase. 
The 24 images were randomly assigned to the 24 CSs.
Twelve CSs (50% female) were paired with positive USs and twelve CSs (50% female) were paired with negative USs.

### Measures and procedures
All verbal materials were presented in English.
After providing informed consent and being asked to focus on the study, participants were thanked for their participation and asked to pay close attention to all instructions and tasks.
Subsequently, participants were presented with the instructions for the learning phase.

#### Learning phase
First, participants read the following instruction slide: "In the first part of the experiment you will be presented with image pairs. Each image pair consists of a face and another picture. Your task is simply to pay close attention to each image pair. Press the spacebar to continue with the instructions."
Subsequently, participants read the following instruction slide: "Next you will be presented with the image pairs. Please pay close attention to each image pair. This part of the experiment will take about 2.5 minutes. Press the spacebar to start the task."

After pressing the spacebar, participants worked through the learning task, consisting of 72 trials.
The trials were separated by blank screens presented for 1,000 ms.
On each trial, a CS (face image, center-left position) was presented together with a US (positive or negative image, center-right position).
CS and US appeared at the same time and remained on screen for 1,000 ms.
For each participant, trial order was randomized in sets of 24 trials.
In each set (three in total), each of the 24 CS-US pairs was presented once.

After the last trial of the learning task, participants read the following instruction slide: "The first part of the experiment is now finished. You have now seen all image pairs and may continue with the second part of the experiment. Press the spacebar to continue."

#### Test phase
After the learning phase, participants entered the test phase. 
In the test phase, participants performed two measurement tasks: a memory task followed by an evaluation task. 
The order of the two tasks (memory task first) was the same for all participants.

##### Memory task
First, participants were presented with the following instruction slide: "In the second part of the experiment you will be presented with individual faces. Some of these faces were part of the previously presented image pairs. Other faces will be new: they were not part of the image pairs you saw in the previous task. For each face, please indicate whether it is 'old' (i.e., part of the previously presented image pairs) or 'new' (i.e., not part of the previously presented image pairs). Press the spacebar to continue."

Next, participants read the following instruction slide: "Whenever you classify a face as 'old', you will be asked to perform a second task. In this second task, you will be presented with eight pictures. Your task will be to select the picture with which the face was previously paired with. If you can remember the paired picture, click on it and then click on the 'continue' button at the bottom of the screen. If you cannot remember the previously paired picture, try to guess the correct one. Again, click on the image corresponding to your guess and then on the "continue" button at the bottom of the screen. Ready? Then press the spacebar to continue."

After pressing the spacebar, participants performed the memory task, consisting of 48 trials.
In the memory task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit.
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.
Each trial began with the CS recognition task: participants were asked whether the presented face image had been part of the image pairs presented in the learning phase.
To indicate their response, participants were presented with two buttons labeled "Yes (old)" and "No (new)".
If participants responded "No (new)", they proceeded to the next recognition memory trial.
If participants responded "Yes (old)", they proceeded to the US memory task (in which they were asked to identify the previously paired US).
In this task, the CS was presented together with eight USs from the learning phase.
The eight US images were displayed in two rows of four images (with random assignment of images to positions).
For CSs that were correctly recognized as "old", the correct US was presented together with seven randomly selected distractors (3 images of the same valence as the correct US; 4 images of the opposite valence). 
For distractor stimuli incorrectly classified as "old", eight randomly selected images (4 $\times$ positive, 4 $\times$ negative) were presented.

After the last trial of the memory task, participants read the following instruction slide: "The second part of the experiment is now finished. Press the spacebar to continue with the third part."

##### Evaluation task
First, participants read the following instruction slide: "In the third part of the experiment, you will again be presented with the face images. This time you will be asked to indicate your impression of the persons depicted in the images. To indicate your impression of a given person, you will be presented with an 11-point scale ranging from very negative (left) to very positive (right). Please click on the scale point that best represents your impression of the person depicted in a given image. Then click on the 'continue' button to proceed to the next face. Ready? Then press the spacebar to continue."

After pressing the spacebar, particpants worked through the evaluation task, consisting of 48 trials.
In the evaluation task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit. 
Participants rated how positive or negative they found each face on a 11-point rating scale ranging from "very negative" (-5) to "very positive" (+5). (Note that participants responses were stored as numerical values ranging from 1 to 11.)
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

After the last trial of the evaluation task, participants read the following instruction slide: "The third and final part of the experiment is now finished. Now we have a few short questions about your experience performing the study. Press the spacebar to continue."

#### Control measures and debriefing
After pressing the spacebar, participants were asked whether they paid attention to the images presented throughout the experiment.
The response options ("Yes" vs. "No") were presented as two buttons.
Subsequently, participants were asked whether they took the requested responses seriously.
The response options ("I have taken the requested responses seriously." vs. "I have just clicked through, please discard my data.") were again presented as two buttons.
Subsequently, participants were given the chance to comment on the study.
Finally, participants were presented with the following text (debriefing them about the purpose of the study): "The experiment is now over. Thank you very much for your participation. In this experiment, we wanted to see whether the valence of the paired scene (positive vs. negative) has an influence on your evaluation of the faces. In addition, we were also interested in your memory for the image pairs. If you have any question or comment, or if you would like to receive additional information on the present study, please do not hesitate to contact the person in charge of this research at the following e-mail address: [*e-mail address*]. Press the spacebar to be redirected to Prolific."
After pressing the spacebar, participants were redirected to Prolific and reimbursed for their participation.

### Data processing and statistical analysis
The evaluative ratings were analyzed (without further processing) using the R package *stats*.
For the MPT model analyses, the responses from the two memory tasks were first recoded into a joint memory index according to the following scheme:

- If a positively (negatively) paired CS was incorrectly classified as "new", the response was recoded as "PosUSnew" ("NegUSnew"). 

- If a positively (negatively) paired CS was correctly classified as "old" and the correct positive (negative) US was selected, the responses were recoded as "PosUSposcor" ("NegUSnegcor").

- If a positively (negatively) paired CS was correctly classified as "old" and an incorrect positive (negative) trait was selected, the responses were recoded as "PosUSposincor" ("NegUSnegincor").

- If a positively (negatively) paired CS was correctly classified as "old" and a negative (positive) trait was selected, the responses were recoded as "PosUSnegincor" ("NegUSposincor")

- If an (unpaired) new stimulus was correctly classified as "new", the response was recoded as "Newnew".

- If an (unpaired) new stimulus was incorrectly classified as "old" and a positive (negative) trait was selected, the responses will be recoded as "Newposincor" ("Newnegincor").

```{r}
baseline_restrictions <- list(
  G = .25
)

hypothesis_restrictions_8b <- list(
  baseline = list()
  , D_eq0  = list(D = .Machine$double.eps)
  , Cpos_eq0  = list(C_positive = .Machine$double.eps)
  , Cneg_eq0  = list(C_negative = .Machine$double.eps)
  , dpos_eq0  = list(d_positive = .Machine$double.eps)
  , dneg_eq0  = list(d_negative = .Machine$double.eps)
  , a_eq.5 = list(a = .5)
  , b_eq.5 = list(b = .5)
  , C_eq  = list(C = c("C_positive", "C_negative"))
  , d_eq  = list(d = c("d_positive", "d_negative"))
  , fiveparam = list(C = c("C_positive", "C_negative"),d = c("d_positive", "d_negative"))
)

models_8b <- lapply(
  hypothesis_restrictions_8b
  , FUN = function(x) {
    fit_mpt(
      model = file.path(project_root, "model-equations", "wsw-8b.eqn")
      , data = subset(data_list_pilot$mpt_data_hierarchical,sid!=43)
      , restrictions = c(baseline_restrictions, x)
    )
  }
)

exp1 <- apa_print(compare(models_8b))
#models_8b$baseline$goodness_of_fit
#models_8b$baseline
```

Next, the frequency distribution of the joint memory index (aggregated across participants) was analyzed with the R package *HMMTreeC*.
We implemented different versions of the MPT model 
In all versions of the model, the $g$ parameter was set to $.25$ (corresponding to the inverse of the number of response options per US valence in the US memory task).
To allow for potential memory differences across US valence conditions, we first fitted a model variant that included separate $C$ and $d$ parameters for positively vs. negative paired CSs (next to joint $D$, $b$ and $a$ parameters).
We then compared this 7-parameter model with a more restrictive 5-parameter model in which $C$ and $d$ parameters were set equal across US valence conditions.
There was no significant difference in model fit between the two models, `r $full_result$fiveparam`.
We therefore decided to report the simpler 5-parameter model.

```{r}
exp1_ratings <- data_list_pilot$rating
exp1_ratings_sd <- aggregate(exp1_ratings,evaluative_rating ~ sid, FUN = sd) # 1 x sd = 0 --> sid = 43
exp1_ratings <- subset(exp1_ratings,sid != 43)
exp1_ratings_wide <- subset(data_list_pilot$rating_wide,sid != 43)

data_list_pilot$rating <- subset(data_list_pilot$rating,sid != 43)
data_list_pilot$rating_wide <- subset(data_list_pilot$rating_wide,sid != 43)
```

```{r}
rrr <- data_list_pilot$memory
#table(rrr$sid)
rrr$reco_resp_num <- ifelse(rrr$reco_resp=="old",1,0)
rrr2 <- aggregate(rrr,reco_resp_num ~ sid, FUN = sd) # 0 x sd = 0
```

```{r}
#table(rrr$sid,rrr$source_mem) # 0 x sd = 0
```

## Results

### Evaluative ratings

```{r}
exp1_ratings$cs_sex <- ifelse(exp1_ratings$cs %in% c("f1","f2","f3","f4","f5","f6","f7","f8","f9","f10","f11","f12","f13","f14","f15","f16","f17","f18","f19","f20","f21","f22","f23","f24"),"female","male")
exp1_ratings$us_valence2 <- "no pairing"
exp1_ratings$us_valence2 <- ifelse(is.na(exp1_ratings$us_valence)==TRUE, "no pairing",as.character(exp1_ratings$us_valence))

exp1_ratings_agg <- aggregate(exp1_ratings,FUN=mean,evaluative_rating~sid*us_valence2)
library(reshape2)
exp1_ratings_agg2 <- dcast(exp1_ratings_agg,value.var="evaluative_rating",sid~us_valence2)

exp1_posneg <- apa_print(t.test(exp1_ratings_agg2$positive,exp1_ratings_agg2$negative,paired=TRUE,alternative="greater"))

mean_pos <- round(mean(exp1_ratings_agg2$positive),2)
sd_pos <- round(sd(exp1_ratings_agg2$positive),2)

mean_neg <- round(mean(exp1_ratings_agg2$negative),2)
sd_neg <- round(sd(exp1_ratings_agg2$negative),2)

mean_nop<- round(mean(exp1_ratings_agg2$`no pairing`),2)
sd_nop <- round(sd(exp1_ratings_agg2$`no pairing`),2)

exp1_posdist <- apa_print(t.test(exp1_ratings_agg2$positive,exp1_ratings_agg2$`no pairing`,paired=TRUE,alternative="greater"))
exp1_negdist <- apa_print(t.test(exp1_ratings_agg2$negative,exp1_ratings_agg2$`no pairing`,paired=TRUE,alternative="less"))

#effectsize::cohens_d(exp1_ec)
```

Evaluative ratings for CSs and distractor stimuli were analyzed using paired-samples *t*-tests.
The mean rating for positively paired CSs was higher than the mean rating for negatively paired CSs ($M_{positive}=$ `r mean_pos`, $SD_{positive}=$ `r sd_pos`; $M_{negative}=$ `r mean_neg`, $SD_{negative}=$ `r sd_neg`).
The mean difference (i.e., the mean EC effect) reached significance in a one-tailed test, `r exp1_posneg$full_result`.
The mean rating for positively paired CSs was also higher than the mean rating for unpaired distractor stimuli ($M_{distractor}=$ `r mean_nop`, $SD_{distractor}=$ `r sd_nop`).
However, the mean difference failed to reach significance even in a one-tailed test, `r exp1_posdist$full_result`.
Finally, the mean rating for negatively paired CSs was lower than the mean rating for unpaired distractor stimuli.
The mean difference was again non-significant even in a one-tailed test, `r exp1_negdist$full_result`.

### MPT model analyses

```{r}
baseline_restrictions <- list(
  G = .25
)

hypothesis_restrictions_8b <- list(
  baseline = list()
  , D_eq0  = list(D = .Machine$double.eps)
  , Cpos_eq0  = list(C_positive = .Machine$double.eps)
  , Cneg_eq0  = list(C_negative = .Machine$double.eps)
  , dpos_eq0  = list(d_positive = .Machine$double.eps)
  , dneg_eq0  = list(d_negative = .Machine$double.eps)
  , a_eq.5 = list(a = .5)
  , b_eq.5 = list(b = .5)
  , C_eq  = list(C = c("C_positive", "C_negative"))
  , d_eq  = list(d = c("d_positive", "d_negative"))
)

models_8b <- lapply(
  hypothesis_restrictions_8b
  , FUN = function(x) {
    fit_mpt(
      model = file.path(project_root, "model-equations", "wsw-8b.eqn")
      , data = subset(data_list_pilot$mpt_data_hierarchical,sid!=43)
      , restrictions = c(baseline_restrictions, x)
    )
  }
)

#compare(models_8b)
#models_8b$baseline$goodness_of_fit
#models_8b$baseline
```

```{r}
#exp1_mpt_data <- data_list_pilot$memory

baseline_restrictions <- list(
  G = .25
)

hypothesis_restrictions <- list(
  baseline = list()
  , D_eq0  = list(D = .Machine$double.eps)
  , C_eq0  = list(C = .Machine$double.eps)
  , d_eq0  = list(d = .Machine$double.eps)
  , a_eq.5 = list(a = .5)
  , b_eq.5 = list(b = .5)
)

models <- lapply(
  hypothesis_restrictions
  , FUN = function(x) {
    fit_mpt(
      model = file.path(study_folders$wsw1, "WSW_pilot_hierarchical.eqn")
      , data = subset(data_list_pilot$mpt_data_hierarchical,sid!=43)
      , restrictions = c(baseline_restrictions, x)
    )
  }
)
apa_wsw6 <- apa_print(models$baseline)
#compare(models)
#models$baseline
apa_wsw_comps <- apa_print(compare(models))
```

```{r}
apa_wsw8b <- apa_print(models_8b$baseline)
apa_wsw8b_comps <- apa_print(compare(models_8b))
```

The 5-parameter model fit the data well, `r apa_wsw6$statistic$modelfit`.
Parameter estimates and confidence intervals are reported in Table 1.

The $D$ parameter estimate was significantly larger than zero, `r apa_wsw_comps$full_result$D_eq0`, indicating that participants recognized CSs as "old" (and distractor stimuli as "new") in $45.6$% of trials.
By implication, participants did not recognize CSs as "old" (and distractor stimuli as "new") in $54.4$% of trials.

The $C$ parameter estimate was significantly larger than zero, `r apa_wsw_comps$full_result$C_eq0`, indicating that participants had US identity memory for $67.4$% of CSs that had been recognized as "old".
This implies that US identity was absent for $32.6$% of CSs recognized as "old" (hereafter also referred to as "recognized" CSs).

The $d$ parameter estimate was significantly larger than zero, `r apa_wsw_comps$full_result$d_eq0`, indicating that in the subset of "recognized" CSs without US identity memory, participants possessed US valence memory in approx. $19.9$% of trials.
By implication, participants did not possess US valence memory for approx. $80.1$% of "recognized" CSs without US identity memory.

The $b$ parameter estimate was significantly lower than $.5$, `r apa_wsw_comps$full_result$b_eq_5`.
This suggests that, for CSs not recognized as "old" (as distractor stimuli not recognized as "new"), participants had an overall tendency towards selecting the "new" response.
Specifically, the size of the parameter estimate indicates that $14.8$% of CSs not recognized as "old" (and distractor stimuli not recognized as "new") were guessed as "old".
By implication, participants guessed "new" for $85.2$% of CSs not recognized as "old" (and distractor stimuli not recognized as "new").

The $a$ parameter estimate was significantly lower than $.5$, `r apa_wsw_comps$full_result$a_eq_5`.
This suggests that, in the absence of both US identity and US valence memory, participants had an overall tendency towards selecting a negative image in the US memory task.
Specifically, the size of the parameter estimate indicates that participants selected a positive (negative) US for $42.7$% ($57.3$%) of CSs and distractor stimuli guessed as "old" (and "recognized" CSs without US identity and US valence memory).

### Relationships between evaluative ratings and MPT parameters

```{r}

exp1_trait_model_ec <- readRDS(file.path(study_folders$wsw1, "model-objects", "trait-mpt-with-ec.rds"))
#summary(exp1_trait_model_ec)

exp1_trait_model <- readRDS(file.path(study_folders$wsw1, "model-objects", "trait-mpt.rds"))
#summary(exp1_trait_model)
```

```{r}
ratings_and_parameters_ec_long <- merge(
  data_list_pilot$rating
  , get_theta(model = exp1_trait_model_ec, data = data_list_pilot$mpt_data_hierarchical)
)

ratings_and_parameters_long <- merge(
  data_list_pilot$rating
  , get_theta(model = exp1_trait_model, data = data_list_pilot$mpt_data_hierarchical)
)

contrasts(ratings_and_parameters_ec_long$us_valence) <- "contr.sum"
contrasts(ratings_and_parameters_long$us_valence) <- "contr.sum"
```

```{r}
ratings_and_parameters_ec <- merge(
  data_list_pilot$rating_wide
  , get_theta(model = exp1_trait_model_ec, data = data_list_pilot$mpt_data_hierarchical)
)

ratings_and_parameters <- merge(
  data_list_pilot$rating_wide
  , get_theta(model = exp1_trait_model, data = data_list_pilot$mpt_data_hierarchical)
)

both <- merge(ratings_and_parameters_ec,ratings_and_parameters, by = c("sid"))

# cor.test(both$D.x,both$D.y)
# t.test(both$D.x,both$D.y)
# 
# cor.test(both$C.x,both$C.y)
# t.test(both$C.x,both$C.y)
# 
# cor.test(both$d.x,both$d.y)
# t.test(both$d.x,both$d.y)
# 
# cor.test(both$b.x,both$b.y)
# t.test(both$b.x,both$b.y)
# 
# cor.test(both$a.x,both$a.y)
# t.test(both$a.x,both$a.y)
```

```{r}
model5_exp1 <- afex::lmer(evaluative_rating ~ (cntr(D)+cntr(C)+cntr(d))*us_valence + (1|sid)
           , data = subset(ratings_and_parameters_long,us_valence%in%c("positive","negative")))

ap_model5_exp1 <- apa_print(model5_exp1)
ap_model5_exp1$full_result
```

```{r}
ggeffect(model5_exp1, terms = ~ D:us_valence) |> plot()
```

```{r}
summary(emtrends(model5_exp1,var= "D", specs= ~ us_valence),infer=TRUE)
```

```{r}
ggeffect(model5_exp1, terms = ~ C:us_valence) |> plot()
```

```{r}
summary(emtrends(model5_exp1,var= "C", specs= ~ us_valence),infer=TRUE)
```

```{r}
ggeffect(model5_exp1, terms = ~ d:us_valence) |> plot()
```

```{r}
summary(emtrends(model5_exp1,var= "d", specs= ~ us_valence),infer=TRUE)
```

```{r}
# ggeffect(model5_exp1, terms = ~ b:us_valence) |> plot()
```

```{r}
# summary(emtrends(model5_exp1,var= "b", specs= ~ us_valence),infer=TRUE)
```

```{r}
# ggeffect(model5_exp1, terms = ~ a:us_valence) |> plot()

```

```{r}
# summary(emtrends(model5_exp1,var= "a", specs= ~ us_valence),infer=TRUE)
```

## Discussion

# Experiment 1

```{r}
data_list_exp2 <- readRDS(file.path(study_folders$wsw2_main, "data", "data.rds"))
data_list_exp2$excluded_participants

n_final <- sum(length(unique(data_list_exp2$rating$sid)))
n_conditions <- split(data_list_exp2$rating, data_list_exp2$rating$task_order) |>
  lapply(function(x) {paste0("$n = ", length(unique(x$sid)), "$")})
n_conditions <- setNames(n_conditions, papaja::sanitize_terms(tolower(names(n_conditions))))

dat <- readRDS(file.path(project_root, "Paper", "/data/data_wsw2.RDS"))
dat$url.srid <- as.factor(dat$url.srid)
n_total <- length(unique(dat$url.srid))
dat <- dat %>% filter(pay_attention != 0 & serious != 0) %>% droplevels() 
n_final <- length(unique(dat$url.srid))
socio = read.csv(file.path(project_root, "/Paper/data/sociodemo.csv"))
socio = socio %>% filter(Status != "RETURNED")
```

## Method
The experiment was pre-registered on the OSF.
The pre-registration, materials and data are publicly available at: https://osf.io/rqkvy/.
The data collection was conducted online.

### Participants
Participants were recruited through Prolific and received monetary compensation for their participation.
The sampling pool was restricted to English speakers with at least 100 previous submissions and an approval rating of at least 90%.
Prolific users who had participated in our previous evaluative conditioning studies were excluded from the sampling pool.
We recruited 172 participants (50$\%$ female; $M_{age} =$ `r mean(as.numeric(socio$Age))`; $SD_{age} =$ `r sd(as.numeric(socio$Age))`).
Based on pre-registered criteria, we excluded five participants who declared that they did not pay attention or that they did not take their responses seriously.
We excluded another participant whose data was unavailable due to an unknown technical error.
Taken together, this resulted in a final sample size of `r n_final` participants (`r n_conditions$rating_first` in the evaluation task first condition and `r n_conditions$memory_first` in the memory task first condition).

The number of recruited participants was based on a power analysis for an EC effect as small as Cohen's $d = 0.2$. 
The power analysis was conducted with the R package *pwr* (version 1.3-0; Champely, 2020).
The test of the EC effect was implemented as a one-tailed paired-samples *t*-test with $\alpha=.05$ (IV: US valence; DV: evaluative ratings).
We found that 156 participants were required to achieve a statistical power of $1-\beta = .8$ to detect a significant EC effect.
We also found that a sample size of $N = 156$ provided statistical power of $1-\beta = .8$ to detect correlations $r\mathrm{s} \geq |.22|$ (e.g., between parameter estimates and evaluative ratings).
To avoid a final sample smaller than $N = 156$ after applying the pre-registered exclusion criteria, we chose to recruit 172 participants (the required sample size increased by 10%).

### Design
The experiment followed a 2 (US valence: positive vs. negative) $\times$ 2 (measurement task order: evaluation task first vs. memory task first) mixed design.
The first factor varied within participants and the second factor varied between participants.

### Materials
The experiment was programmed with *lab.js* [@henninger_labjs_2022] and exported to an HTTPS-protected website with JATOS [@lange_just_2015].

The CS pool comprised 54 nonwords made up of five to seven letters (e.g., *botsy*, *ikzunt*, *ampfong*).
The nonwords were taken from a previous EC study (Stahl & Bading, 2020).
For each participant, 24 nonwords were randomly selected to serve as CSs during the learning phase.
Another 24 nonwords were randomly selected to serve as distractor stimuli in the test phase.

As USs, we used 24 colored images animals (e.g., a cockroach), scenes (e.g., a rainbow) and objects (e.g., a knife).
The images were taken from the Open Affective Standardized Image Set (OASIS; Kurdi et al., 2017). Based on OASIS ratings (on a 7-point Likert scale), 12 images were positive ($M_{valence} = 5.88$; $SD_{valence}$ = 0.24; $M_{arousal} = 4.10$; $SD_{arousal} = 0.50$) were therefore used as positive USs, while 12 were negative ($M_{valence} = 2.05$; $SD_{valence} = 0.32$; $M_{arousal} = 4.27$; $SD_{arousal} = 0.52$) and were thus used as negative USs. Positive and negative USs differed with regard valence, Welch’s $t(20.23) = 33.36, p < .001$, but not with regard to arousal, Welch’s $t(21.96) = 0.82, p = .419$. 
For each participant, the 24 USs were combined one-to-one with the 24 CSs (via random assignment).

### Measures and procedures
We used JATOS [@lange_just_2015] to run the study online.
All verbal materials were presented in English.
The task instructions can be found in the pre-registration (see OSF repository).

#### Learning phase
Participants were told that they would see pairs of nonwords and images and were instructed to pay close attention to each pair.
The learning phase consisted of 72 trials.
The trials were separated by blank screens presented for 1,000 ms.
On each trial, a nonword (CS, center-left position) was presented together with a valenced image (US, center-right position).
CS and US appeared at the same time and remained on screen for 1,000 ms.
For each participant, trial order was randomized in sets of 24 trials.
In each set (three in total), each of the 24 CS-US pairs was presented once.

#### Test phase
After the learning phase, participants entered the test phase. 
In the test phase, participants performed two measurement tasks: an evaluation task and a memory task. Participants were randomly assigned to one of the two measurement task order conditions. 
In the "evaluation task first" condition, participants performed the evaluation task and then the memory task. The task order was reversed in the "memory task first" condition. 
The wording of the task instructions differed slightly between task order conditions. 

##### Evaluation task
In the evaluation task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit. 
Participants rated how positive or negative they found each nonword on a 8-point Likert scale ranging from "very negative" (1) to "very positive" (8).
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

##### Memory task
In the memory task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit.
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

Each trial began with with the recognition memory task: participants were asked whether the nonword had been part of the nonword-image pairs presented in the learning phase.
To indicate their response, participants were presented with two buttons labeled "Yes (old)" and "No (new)".
If participants responded "No (new)", they proceeded to the next recognition memory trial.
If participants responded "Yes (old)", they proceeded to the associative memory task (in which they were asked to identify the previously paired US).
In this task, the nonword was presented together with eight images (all of which had been shown as USs during the learning phase).
The eight images were displayed in two rows of four images (with random assignment of images to positions).
For CSs that were correctly recognized as "old", the correct US was presented together with seven randomly selected distractors (3 images of the same valence as the correct US; 4 images of the opposite valence). 
For distractor stimuli incorrectly classified as "old", eight randomly selected images (4 $\times$ positive, 4 $\times$ negative) were presented.
Participants were instructed to guess the correct option if they could not remember the previously paired US.

#### Control measures and debriefing
After the test phase, participants were asked whether they paid attention to the nonwords and images presented throughout the experiment.
The response options ("Yes" vs. "No") were presented as two buttons.
Subsequently, participants were asked whether they took the requested responses seriously.
The response options ("I have taken the requested responses seriously" vs. "I have just clicked through, please discard my data") were again presented as two buttons.
Subsequently, participants were given the chance to comment on the study.
Finally, participants were debriefed about the purpose of the study and then redirected to Prolific.

### Data processing and statistical analysis

```{r}
exp2_ratings <- data_list_exp2$rating
exp2_ratings_sd <- aggregate(exp2_ratings,evaluative_rating ~ sid, FUN = sd) # 4 x sd = 0 --> sid = 49, 104, 138, 139
exp2_ratings_sd <- subset(exp2_ratings_sd, evaluative_rating > 0)
exp2_ratings <- subset(exp2_ratings,sid %in% exp2_ratings_sd$sid)
exp2_ratings_wide <- subset(data_list_exp2$rating_wide,sid %in% exp2_ratings_sd$sid)
```

```{r}
rrr <- data_list_exp2$memory
#table(rrr$sid)
rrr$reco_resp_num <- ifelse(rrr$reco_resp=="old",1,0)
rrr2 <- aggregate(rrr,reco_resp_num ~ sid, FUN = sd) # 1 x sd = 141 (always 'new')
#table(rrr$sid,rrr$source_mem) # 0 x sd = 0

```

## Results
### Evaluative ratings
```{r}
exp2_ratings <- subset(exp2_ratings,sid!=141)
exp2_ratings_wide <- subset(exp2_ratings_wide,sid!=141)

exp2_ratings_anova <- aov_ez(data = subset(exp2_ratings, us_valence %in% c("positive","negative")), id = "sid", dv = "evaluative_rating", within = c("us_valence"),between=c("task_order"))

exp2_ec <- t.test(exp2_ratings_wide$positive,exp2_ratings_wide$negative,paired=TRUE,alternative="greater")

effectsize::cohens_d(exp2_ec)
```

### MPT model

```{r}
baseline_restrictions <- list(
  G_x1 = .25
  , G_x2 = .25
)

hypothesis_restrictions <- list(
  baseline = list()
  , D_equal = list(D = c("D_x1", "D_x2"))
  , Cx1_equal = list(C = c("Cpos_x1", "Cneg_x1"))
  , Cx2_equal = list(C = c("Cpos_x2", "Cneg_x2"))
  , dx1_equal = list(d = c("dpos_x1", "dneg_x1"))
  , dx2_equal = list(d = c("dpos_x2", "dneg_x2"))
  , a_equal = list(a = c("a_x1", "a_x2"))
  , b_equal = list(b = c("b_x1", "b_x2"))
  , Deval0   = list(D_x1 = .Machine$double.eps)
  , Dmem0   = list(D_x2 = .Machine$double.eps)
  , Cposeval0   = list(Cpos_x1 = .Machine$double.eps)
  , Cnegeval0   = list(Cneg_x1 = .Machine$double.eps)
  , Cposmem0   = list(Cpos_x2 = .Machine$double.eps)
  , Cnegmem0   = list(Cneg_x2 = .Machine$double.eps)
  , dposeval0   = list(dpos_x1 = .Machine$double.eps)
  , dnegeval0   = list(dneg_x1 = .Machine$double.eps)
  , dposmem0   = list(dpos_x2 = .Machine$double.eps)
  , dnegmem0   = list(dneg_x2 = .Machine$double.eps)
  , beval5   = list(b_x1 = 0.5)
  , bmem5   = list(b_x2 = 0.5)
  , aeval5   = list(a_x1 = 0.5)
  , amem5   = list(a_x2 = 0.5)
)

models <- lapply(
  hypothesis_restrictions
  , FUN = function(x) {
    fit_mpt(
      data = data_list_exp2$mpt_data
      , model = file.path(study_folders$wsw2_main, "WSW_8b_exp2.eqn")
      , restrictions = c(baseline_restrictions, x)
    )
  }
)
wesanderson::wes_palette("Zissou1", n = 3, type = "c") |> 
  palette()

# plot(
#   models$baseline
#   , factors = list("Task order" = c("Rating first" = "_x1", "Memory first" = "_x2"))
#   , parameters = c(
#     D   = expression(italic(D))
#     , Cpos = expression(italic(Cpos))
#     , Cneg = expression(italic(Cneg))
#     , dpos = expression(italic(dpos))
#     , dneg = expression(italic(dneg))
#     , a = expression(italic(a))
#     , b = expression(italic(b))
#   )
# )

```

```{r}
model_comparisons <- compare(models)
model_comparisons
```

```{r}
baseline_restrictions <- list(
  G_x1 = .25
  , G_x2 = .25
)

hypothesis_restrictions <- list(
  baseline = list()
  , D_equal = list(D = c("D_x1", "D_x2"))
  , C_equal = list(C = c("C_x1", "C_x2"))
  , d_equal = list(d = c("d_x1", "d_x2"))
  , a_equal = list(a = c("a_x1", "a_x2"))
  , b_equal = list(b = c("b_x1", "b_x2"))
  , Dage0   = list(D_x1 = .Machine$double.eps)
  , Dval0   = list(D_x2 = .Machine$double.eps)
  , Cage0   = list(C_x1 = .Machine$double.eps)
  , Cval0   = list(C_x2 = .Machine$double.eps)
  , dage0   = list(d_x1 = .Machine$double.eps)
  , dval0   = list(d_x2 = .Machine$double.eps)
  , bage5   = list(b_x1 = 0.5)
  , bval5   = list(b_x2 = 0.5)
  , aage5   = list(a_x1 = 0.5)
  , aval5   = list(a_x2 = 0.5)
)

models <- lapply(
  hypothesis_restrictions
  , FUN = function(x) {
    fit_mpt(
      data = data_list_exp2$mpt_data
      , model = file.path(study_folders$wsw2_main, "WSW_exp2.eqn")
      , restrictions = c(baseline_restrictions, x)
    )
  }
)
wesanderson::wes_palette("Zissou1", n = 3, type = "c") |> 
  palette()

# plot(
#   models$baseline
#   , factors = list("Task order" = c("Rating first" = "_x1", "Memory first" = "_x2"))
#   , parameters = c(
#     D   = expression(italic(D))
#     , C = expression(italic(C))
#     , d = expression(italic(d))
#     , a = expression(italic(a))
#     , b = expression(italic(b))
#   )
# )

```

```{r}
model_comparisons <- compare(models)
model_comparisons
```

### Relationships between evaluative ratings and MPT parameters

```{r}
exp2_trait_model <- readRDS(file.path(study_folders$wsw2_main, "model-objects", "trait-mpt.rds"))
summary(exp2_trait_model)
# 
exp2_trait_model_ec <- readRDS(file.path(study_folders$wsw2_main, "model-objects", "trait-mpt-with-ec.rds"))
summary(exp2_trait_model_ec)

exp2_trait_model_8b <- readRDS(file.path(study_folders$wsw2_main, "model-objects", "trait-mpt-wsw-8b.rds"))
summary(exp2_trait_model_8b)
plotParam(exp2_trait_model_8b)

exp2_trait_model_8b_ec <- readRDS(file.path(study_folders$wsw2_main, "model-objects", "trait-mpt-wsw-8b-with-ec.rds"))
summary(exp2_trait_model_8b_ec)
plotParam(exp2_trait_model_8b_ec)
```


```{r}
ratings_and_parameters_ec_long <- merge(
  data_list_exp2$rating
  , get_theta(model = exp2_trait_model_ec, data = data_list_exp2$mpt_data_hierarchical)
)

ratings_and_parameters_long <- merge(
  data_list_exp2$rating
  , get_theta(model = exp2_trait_model, data = data_list_exp2$mpt_data_hierarchical)
)

ratings_and_parameters_8b_long <- merge(
  data_list_exp2$rating
  , get_theta(exp2_trait_model_8b, data = data_list_exp2$mpt_data_hierarchical)
  )|>
  within({
    d <- ifelse(us_valence == "positive", d_positive, d_negative)
    C <- ifelse(us_valence == "positive", C_positive, C_negative)
  })

ratings_and_parameters_8b_long_ec <- merge(
  data_list_exp2$rating
  , get_theta(exp2_trait_model_8b_ec, data = data_list_exp2$mpt_data_hierarchical)
  )|>
  within({
    d <- ifelse(us_valence == "positive", d_positive, d_negative)
    C <- ifelse(us_valence == "positive", C_positive, C_negative)
  })

contrasts(ratings_and_parameters_ec_long$us_valence) <- "contr.sum"
contrasts(ratings_and_parameters_long$us_valence) <- "contr.sum"
contrasts(ratings_and_parameters_8b_long$us_valence) <- "contr.sum"
contrasts(ratings_and_parameters_8b_long_ec$us_valence) <- "contr.sum"

contrasts(ratings_and_parameters_ec_long$task_order) <- "contr.sum"
contrasts(ratings_and_parameters_long$task_order) <- "contr.sum"
contrasts(ratings_and_parameters_8b_long$task_order) <- "contr.sum"
contrasts(ratings_and_parameters_8b_long_ec$task_order) <- "contr.sum"
```

```{r}
ratings_and_parameters_ec <- merge(
  data_list_exp2$rating_wide
  , get_theta(model = exp2_trait_model_ec, data = data_list_exp2$mpt_data_hierarchical)
)

ratings_and_parameters <- merge(
  data_list_exp2$rating_wide
  , get_theta(model = exp2_trait_model, data = data_list_exp2$mpt_data_hierarchical)
)

both <- merge(ratings_and_parameters_ec,ratings_and_parameters, by = c("sid"))

cor.test(both$D.x,both$D.y)
# t.test(both$D.x,both$D.y)
# 
cor.test(both$C.x,both$C.y)
# t.test(both$C.x,both$C.y)
# 
cor.test(both$d.x,both$d.y)
# t.test(both$d.x,both$d.y)
# 
cor.test(both$b.x,both$b.y)
# t.test(both$b.x,both$b.y)
# 
cor.test(both$a.x,both$a.y)
# t.test(both$a.x,both$a.y)
```

```{r}
model5_exp2 <- afex::lmer(evaluative_rating ~ (cntr(D)+cntr(C)+cntr(d)+cntr(b)+cntr(a))*us_valence*task_order + (1|sid)
           , data = subset(ratings_and_parameters_8b_long,us_valence%in%c("positive","negative")))

ap_model5_exp2 <- apa_print(model5_exp2)
ap_model5_exp2$full_result
```

```{r}
ggeffect(model5_exp2, terms = ~ b:task_order) |> plot()
```

```{r}
summary(emtrends(model5_exp2,var= "b", specs= ~ task_order),infer=TRUE)
```

```{r}
ggeffect(model5_exp2, terms = ~ D:us_valence) |> plot()
```

```{r}
summary(emtrends(model5_exp2,var= "D", specs= ~ us_valence),infer=TRUE)
```

```{r}
ggeffect(model5_exp2, terms = ~ C:us_valence) |> plot()
```

```{r}
summary(emtrends(model5_exp2,var= "C", specs= ~ us_valence),infer=TRUE)
```

```{r}
ggeffect(model5_exp2, terms = ~ d:us_valence) |> plot()
```

```{r}
summary(emtrends(model5_exp2,var= "d", specs= ~ us_valence),infer=TRUE)
```

## Discussion

# Experiment 2

```{r}
# from MB:
data_list <- readRDS(file.path(study_folders$wsw3_main, "data", "data.rds"))
data_list$excluded_participants                       # Participant IDs
#lapply(data_list$excluded_participants, FUN = length) # How many?

# data_list_pilot <- readRDS(file.path(study_folders$wsw3_p2, "data", "data.rds"))

# data_list_joint <- readRDS(file.path(study_folders$wsw3_joint, "data", "data.rds"))

# data_list_pilot$rating$study <- "pilot"
# data_list_pilot$rating_wide$study <- "pilot"

data_list$rating$study <- "main"
data_list$rating_wide$study <- "main"

```

## Method
The experiment was pre-registered on the OSF.
The pre-registration, materials and data are publicly available at: https://osf.io/rqkvy/.
The data collection was conducted online.

### Participants

```{r}

demographics <- read.csv(file.path(study_folders$wsw3_main, "data-raw", "demographics.csv")) |>
  subset(Status == "APPROVED") |>
  within({
    Age <- as.integer(Age)
    Sex <- factor(Sex, levels = c("Female", "Male"))
  })

n_sex <- table(demographics$Sex)
prop_sex <- as.list(proportions(n_sex)) |>
  lapply(function(x) {
    paste0("$",apa_num(x * 100), "\\%$")
  })

age <- with(demographics, {
  paste0(
    "$M_\\mathrm{age} = "
    , apa_num(mean(Age))
    , "$, $\\mathit{SD}_\\mathrm{age} = "
    , apa_num(sd(Age))
    , "$"
  )
})

n_conditions <- split(data_list$rating, data_list$rating$task_focus) |>
  lapply(function(x) {paste0("$n = ", length(unique(x$sid)), "$")})
n_conditions <- setNames(n_conditions, papaja::sanitize_terms(tolower(names(n_conditions))))



```

Participants were recruited through Prolific and received monetary compensation for their participation.
We collected a quota sample with approximately equal shares of male and female participants.
The sampling pool was restricted to Prolific users (1) whose first language is English, (2) who live in the USA or in the United Kingdom, and (3) who have at least 20 previous submissions and a Prolific approval rate of at least 90%.
Prolific users who participated in previous pretests and experiments (conducted by the members of the current team of authors) using the same materials were excluded from the sampling pool.

We recruited 142 participants (`r prop_sex$Female` female; `r age`).
As pre-registered,
we excluded `r length(unique(unlist(data_list$excluded_participants)))` participants
who failed at least one control measure (by selecting at least one physical activity, by reporting a lack of attention or by responding with "I have just clicked through, please discard my data").
Taken together, this resulted in a final sample size of `r sum(n_sex)` participants (`r n_conditions$valence` in the valence focus condition and `r n_conditions$age` in the age focus condition).

The number of recruited participants was based on a pre-registered sampling plan (see OSF repository).
We pre-registered a minimum sample size (per learning task focus condition), a maximum sample size (across learning task focus conditions), and two stopping criteria (determining the [dis-]continuation of data collection within the pre-registered sample size range).
The minimum sample size ($N=52$ participants per learning task focus condition) was based on a power analyses for the two-way interaction between US valence and learning task focus.
In this power analysis, we targeted a test power of $1-\beta=.9$ and used an $\alpha$-level of $.1$ to implement a one-tailed test of the two-way interaction (reflecting our directional prediction about the interaction pattern).
The effect size of the US valence $\times$ Task focus interaction was set to $\eta^2_{p}\approx.079$ (corresponding, approximately, to one half of the effect size obtained in a pilot study).
In a first round of data collection, we recruited participants until the minimum sample size (after applying exclusion criteria) was reached.
Based on the available data, we then fitted the pre-registered MPT model (see below) and calculated Bayes factors comparing a nested version of the model with $d_\textrm{valence-focus}=d_\textrm{age-focus}$ to the baseline model with free $d_\textrm{valence-focus}$ and $d_\textrm{age-focus}$ (for details, see section "Data processing and statistical analyses").
The data collection was discontinued at this point because the first stopping criteria was met: the Bayes factor in favor of the baseline model exceeded 10 (i.e., we found strong evidence for the presence of a learning task focus effect on the $d$ parameter).

### Design
The experiment followed a 2 (US valence: positive vs. negative) $\times$ 2 (US age: young vs. old) $\times$ 2 (learning task focus: valence focus vs. age focus) mixed design.
The first factor varied within participants and the second factor varied between participants.

### Materials
The experiment was programmed with *lab.js* [@henninger_labjs_2022] and exported to an HTTPS-protected website with JATOS [@lange_just_2015].

The CS pool comprised 48 colored images of middle-aged human faces with neutral expressions (24 female faces, 24 male faces).
The images were taken from the FACES database (Ebner, Riediger, & Lindenberger, 2010) and optimized with an AI image optimization tool.

The US pool comprised 24 adjectives describing human traits, with six adjectives in each US valence $\times$ US age condition.
Based on a pilot study, we selected six adjectives describing traits that are positive and more typical for younger (than for older) people (*energetic*, *flexible*, *lively*, *open-minded*, *optimistic*, *strong* ), six adjectives describing traits that are positive and more typical for older (than for younger) people (*calm*, *dignified*, *nurturing*, *patient*, *realistic*, *wise*), six adjectives describing traits that are negative and more typical for younger (than for older) people (*careless*, *impulsive*, *naive*, *selfish*, *spoilt*, *self-absorbed*), and six adjectives describing traits that are negative and more typical for older (than for younger) people (*demented*, *feeble*, *frail*, *rigid*, *stubborn*, *self-weak*). 

For each participant, 12 randomly selected face images (50% female) served as positively paired CSs (i.e., they were paired with a positive US during the learning phase), 12 randomly selected face images (50% female) served as negatively paired CSs, and the remaining 24 face images served as distractor stimuli in the test phase.

For each participant, all 24 adjectives served as USs during the learning phase. The 24 adjectives were randomly assigned to the 24 CSs.
Six CSs (50% female) were paired with positive and "younger" USs, six CSs (50% female) were paired with positive and "older" USs, six CSs (50% female) were paired with negative and "younger" USs, and six CSs (50%) were paired with negative and "older" USs.

### Measures and procedures
All verbal materials were presented in English.
After providing informed consent and being asked to focus on the study,
participants were thanked for their participation and asked to carefully read all instructions and perform the tasks.
Subsequently, participants were presented with the instructions for the learning phase.

#### Learning phase
All participants read the following instructions:
"In the first part of the experiment you will be presented with photographs of faces (called 'faces' below) shown together with adjectives that denote human traits (called 'traits' below).
Each face will be paired with a single trait.
For each face-trait pair, the trait will be presented first. 
After a brief delay, the face will appear underneath.
Please pay close attention to all traits and faces.
Each face-trait pair will be presented for a limited time.
Your task will be to form an impression of each face-trait pair."

Next, depending on the learning task focus condition (valence focus or age focus), participants read:
"Specifically, you will have to indicate whether you think each face-trait pair is rather 'negative' ['typically old'] or rather 'positive' ['typically young'].
Press the spacebar to continue with the instructions."

The ordering of the two category labels in the presented sentence was randomly determined for each participant anew (valence focus condition: "positive first" vs. "negative first"; age focus condition: "typically old first" vs. "typically young first").
Note that the key assignment in the learning phase matched the ordering of the category labels in the instructions (e.g., when the "positive" label was mentioned first [i.e., further to the left in the sentence], the "positive" response was assigned to the left-hand key [A]).

The instructions continued as follows: 
"Next, you will be presented with the face-trait pairs.
Again, your task is to form an impression of each face-trait pair. You need to carefully look at both the faces and traits being presented.
Please indicate whether you think each pair is rather 'negative' ['typically old'] or rather 'positive' ['typically old'].
If you think the pair is rather 'negative' ['typically old'], press 'A' on your keyboard.
If you think the pair is rather 'positive' [typically young'], press 'L' on your keyboard. 
Although the presentation time is limited, 
please try your best to provide a response on each face-trait pair.
This part of the experiment will take about 8 minutes.
When you are ready, press the spacebar to start the task.
(This may take a few seconds.)"

Subsequently, participants worked through the learning task, consisting of 72 trials.
Each trial started with a US presented alone in green lower-case letters against a black background at the top of the screen. After 1000 ms, the CS appeared right below the US. 
The CS-US pair remained on screen for 4000 ms.
Participants had to respond within 3000 ms after the onset of the US (by pressing the "A" or "L" key).
If participants responded in time, three hyphens appeared below the CS-US pair right after participants had entered a response (to indicate that the response was recorded).
If participants did not respond in time (i.e., within the 3000 ms), the text "no response" appeared in red below the CS-US pair for 1000 ms.
The CS-US pair then disappeared and the message "No response. Press the spacebar to continue." was displayed (in red) at the bottom of the screen (without time limit).
The trials were separated by empty screens presented for 2500 ms.
For each participant, trial order was randomized in sets of 24 trials.
In each set (three in total), each of the 24 CS-US pairs was presented once.

After the learning phase, the following text was displayed: "The first part of the experiment is now finished! You have now seen all face-trait pairs and may continue with the second part of the experiment. 
Press the spacebar to continue."

#### Test phase
After the learning phase, participants entered the test phase. 
In the test phase, participants performed two measurement tasks: an evaluation task followed by a memory task. 
The order of the two tasks (evaluation task first) was the same for all participants.
The task instructions can be found in the pre-registration (see OSF repository).

##### Evaluation task
In the evaluation task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit. 
Participants rated how positive or negative they found each face on a 8-point Likert scale ranging from "very negative" (1) to "very positive" (8).
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

##### Memory task
In the memory task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit.
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

Each trial began with with the recognition memory task: participants were asked whether the face had been shown as part of the face-image pairs presented in the learning phase.
To indicate their response, participants were presented with two buttons labeled "Shown" and "Not shown".
(Note that we used these response labels [instead of "old" and "new"] to avoid confusion with the response labels in the age focus task.)
If participants responded "Not shown", they proceeded to the next recognition memory trial.
If participants responded "Shown", they proceeded to the associative memory task (in which they were asked to identify the previously paired US).
In this task, the face image was presented together with 16 adjectives (four adjectives per US valence $\times$ US age condition).
Note that all presented adjectives had been shown as USs during the learning phase.
The 16 adjectives were displayed as buttons organized in three rows (with six buttons in the upper two rows and four buttons in the bottom row).
For truly "shown" faces, the correct US was presented on a randomly selected button, while the remaining buttons were filled with 15 randomly selected USs that have been paired with other CSs.
For new faces (erroneously classified as "shown"), the buttons will be filled with 16 randomly selected USs. 
After clicking on one of the 16 USs, participants were presented with a blank screen (500 ms) followed by the next trial (i.e., they were presented with a screen showing another face, the recognition task question, and the two response options).

#### Control measures and debriefing
After the test phase, participants were presented with a total of three control measures (all of which were used were as exclusion criteria).
First, participants were presented with a screen showing four long sentences in a single paragraph.
The first three sentences referred to attitude research and through length and writing style was meant to discourage participants from reading the whole text. 
In the very last sentence of the text, participants were instructed to ignore the upcoming question about their exercise habits (in order to demonstrate that they had read the entire passage). 
On the next screen, participants were presented with a list of seven physical activities and were asked to indicate which of these activities they perform regularly (by ticking a small box next to the respective activity). 
After having ticked all relevant boxes (or none at all), participants proceeded to the next screen by clicking on the "Continue" button displayed at the bottom of the screen.
Subsequently, participants were asked whether they paid attention to the nonwords and images presented throughout the experiment.
The response options ("Yes" vs. "No") were presented as two buttons.
Next, participants were asked whether they took the requested responses seriously [based on @aust_seriousness_2013].
The response options ("I have taken the requested responses seriously" vs. "I have just clicked through, please discard my data") were again presented as two buttons.
Afterwards, participants were given the chance to comment on the study.
Finally, participants were debriefed about the purpose of the study and then redirected to Prolific.

### Data processing and statistical analysis

## Results

```{r}
exp3_trait_model <- readRDS(file.path(study_folders$wsw3_main, "model-objects", "trait-mpt.rds"))

exp3_trait_model_8b <- readRDS(file.path(study_folders$wsw3_main, "model-objects", "trait-mpt-wsw-8b.rds"))

exp3_trait_model_ec <- readRDS(file.path(study_folders$wsw3_main, "model-objects", "trait-mpt-with-ec.rds"))

exp3_trait_model_8b_ec <- readRDS(file.path(study_folders$wsw3_main, "model-objects", "trait-mpt-wsw-8b-with-ec.rds"))

# exp3pilot_trait_model <- readRDS(file.path(study_folders$wsw3_p2, "model-objects", "trait-mpt.rds"))
# 
# exp3pilot_trait_model_8b <- readRDS(file.path(study_folders$wsw3_p2, "model-objects", "trait-mpt-wsw-8b.rds"))

# exp3joint_trait_model <- readRDS(file.path(study_folders$wsw3_joint, "model-objects", "trait-mpt.rds"))
# exp3joint_trait_model_8b <- readRDS(file.path(study_folders$wsw3_joint, "model-objects", "trait-mpt-wsw-8b.rds"))
```

```{r}
summary(exp3_trait_model)
summary(exp3_trait_model_8b)
```

```{r}
ratings_and_parameters_ec_long <- merge(
  data_list$rating
  , get_theta(model = exp3_trait_model_ec, data = data_list$mpt_data_hierarchical)
)

ratings_and_parameters_long <- merge(
  data_list$rating
  , get_theta(model = exp3_trait_model, data = data_list$mpt_data_hierarchical)
)

contrasts(ratings_and_parameters_ec_long$us_valence) <- "contr.sum"
contrasts(ratings_and_parameters_long$us_valence) <- "contr.sum"
contrasts(ratings_and_parameters_ec_long$task_focus) <- "contr.sum"
contrasts(ratings_and_parameters_long$task_focus) <- "contr.sum"
```

```{r}
model_exp3 <- afex::lmer(evaluative_rating ~ (cntr(D)+cntr(C)+cntr(d)+cntr(b)+cntr(a))*us_valence*task_focus + (1|sid)
           , data = ratings_and_parameters_ec_long)
ap_model_exp3 <- apa_print(model_exp3)
ap_model_exp3$table
```

```{r}
summary(emtrends(model_exp3,var= "C", specs= ~ us_valence),infer=TRUE)
```


## Discussion 

# General discussion

# References

::: {#refs custom-style="Bibliography"}
:::


# (APPENDIX) Appendix {-}

```{r child = file.path(project_root, "Paper/appendix.rmd"), eval = TRUE}
```



