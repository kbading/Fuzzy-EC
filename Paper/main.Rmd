---
title             : "Measuring memory involved in evaluative conditioning: a multinomial modeling approach"
shorttitle        : "Memory in evaluative conditioning"

author: 
  - name          : "Karoline Bading"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Schleichstraße 4, 72074 Tübingen (Germany)"
    email         : "karoline.bading@uni-tuebingen.de"
  - name          : "Jérémy Béna"
    affiliation   : "2"
    # role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
    #   - "Conceptualization"
    #   - "Writing - Original Draft Preparation"
    #   - "Writing - Review & Editing"
  - name          : "Marius Barth"
    affiliation   : "3"
    # role:
    #   - "Writing - Review & Editing"
    #   - "Supervision"
  - name          : "Klaus Rothermund"
    affiliation   : "4"
    # role:
    #   - "Writing - Review & Editing"
    #   - "Supervision"
      
affiliation:
  - id            : "1"
    institution   : "University of Tübingen"
  - id            : "2"
    institution   : "Aix-Marseille University"
  - id            : "3"
    institution   : "University of Cologne"
  - id            : "4"
    institution   : "Friedrich Schiller University Jena"
    


abstract: |
  The present research is aimed at introducing a refined perspective on different forms of pairing memory, on the one hand, and introducing improved tools for measuring US identity, US valence and other forms of EC-related memory, on the other hand. We identify several shortcomings in common measures of CS-US pairing memory and explain their problematic implications for testing hypotheses about the role of US identity and US valence memory in the emergence of EC. We then introduce a multinomial processing tree (MPT) model that solves the previously identified problems and may therefore bring methodological progress in measuring different forms of EC-related memory. Finally, we present three experiments demonstrating the suitability and power of the proposed model for studying the relationship between memory and EC.

  <!-- https://tinyurl.com/ybremelq -->
  
  
authornote: |
  Karoline Bading and Jérémy Béna share first authorship.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : '`r file.path(rprojroot::find_rstudio_root_file(), "..", "methexp.bib")`'
floatsintext      : yes
figsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : jou
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(afex)
library(emmeans)
library(ggeffects)
library(papaja)
library(dplyr)
library(MPTinR)
library(TreeBUGS)
library(HMMTreeC)

if(!requireNamespace("magick", quietly = FALSE)) install.packages("magick")
set_sum_contrasts()

r_refs("r-references.bib")

project_root <- rprojroot::find_rstudio_root_file()

study_folder_pilot <- file.path(
  project_root
  , "pilot_data_analyses"
)

study_folder_main <- file.path(
  project_root
  , "Study 2"
)

study_folders <- list(
    wsw1 = file.path(project_root, "studies", "wsw1")
  , wsw2_main = file.path(project_root, "studies", "wsw2-main")
  , wsw3_main = file.path(project_root, "studies", "wsw3-main")
  , wsw3_p2   = file.path(project_root, "studies", "wsw3-p2")
  , wsw3_joint  = file.path(project_root, "studies", "wsw3-joint-analysis")
)

source(file.path(project_root, "R", "apa_print_treebugs.R"))
source(file.path(project_root, "R", "mptinr_helper.R"))
source(file.path(project_root, "R", "treestan_helper.R"))

knitr::opts_chunk$set(
  echo = FALSE
  , warning = FALSE
  , message = FALSE
  , cache = FALSE
  , fig.env = "figure*"
  , fig.crop = TRUE
  # , fig.width  = 15
  # , fig.height =  9
)

cntr <- function(x, ...){x - mean(x, ...)}
```

```{r analysis-preferences}
# Seed for random number generation
# set.seed(42)
# knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Evaluative conditioning (EC) refers to a change in the evaluation of an initially neutral "conditioned" stimulus (CS) after its pairing with a positive or negative "unconditioned" stimulus (US).
As a tool for studying the cognitive processes involved in attitude formation, the EC paradigm is widely used in social psychology and in research on evaluative learning.
Over the years, these two fields of research have produced a large and diverse set of EC accounts (e.g., Levey & Martin,
1975; Baeyens, Eelen, Crombez, & Van den Bergh, 1992; Jones, Fazio, & Olson, 2009; Walther, Gawronski, Blank, &
Langer, 2009; De Houwer, 2018; Gawronski & Bodenhausen, 2018; Stahl & Aust, 2018; Walther, Halbweisen & Blast, 2018).
Despite their many differences, most of these accounts share a core idea: one way or another, they all give central importance to the formation and retrieval of memory for the CS-US pairings as the cognitive foundation for the emergence of EC.
Based on a widely cited meta-analysis [@hofmann_evaluative_2010], this shared assumption seems to be well founded: pairing memory (as a proxy of contingency awareness) was found to be the strongest moderator of EC.

In early research, pairing memory was mainly understood as associative memory for the specific US that had been paired with a given CS [e.g., @baeyens_contingency_1990; Walther & Nagengast, 2006].
However, as explained by @stahl_evaluative_2009, such US identity memory is not the only form of pairing memory that might be relevant for EC: instead of (or in addition to) being built on memory for the previously paired USs, EC effects may also stem from memory for the valence of these stimuli [@stahl_respective_2009-1].
This seminal distinction between memory for the US identity and memory for the US valence is important in EC research for multiple reasons.
On the methodological side, US valence memory is less specific and thus more exhaustive than US identity memory, making it the more suitable construct for testing the intriguing possibility of EC without memory for the CS-US pairings (Hütter et al., 2012; Stahl et al., 2009; Shanks and St. John, 1994).
On the theoretical side, the distinction between US identity and US valence memory is crucial because different accounts of EC draw on different types of pairing memory as its cognitive foundation (e.g., Walther, Gawronski, Blank, &
Langer, 2009; Gast & Rothermund, 2011).
Importantly, this implies that conceptualizing (and measuring) US identity and US valence memory as separate constructs is required to achieve theoretical progress in explaining EC effects.
In this vein, the present research is aimed at providing a refined perspective on different forms of CS-US pairing memory, on the one hand, and introducing superior tools for measuring US identity, US valence and other forms of EC-related memory, on the other hand.

The remainder of this article is structured as follows.
Firstly, we identify several shortcomings in common US identity and US valence measures and explain their problematic implications for testing various hypotheses about the role of CS-US pairing memory in the emergence of EC.
Secondly, we introduce a multinomial processing tree (MPT) model that solves the previously identified problems and thus brings methodological progress in measuring different forms of EC-related memory.
Thirdly, we present three experiments showing how the MPT model can be estimated in the context of a standard EC procedure.
Taken together, these experiments demonstrate the suitability and power of the proposed model for studying the cognitive foundations of EC.

# Common measures of CS-US pairing memory and their shortcomings
In a typical procedure for measuring US identity memory, individual CSs are presented together with a fixed number of USs (including the previously paired/correct one).
For a given CS thus presented, the available USs may all have the same valence (equal to that of the previously paired US; e.g., Stahl & Bading, 2020) or include both positive and negative options (typically in equal numbers; e.g., Gast & Rothermund, 2011).
For each CS, participants are asked to select the correct (i.e., previously paired) option from the set of available USs.
For cases in which US identity memory is absent, participants may be instructed to guess the correct option or be provided with an additional "I don't know" option (e.g., Stahl & Bading, 2020).
The typical format for measuring US identity memory is thus forced-choice recognition with a chance level equal to the inverse of the number of response options (USs).

Memory for the US valence is usually measured in one of two ways: either by recoding responses from the US identity measure in terms of their valence (e.g., Gast & Rothermund, 2011) or by implementing a separate procedure in which US valence memory is probed directly (e.g., Stahl & Bading, 2020).
In a typical example for such a procedure, participants are presented with individual CSs and asked to indicate, for a given CS, whether it had been paired with positive or negative stimuli during the learning phase.
For cases in which such US valence memory is not available, participants may be instructed to guess the correct valence or be provided with an additional "I don't know" option.
The chance level in a typical US valence measure is thus $.5$ (corresponding to the inverse of the two US valences represented in the measure) and therefore higher than that of typical US identity measures (assuming that more than two USs are included as response options).

Though widely used, these pairing measures have been criticized before.
Most importantly, Hütter et al. (2012) explained that standard measures of US valence memory may be affected by a phenomenon called affect-as-information.
Affect-as-information happens when participants cannot remember the valence of the previously paired US and then rely on their affective reaction towards the CS to select a response in the US valence measure.
As shown by Hütter et al. (2021), affect-as-information may create artifactual correlations between measures of US valence memory and EC effects, resulting in a bias against empirical demonstrations of EC without memory for the CS-US pairings.
In the next section, we will add to this critique by showing that affect-as-information (as a source of responding in measures of US identity and US valence memory) also interferes with separating the respective contributions of different types of pairing memory in the emergence of EC.

## US valence may be inferred from US identity

```{r cache = TRUE}
IDs <- 100
CSs <- 24
val <- c(rep("positive",12),rep("negative",12))
z_USid <- rnorm(IDs,0,1)
z_USval <- rnorm(IDs,0,1)

ID_vector <- rep(1:IDs,each = CSs)
CS_vector <- rep(1:CSs,IDs)
val_vector <- rep(val,IDs)
USid_vector <- rep(z_USid,each = CSs)
USval_vector <- rep(z_USval,each = CSs)

sim_data <- data.frame(id = ID_vector
                       , cs = CS_vector
                       , us_valence = val_vector
                       , p_USid = pnorm(USid_vector)
                       , p_USval = 0
                       )

for(i in 1:nrow(sim_data)){
  sim_data$USid_mem[i] <- rbinom(1,1,sim_data$p_USid[i])
  sim_data$USval_mem[i] <- rbinom(1,1,sim_data$p_USval[i])
}

sim_data$cs_rating <- NA
sim_data$USid_guess <- NA
sim_data$USval_guess <- NA
sim_data$USid_resp <- NA
sim_data$USval_resp <- NA

for(i in 1:nrow(sim_data)){
  if(sim_data$USid_mem[i]==0){
    sim_data$USval_guess[i] <- rbinom(1,1,0)#rbinom(1,1,0.5)
    if(sim_data$USval_guess[i]==1){
      sim_data$USid_guess[i] <- rbinom(1,1,.25)
    }
    if(sim_data$USval_guess[i]==0){
      sim_data$USid_guess[i] <- 0
    }
  }
  if(sim_data$USid_mem[i]==1){
    sim_data$USval_guess[i] <- 0
    sim_data$USid_guess[i] <- 0
  }
}

for(i in 1:nrow(sim_data)){
  if(sim_data$us_valence[i]=="positive" & sim_data$USid_mem[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,0.5,1)
    sim_data$USid_resp[i] <- 1
    sim_data$USval_resp[i] <- 1 #rbinom(1,1,.5)
  }
  if(sim_data$us_valence[i]=="positive" & sim_data$USid_mem[i]==0 & sim_data$USval_guess[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
  if(sim_data$us_valence[i]=="positive" & sim_data$USid_mem[i]==0 & sim_data$USval_guess[i]==0){
    sim_data$cs_rating[i] <- rnorm(1,-0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
  
  if(sim_data$us_valence[i]=="negative" & sim_data$USid_mem[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,-0.5,1)
    sim_data$USid_resp[i] <- 1
    sim_data$USval_resp[i] <- 1 #rbinom(1,1,.5)
  }
  if(sim_data$us_valence[i]=="negative" & sim_data$USid_mem[i]==0 & sim_data$USval_guess[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,-0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
  if(sim_data$us_valence[i]=="negative" & sim_data$USid_mem[i]==0 & sim_data$USval_guess[i]==0){
    sim_data$cs_rating[i] <- rnorm(1,0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
}

eval_agg3b <- aggregate(sim_data,FUN=mean,cs_rating~id*us_valence)
USid_agg3b <- aggregate(sim_data,FUN=mean,USid_resp~id*us_valence)
USval_agg3b <- aggregate(sim_data,FUN=mean,USval_resp~id*us_valence)
data_agg3b <- merge(eval_agg3b,USid_agg3b,by = c("id","us_valence"))
data_agg3b <- merge(data_agg3b,USval_agg3b,by = c("id","us_valence"))

data_agg3b$us_valence <- factor(data_agg3b$us_valence, levels = c("positive","negative"))
contrasts(data_agg3b$us_valence) <- "contr.sum"
# summary(lm(data=data_agg3b, cs_rating ~ us_valence))
# summary(lm(data=data_agg3b, cs_rating ~ USid_resp * us_valence))
# summary(lm(data=data_agg3b, cs_rating ~ USval_resp * us_valence))
# summary(lm(data=data_agg3b, cs_rating ~ USid_resp * USval_resp * us_valence))
```

A first shortcoming in current pairing memory measures is the fact that US valence is perfectly inferable from US identity.
This implies that, in the presence of US identity memory, indicators of US valence memory may serve as manipulation checks (to probe whether specific USs were interpreted as intended by the experimenter), but cannot be taken to measure an independent form of CS-US pairing memory.

To illustrate this point, imagine a sample of participants who each formed US identity memory for the majority of CSs presented during the learning phase.
In the CS evaluation measure, these participants then use their US identity memory to derive CS evaluations, resorting to unbiased guessing whenever they cannot remember the previously paired US.
In the two measures of CS-US pairing memory, the participants give correct responses whenever US identity memory is present (using their recollection of the correct US to infer the correct valence) and resort to unbiased guessing whenever US identity memory is absent.
As long as guessing processes are uncorrelated across measures (CS evaluation, US identity, US valence), such a sample will produce a positive correlation between the two memory measures as well as comparably strong (positive) correlations between either memory measure and the measure of CS evaluation.
Importantly, the multicollinearity between the two memory measures will make it statistically impossible to separate the respective contributions of the two types of pairing memory (e.g., in a linear regression predicting CS evaluations from US valence and the two memory measures), thereby concealing the perfect association between genuine EC effects and memory for the US identity.

When CS evaluation and memory measures are affected by independent guessing processes, the inferability of US valence from US identity thus undermines evidence for the role of US identity memory in the emergence of EC effects.
Importantly, matters are even worse when guessing processes are correlated across measures (due to affect-as-information): in this case, evidence for the relationship between EC effects and US identity memory will be nullified and replaced by artifactual evidence for an association between EC effects and US valence memory.
To illustrate this point, imagine the sample of participants now using their pre-existing attitudes (towards the CSs) to generate responses in the CS evaluation and memory measures whenever they cannot remember the previously paired US.
In the measure of US valence memory, there is a one-to-one mapping of a pre-existing attitude onto a single response option in the measurement task, resulting in a strong (positive) correlation between CS evaluations and memory responses driven by affect-as-information.
In the US identity measure, however, the pre-existing attitude can be mapped onto several USs (those with the same valence as the pre-existing attitude), so that selecting the correct US involves an additional guessing process (on top of affect-as-information).
As a consequence of this one-to-many mapping, the subset of CSs without US identity memory will show a weaker (positive) correlation between CS evaluations and the US identity measure than between CS evaluations and the US valence measure.
When mixed with CSs for which US identity memory is present, CSs without US identity memory will thus add variance in the dependent variable (CS evaluations) that is more strongly associated with the US valence measure than with the US identity measure (whereas, among CS with US identity memory, variance in the dependent variable is equally associated with both memory measures).
In a linear regression including both memory measures as predictors, this data structure will result in a strong association between EC effects and the US valence measure, while nullifying the statistical relationship between EC effects and the US identity measure.
When combined with affect-as-information, the inferability of US valence from US identity thus poses a serious methodological problem: even if every genuinely conditioned CS evaluation is based on US identity memory, statistical analyses will suggest that US valence memory is the sole predictor of EC.

## US valence may be used to guess US identity

```{r cache = TRUE}
IDs <- 100
CSs <- 24
val <- c(rep("positive",12),rep("negative",12))
z_USid <- rnorm(IDs,0,1)
z_USval <- rnorm(IDs,0,1)

ID_vector <- rep(1:IDs,each = CSs)
CS_vector <- rep(1:CSs,IDs)
val_vector <- rep(val,IDs)
USid_vector <- rep(z_USid,each = CSs)
USval_vector <- rep(z_USval,each = CSs)

sim_data <- data.frame(id = ID_vector
                       , cs = CS_vector
                       , us_valence = val_vector
                       , p_USid = 0
                       , p_USval = pnorm(USval_vector)
                       )

for(i in 1:nrow(sim_data)){
  sim_data$USid_mem[i] <- rbinom(1,1,sim_data$p_USid[i])
  sim_data$USval_mem[i] <- rbinom(1,1,sim_data$p_USval[i])
}

sim_data$cs_rating <- NA
sim_data$USid_guess <- NA
sim_data$USval_guess <- NA
sim_data$USid_resp <- NA
sim_data$USval_resp <- NA

for(i in 1:nrow(sim_data)){
  if(sim_data$USval_mem[i]==0){
    sim_data$USval_guess[i] <- rbinom(1,1,0.5)
    if(sim_data$USval_guess[i]==1){
      sim_data$USid_guess[i] <- rbinom(1,1,.25)
    }
    if(sim_data$USval_guess[i]==0){
      sim_data$USid_guess[i] <- 0
    }
  }
  if(sim_data$USval_mem[i]==1){
    sim_data$USval_guess[i] <- 0
    sim_data$USid_guess[i] <- rbinom(1,1,.25)
  }
}

for(i in 1:nrow(sim_data)){
  if(sim_data$us_valence[i]=="positive" & sim_data$USval_mem[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- 1
  }
  if(sim_data$us_valence[i]=="positive" & sim_data$USval_mem[i]==0 & sim_data$USval_guess[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
  if(sim_data$us_valence[i]=="positive" & sim_data$USval_mem[i]==0 & sim_data$USval_guess[i]==0){
    sim_data$cs_rating[i] <- rnorm(1,-0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
  
  if(sim_data$us_valence[i]=="negative" & sim_data$USval_mem[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,-0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- 1
  }
  
  if(sim_data$us_valence[i]=="negative" & sim_data$USval_mem[i]==0 & sim_data$USval_guess[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,-0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
  
  if(sim_data$us_valence[i]=="negative" & sim_data$USval_mem[i]==0 & sim_data$USval_guess[i]==0){
    sim_data$cs_rating[i] <- rnorm(1,0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
}

eval_agg3b <- aggregate(sim_data,FUN=mean,cs_rating~id*us_valence)
USid_agg3b <- aggregate(sim_data,FUN=mean,USid_resp~id*us_valence)
USval_agg3b <- aggregate(sim_data,FUN=mean,USval_resp~id*us_valence)
data_agg3b <- merge(eval_agg3b,USid_agg3b,by = c("id","us_valence"))
data_agg3b <- merge(data_agg3b,USval_agg3b,by = c("id","us_valence"))

data_agg3b$us_valence <- factor(data_agg3b$us_valence, levels = c("positive","negative"))
contrasts(data_agg3b$us_valence) <- "contr.sum"
#summary(lm(data=data_agg3b, cs_rating ~ us_valence))
#summary(lm(data=data_agg3b, cs_rating ~ USid_resp * us_valence))
#summary(lm(data=data_agg3b, cs_rating ~ USval_resp * us_valence))
#summary(lm(data=data_agg3b, cs_rating ~ USid_resp * USval_resp * us_valence))
```

A second shortcoming in current measures of CS-US pairing memory is the fact that memory for the US valence can be used to help guess the previously paired US (by restricting the number of possibly correct USs to those that have the correct US valence).
Fortunately, this shortcoming can be avoided by using measures that include only USs that have the correct US valence (e.g., Stahl & Bading, 2020).
However, in US identity measures that include positive and negative USs, this shortcoming can lead to erroneous conclusions about the relationship between EC and US identity memory (when US valence memory is not controlled for).

To illustrate this point, imagine now a sample of participants who did not form any US identity memory but can remember the correct US valence for the majority of CSs presented during the learning phase.
In the CS evaluation measure, these participant then use their US valence memory to derive CS evaluations, resorting to unbiased guessing or pre-existing attitudes whenever they cannot remember the US valence.
In the US valence measure, the participants give correct responses whenever US valence memory is present and resort to unbiased guessing or pre-existing attitudes whenever US valence memory is absent.
In the US identity measure (showing both positive and negative USs), the participants perform informed guessing when US valence memory is present (by considering only USs that have the correct valence) and resort to unbiased guessing (or informed guessing based on pre-existing attitudes) whenever US valence memory is absent.

Regardless of whether guessing processes (across measures) are independent or correlated due to affect-as-information, such a sample will produce a positive correlation between the two memory measures as well as positive correlations between either memory measure and the measure of CS evaluation.
This implies that analyses that do not control for US valence memory will erroneously suggest that EC effects were somehow related to US identity memory (when, in fact, no such memory was present in the sample of participants).
However, since the US identity measure will not explain any additional variance in the CS evaluations (over and above to what is already explained by the US valence measure), a linear regression that includes both memory measures as predictors will rightfully reveal US valence memory as the sole predictor of EC effects.

In principle, the second shortcoming is thus less problematic than the first (inferability of US valence from US identity): as long as US valence memory is controlled for, the true relationships between EC effects and the two types of pairing memory may be revealed by standard statistical analyses.
However, it is important to note that such analyses will produce results that are indistinguishable from those obtained in the earlier thought experiment (where genuine EC effects were based on US identity memory).
This implies that, unfortunately, common measures of pairing memory may not even deliver strong conclusions (about the respective contributions of US identity and US valence memory) when they reflect the true state of affairs.

## US identity and and US valence measures may be biased by response tendencies

```{r cache = TRUE}
IDs <- 300
CSs <- 100
val <- c(rep("positive",50),rep("negative",50))
z_USid <- rnorm(IDs,0,1)
z_USvalpos <- rnorm(IDs,-1,1)
z_USvalneg <- rnorm(IDs,-.5,1)

ID_vector <- rep(1:IDs,each = CSs)
CS_vector <- rep(1:CSs,IDs)
val_vector <- rep(val,IDs)
USid_vector <- rep(z_USid,each = CSs)
USvalpos_vector <- rep(z_USvalpos,each = CSs)
USvalneg_vector <- rep(z_USvalneg,each = CSs)

sim_data <- data.frame(id = ID_vector
                       , cs = CS_vector
                       , us_valence = val_vector
                       , p_USid = 0
                       , p_USvalpos = pnorm(USvalpos_vector)
                       , p_USvalneg = pnorm(USvalneg_vector)
                       )

for(i in 1:nrow(sim_data)){
  sim_data$USid_mem[i] <- rbinom(1,1,sim_data$p_USid[i])
  if(sim_data$us_valence[i]=="positive"){
    sim_data$USval_mem[i] <- rbinom(1,1,sim_data$p_USvalpos)
  }
  if(sim_data$us_valence[i]=="negative"){
    sim_data$USval_mem[i] <- rbinom(1,1,sim_data$p_USvalneg)
  }
}

sim_data$cs_rating <- NA
sim_data$USid_guess <- NA
sim_data$USval_guess <- NA
sim_data$USid_resp <- NA
sim_data$USval_resp <- NA

for(i in 1:nrow(sim_data)){
  if(sim_data$USval_mem[i]==0){
    if(sim_data$us_valence[i]=="positive"){
      sim_data$USval_guess[i] <- rbinom(1,1,.8)
    }
    if(sim_data$us_valence[i]=="negative"){
      sim_data$USval_guess[i] <- rbinom(1,1,.2)
    }
    if(sim_data$USval_guess[i]==1){
      sim_data$USid_guess[i] <- rbinom(1,1,.25)
    }
    if(sim_data$USval_guess[i]==0){
      sim_data$USid_guess[i] <- 0
    }
  }
  if(sim_data$USval_mem[i]==1){
    sim_data$USval_guess[i] <- 0
    sim_data$USid_guess[i] <- rbinom(1,1,.25)
  }
}

for(i in 1:nrow(sim_data)){
  if(sim_data$us_valence[i]=="positive" & sim_data$USval_mem[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- 1
  }
  if(sim_data$us_valence[i]=="positive" & sim_data$USval_mem[i]==0 & sim_data$USval_guess[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
  if(sim_data$us_valence[i]=="positive" & sim_data$USval_mem[i]==0 & sim_data$USval_guess[i]==0){
    sim_data$cs_rating[i] <- rnorm(1,-0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
  
  if(sim_data$us_valence[i]=="negative" & sim_data$USval_mem[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,-0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- 1
  }
  
  if(sim_data$us_valence[i]=="negative" & sim_data$USval_mem[i]==0 & sim_data$USval_guess[i]==1){
    sim_data$cs_rating[i] <- rnorm(1,-0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
  
  if(sim_data$us_valence[i]=="negative" & sim_data$USval_mem[i]==0 & sim_data$USval_guess[i]==0){
    sim_data$cs_rating[i] <- rnorm(1,0.5,1)
    sim_data$USid_resp[i] <- sim_data$USid_guess[i]
    sim_data$USval_resp[i] <- sim_data$USval_guess[i]
  }
}

eval_agg3b <- aggregate(sim_data,FUN=mean,cs_rating~id*us_valence)
USid_agg3b <- aggregate(sim_data,FUN=mean,USid_resp~id*us_valence)
USval_agg3b <- aggregate(sim_data,FUN=mean,USval_resp~id*us_valence)
data_agg3b <- merge(eval_agg3b,USid_agg3b,by = c("id","us_valence"))
data_agg3b <- merge(data_agg3b,USval_agg3b,by = c("id","us_valence"))

data_agg3b$us_valence <- factor(data_agg3b$us_valence, levels = c("positive","negative"))

# apa_barplot(data_agg3b
#             , id = "id"
#             , dv = "USid_resp"
#             , factors = c("us_valence"))
# 
# apa_barplot(sim_data
#             , id = "id"
#             , dv = "USval_resp"
#             , factors = c("us_valence"))

# apa_barplot(sim_data
#             , id = "id"
#             , dv = "USval_mem"
#             , factors = c("us_valence"))

contrasts(data_agg3b$us_valence) <- "contr.sum"
#summary(lm(data=data_agg3b, cs_rating ~ us_valence))
#summary(lm(data=data_agg3b, cs_rating ~ USid_resp * us_valence))
#summary(lm(data=data_agg3b, cs_rating ~ USval_resp * us_valence))
#summary(lm(data=data_agg3b, cs_rating ~ USid_resp * USval_resp * us_valence))
#mod4 <- lm(data=data_agg3b, cs_rating ~ USid_resp * USval_resp * us_valence)
```

```{r}
#library(emmeans)
#emtrends(mod4, specs = ~ us_valence, var = "USval_resp",infer=TRUE)
```

A third shortcoming in common pairing memory measures is the fact that their forced-choice format requires guessing (when the probed memory is absent), making them susceptible to bias from response tendencies towards positive (or negative) response options.
When pairing memory is strong (e.g., because the learning phase included only a small number of distinct CS-US pairings), this shortcoming will be of little consequence (since guessing will be required only rarely).
However, when pairing memory is rather low (e.g., because many different CS-US pairings were presented during the learning phase) and response tendencies are strong (e.g., due to a skewed US valence distribution during learning), biased guessing can cause severe problems in the estimation of CS-US pairing memory.

To illustrate this point, imagine a sample of participants who worked through a learning phase that showed many different CSs of which a majority was paired with positive USs (while the remaining CSs were paired with negative USs).
Due to the overall large number of presented stimuli, participants did not form any US identity memory but managed to develop US valence memory for a subset of CSs.
Imagine, further, that the rarity of negative stimuli (in the learning phase) produced a processing advantage for CS-US pairings containing negative USs (Sperlich & Unkelbach, 2025), so that participants possessed relatively better US valence memory among negatively paired CSs than among positively paired CSs.
Finally, imagine that, due to the skewed US valence distribution during learning, a majority of participants developed a strong tendency towards positive response options (since many more CS-US pairings included positive USs).

In the CS evaluation measure, participants then use whatever US valence memory they developed (to derive CS evaluations), resorting to guessing whenever they cannot remember the US valence for a given CS.
Similarly, in the US valence measure, participants give correct responses whenever US valence memory is present and resort to guessing whenever US valence memory is absent.
Finally, in the US identity measure (including equal numbers of positive and negative USs), participants select responses based on a guessing process that is informed by US valence memory whenever present.
Because of the response tendency developed by the majority of participants, the guessing processes will be correlated across measures and, most importantly, strongly biased towards positive response options.
As a consequence, the US valence and US identity measures will show a relatively smaller share of correct responses among negatively paired CSs than among positively paired CSs, despite the fact that participants formed better US valence memory for the former than for the latter.

Note that the imagined sample will also show larger mean EC effects for positively paired CSs (than for negatively paired CSs), despite the fact that genuinely conditioned attitudes are relatively more common among negatively paired CSs (than among positively paired CSs).
Taken together, uncontrolled bias from response tendencies (towards positive or negative response options) may thus lead to erroneous conclusions not only about CS-US pairing memory but also about EC effects themselves.

## US identity and and US valence measures may confound CS-US pairing and CS recognition memory
As a fourth and final shortcoming, we address the fact that common pairing memory measures may confound CS-US pairing memory (the ability to remember the correct US or US valence for a given CS) and CS recognition memory (the ability to remember the CS itself).
Empirically, this confound occurs when participants possess less-than-perfect CS recognition memory and thus resort to guessing in the US identity and US valence measures.
Since lack of CS recognition memory and lack of CS-US pairing memory (for remembered CSs) have the same consequence (response generation based on guessing or affect-as-information), distinguishing between these two types of memory may seem inconsequential at first sight.
However, there are a number of questions in EC research that do require a clear distinction between CS recognition and CS-US pairing memory.

For example, in a widely cited study, Gast and Rothermund (2011) tested whether EC effects depend on a so-called valence focus, that is, a "tendency to attend to valent features ... and to evaluate objects" (p. 90) while observing the CS-US pairings.
To investigate this possibility, Gast and Rothermund (2011) compared "valence task" conditions (where participants performed an evaluative judgment task during the learning phase) with control conditions (where participants performed a non-evaluative judgment task).
In three experiments (using different non-evaluative judgment tasks), Gast and Rothermund (2011) found stronger EC effects and better US valence memory in the "valence task" condition than in the respective control condition.
At first sight, these findings seem to demonstrate that EC effects were indeed boosted by a valence focus that resulted in stronger memory links between individual CSs and US valences.
Importantly, such an interpretation rests on the assumption that the evaluative judgment task did actually improve US valence memory and not some other form of EC-related memory that is not connected to US valence processing (but nevertheless affects performance in the US valence measure).

In this vein, it seems at least possible that different judgment tasks affect participants' overall ability to remember individual CSs (e.g., by directing attention towards the USs) while producing equally strong memory links in the subset of CSs that can actually be remembered.
Similarly, the effects of experimental procedures that prevent conscious processing or reduce cognitive resources during learning may also be mediated primarily by reductions in CS recognition memory  (e.g., Stahl, Haaf, & Corneille, 2016; Mierop, Hütter, & Corneille, 2017).
However, since current memory measures do not distinguish between lack of CS recognition memory and lack of CS-US pairing memory, response data from these measures may erroneously suggest that evaluative judgment tasks, suppression of conscious processing and resource depletion (as well as many other manipulations) moderate EC effects via targeted changes in CS-US pairing memory.

# A multinomial processing tree model for memory involved in evaluative conditioning
As explained above, current measures of CS-US pairing memory possess a number of shortcomings that have negative implications for investigating a wide range of research questions.
To obtain conclusive answers to these questions, improved methods for measuring EC-related memory are therefore needed.
Fortunately, the identified shortcomings are not unique to measures of CS-US pairing memory.
Instead, they show structural equivalence with the shortcomings of similar memory measures that have been overcome in previous research [e.g., @bayen_source_1996; @klauer_who_1998].

@klauer_who_1998 proposed an MPT model that disentangles several confounds in the standard measure of social categorization in the "Who said what?" paradigm (Taylor, Fiske, Etcoff, & Ruderman, 1978).
In the "Who said what?" paradigm, participants listen to tape-recorded statements that are made by different individuals from two social groups (e.g., people with black vs. white skin).
In the subsequent measure of social categorization, participants are presented with individual statements and have to select the correct speaker from a set of response options that include individuals from both groups. 
In its structure, this memory measure bears obvious resemblance to US identity measures in which participants are presented with individual CSs and have to select the correct US from a set of positive and negative response options.
Moreover, standard usage of this memory measure involves the recoding of the selected speakers in terms of their social group, thereby mirroring US valence measures that result from recoding the selected USs in terms of their valence.
Finally, and in line with many US identity and US valence measures, the memory measure in the "Who said what?" paradigm does not include response options for indicating that a given statement or its speaker cannot be remembered (making it equally prone to confounding with recognition memory and to bias from response tendencies towards certain response options).

Based on the structural similarities between measures, we considered the possibility that the problems involved in the measurement of CS-US pairing memory could be overcome with the same tool that was introduced to solve the problems connected to the measurement of social categorization (for further details, see Klauer & Wegener, 1998).
We therefore decided to adapt the MPT model for the "Who said what?" paradigm to the purpose of measuring different forms of EC-related memory.
To this aim, we designed a two-step memory task that provides the response data required for the estimation of the adapted MPT model.
The memory task is administered after the learning phase and combines a CS recognition measure with a contingent measure of CS-US pairing memory.
On each trial of the memory task, participants are presented with a CS (from the learning phase) or with an unpaired distractor (not included in the learning phase) and are asked to indicate whether or not the presented stimulus was included in the previous learning phase.
Whenever participants decide that a given stimulus was included in the learning phase, they are presented with a set of positive and negative USs and are asked to select the previously paired stimulus.

As illustrated in Figure 1,

<!-- As illustrated in Figure 1, the adapted MPT model consists of three model trees and includes five types of parameters ($D$, $C$, $d$, $a$ and $b$) all of which may, in principle, take different values in each of the three model trees -->

<!-- # The present research -->
<!-- - we present three experiments: (1) shows fit to pairing memory data based on standard EC procedure, (2) explores effects of task order (memory first vs. rating first) as important procedural feature for successful applications in future applications, (3) reports an experimental application re-investigating a widely cited finding in EC research (Gast & Rothermund, 2011) -->

# Experiment 1

```{r}
data_list_pilot <- readRDS(file.path(study_folders$wsw1, "data", "data.rds"))
#data_list_pilot$excluded_participants
```

## Methods
Experiment 1 was not pre-registered.
The materials and data are publicly available at: https://osf.io/rqkvy/.
The data collection was conducted online.

### Design and participants
The experiment followed a 2 (US valence: positive vs. negative) within-subjects design.
Participants were recruited through Prolific and received monetary compensation for their participation.
The sampling pool was restricted to English speakers with at least 100 previous submissions and an approval rating of at least 90%.
Prolific users who had participated in our previous evaluative conditioning studies were excluded from the sampling pool.
We recruited 50 participants (xxx$\%$ female; $M_{age} = xxx$; $SD_{age} = xxx$).
The number of recruited participants was determined *ad hoc* and was not based on a power analysis.
We excluded one participant who declared that they did not pay attention to the images presented throughout the experiment.
We excluded another participant who gave the same response on all trials of the evaluation task.
We interpreted this response behavior as an indicator of non-compliance and decided to exclude the participant from all analyses.
Taken together, this resulted in a final sample size of 48 participants.

### Materials
The experiment was programmed with *lab.js* [@henninger_labjs_2022] and exported to an HTTPS-protected website with JATOS [@lange_just_2015].

The CS pool comprised 48 colored images of human faces with neutral expressions (24 female faces, 24 male faces).
The images were taken from xxx.

As USs, we used 24 colored images animals (e.g., a cockroach), scenes (e.g., a rainbow) and objects (e.g., a knife).
The images were taken from the Open Affective Standardized Image Set (OASIS; Kurdi et al., 2017). Based on OASIS ratings (on a 7-point Likert scale), we selected 12 positive images ($M_{valence} = 5.88$; $SD_{valence}$ = 0.24; $M_{arousal} = 4.10$; $SD_{arousal} = 0.50$) and 12 negative images ($M_{valence} = 2.05$; $SD_{valence} = 0.32$; $M_{arousal} = 4.27$; $SD_{arousal} = 0.52$).
The positive and negative images differed with regard to valence, Welch’s $t(20.23) = 33.36, p < .001$, but not with regard to arousal, Welch’s $t(21.96) = 0.82, p = .419$. 

For each participant, 12 randomly selected face images (50% female) served as positively paired CSs, 12 randomly selected face images (50% female) served as negatively paired CSs, and the remaining 24 face (50% female) images served as distractor stimuli in the test phase.

For each participant, all 24 images served as USs during the learning phase. 
The 24 images were randomly assigned to the 24 CSs.
Twelve CSs (50% female) were paired with positive USs and twelve CSs (50% female) were paired with negative USs.

### Measures and procedures
All verbal materials were presented in English.
After providing informed consent and being asked to focus on the study, participants were thanked for their participation and asked to pay close attention to all instructions and tasks.
Subsequently, participants were presented with the instructions for the learning phase.

#### Learning phase
First, participants read the following instruction slide: "In the first part of the experiment you will be presented with image pairs. Each image pair consists of a face and another picture. Your task is simply to pay close attention to each image pair. Press the spacebar to continue with the instructions."
Subsequently, participants read the following instruction slide: "Next you will be presented with the image pairs. Please pay close attention to each image pair. This part of the experiment will take about 2.5 minutes. Press the spacebar to start the task."

After pressing the spacebar, participants worked through the learning task, consisting of 72 trials.
The trials were separated by blank screens presented for 1,000 ms.
On each trial, a CS (face image, center-left position) was presented together with a US (positive or negative image, center-right position).
CS and US appeared at the same time and remained on screen for 1,000 ms.
For each participant, trial order was randomized in sets of 24 trials.
In each set (three in total), each of the 24 CS-US pairs was presented once.

After the last trial of the learning task, participants read the following instruction slide: "The first part of the experiment is now finished. You have now seen all image pairs and may continue with the second part of the experiment. Press the spacebar to continue."

#### Test phase
After the learning phase, participants entered the test phase. 
In the test phase, participants performed two measurement tasks: a memory task followed by an evaluation task. 
The order of the two tasks (memory task first) was the same for all participants.

##### Memory task
First, participants were presented with the following instruction slide: "In the second part of the experiment you will be presented with individual faces. Some of these faces were part of the previously presented image pairs. Other faces will be new: they were not part of the image pairs you saw in the previous task. For each face, please indicate whether it is 'old' (i.e., part of the previously presented image pairs) or 'new' (i.e., not part of the previously presented image pairs). Press the spacebar to continue."

Next, participants read the following instruction slide: "Whenever you classify a face as 'old', you will be asked to perform a second task. In this second task, you will be presented with eight pictures. Your task will be to select the picture with which the face was previously paired with. If you can remember the paired picture, click on it and then click on the 'continue' button at the bottom of the screen. If you cannot remember the previously paired picture, try to guess the correct one. Again, click on the image corresponding to your guess and then on the "continue" button at the bottom of the screen. Ready? Then press the spacebar to continue."

After pressing the spacebar, participants performed the memory task, consisting of 48 trials.
In the memory task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit.
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.
Each trial began with the CS recognition task: participants were asked whether the presented face image had been part of the image pairs presented in the learning phase.
To indicate their response, participants were presented with two buttons labeled "Yes (old)" and "No (new)".
If participants responded "No (new)", they proceeded to the next recognition memory trial.
If participants responded "Yes (old)", they proceeded to the US memory task (in which they were asked to identify the previously paired US).
In this task, the CS was presented together with eight USs from the learning phase.
The eight US images were displayed in two rows of four images (with random assignment of images to positions).
For CSs that were correctly recognized as "old", the correct US was presented together with seven randomly selected distractors (3 images of the same valence as the correct US; 4 images of the opposite valence). 
For distractor stimuli incorrectly classified as "old", eight randomly selected images (4 $\times$ positive, 4 $\times$ negative) were presented.

After the last trial of the memory task, participants read the following instruction slide: "The second part of the experiment is now finished. Press the spacebar to continue with the third part."

##### Evaluation task
First, participants read the following instruction slide: "In the third part of the experiment, you will again be presented with the face images. This time you will be asked to indicate your impression of the persons depicted in the images. To indicate your impression of a given person, you will be presented with an 11-point scale ranging from very negative (left) to very positive (right). Please click on the scale point that best represents your impression of the person depicted in a given image. Then click on the 'continue' button to proceed to the next face. Ready? Then press the spacebar to continue."

After pressing the spacebar, particpants worked through the evaluation task, consisting of 48 trials.
In the evaluation task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit. 
Participants rated how positive or negative they found each face on a 11-point rating scale ranging from "very negative" (-5) to "very positive" (+5). (Note that participants responses were stored as numerical values ranging from 1 to 11.)
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

After the last trial of the evaluation task, participants read the following instruction slide: "The third and final part of the experiment is now finished. Now we have a few short questions about your experience performing the study. Press the spacebar to continue."

#### Control measures and debriefing
After pressing the spacebar, participants were asked whether they paid attention to the images presented throughout the experiment.
The response options ("Yes" vs. "No") were presented as two buttons.
Subsequently, participants were asked whether they took the requested responses seriously.
The response options ("I have taken the requested responses seriously." vs. "I have just clicked through, please discard my data.") were again presented as two buttons.
Subsequently, participants were given the chance to comment on the study.
Finally, participants were presented with the following text (debriefing them about the purpose of the study): "The experiment is now over. Thank you very much for your participation. In this experiment, we wanted to see whether the valence of the paired scene (positive vs. negative) has an influence on your evaluation of the faces. In addition, we were also interested in your memory for the image pairs. If you have any question or comment, or if you would like to receive additional information on the present study, please do not hesitate to contact the person in charge of this research at the following e-mail address: [*e-mail address*]. Press the spacebar to be redirected to Prolific."
After pressing the spacebar, participants were redirected to Prolific and reimbursed for their participation.

### Data processing and statistical analysis
The evaluative ratings were analyzed (without further processing) using the R package *stats*.
For the MPT model analyses, the responses from the two memory tasks were first recoded into a joint memory index according to the following scheme:

- If a positively (negatively) paired CS was incorrectly classified as "new", the response was recoded as "PosUSnew" ("NegUSnew"). 

- If a positively (negatively) paired CS was correctly classified as "old" and the correct positive (negative) US was selected, the responses were recoded as "PosUSposcor" ("NegUSnegcor").

- If a positively (negatively) paired CS was correctly classified as "old" and an incorrect positive (negative) trait was selected, the responses were recoded as "PosUSposincor" ("NegUSnegincor").

- If a positively (negatively) paired CS was correctly classified as "old" and a negative (positive) trait was selected, the responses were recoded as "PosUSnegincor" ("NegUSposincor")

- If an (unpaired) new stimulus was correctly classified as "new", the response was recoded as "Newnew".

- If an (unpaired) new stimulus was incorrectly classified as "old" and a positive (negative) trait was selected, the responses will be recoded as "Newposincor" ("Newnegincor").

```{r}
baseline_restrictions <- list(
  G = .25
)

hypothesis_restrictions_8b <- list(
  baseline = list()
  , D_eq0  = list(D = 0)
  , Cpos_eq0  = list(C_positive = 0)
  , Cneg_eq0  = list(C_negative = 0)
  , dpos_eq0  = list(d_positive = 0)
  , dneg_eq0  = list(d_negative = 0)
  , a_eq.5 = list(a = .5)
  , b_eq.5 = list(b = .5)
  , C_eq  = list(C = c("C_positive", "C_negative"))
  , d_eq  = list(d = c("d_positive", "d_negative"))
  , fiveparam = list(C = c("C_positive", "C_negative"),d = c("d_positive", "d_negative"))
)

models_8b <- lapply(
  hypothesis_restrictions_8b
  , FUN = function(x) {
    fit_mpt(
      model = file.path(project_root, "model-equations", "wsw-8b.eqn")
      , data = subset(data_list_pilot$mpt_data_hierarchical,sid!=43)
      , restrictions = c(baseline_restrictions, x)
    )
  }
)

exp1 <- apa_print(compare(models_8b))
#models_8b$baseline$goodness_of_fit
#models_8b$baseline
```

Next, the frequency distribution of the joint memory index (aggregated across participants) was analyzed with the R package *HMMTreeC*.
We implemented different versions of the MPT model 
In all versions, the $g$ parameter was set to $.25$ (corresponding to the inverse of the number of response options per US valence in the US memory task).
To allow for potential memory differences across US valence conditions, we first fitted a model variant that included separate $C$ and $d$ parameters for positively vs. negative paired CSs (next to joint $D$, $b$ and $a$ parameters).
We then compared this 7-parameter model with a more restrictive 5-parameter model in which $C$ and $d$ parameters were set equal across US valence conditions.
There was no significant difference in model fit between the two models, `r exp1$full_result$fiveparam`.
We therefore decided to report the simpler 5-parameter model.

Finally, we explored the relationships between MPT model parameters and individual EC effects (calculated as the difference between the mean evaluative rating for positively paired CSs and the mean evaluative rating for negatively paired CSs).
Using the probabilistic programming language *Stan* [@carpenter_stan_2017], 
we implemented a joint model of (1) participant-wise frequency distributions of the joint memory index and (2) individual EC effects.
The first part of the joint model accounts for the memory data and is a hierarchical latent-trait version  [cf., @klauer_hierarchical_2010] of the 5-parameter MPT model described above. The second part of the model is a multiple linear regression model in which individual EC effects serve as the criterion variable and individual parameter estimates from the first part of the model serve as predictors.
<!-- Compared with previous studies that used a two-step procedure where -->
To the best of our knowledge, we are the first to use such a joint modeling approach in EC research.
In previous studies, similar analyses were carried out using a two-step procedure in which the estimation of a latent-trait MPT model is followed by a separate linear regression analysis [e.g., @kukken__2020] in which point estimates (from the latent-trait model) are used to predict individual EC effects.
Such an approach is potentially problematic, because
using such point estimates (as predictor values) ignores the uncertainty that is associated with the estimation of each parameter,
thereby violating the assumption of linear regression analysis that predictor values are free of measurement error [cf., @klauer_unbiased_1998].
Using point estimates to predict EC effects may therefore lead to biased conclusions about the statistical relationships between MPT parameters and individual EC\ effects.
To avoid such biases, we decided to use a joint-modeling approach which allows to account for uncertainty in MPT parameter estimates when quantifying the relationships between these MPT parameters and individual EC effects.


```{r}
exp1_ratings <- data_list_pilot$rating
exp1_ratings_sd <- aggregate(exp1_ratings,evaluative_rating ~ sid, FUN = sd) # 1 x sd = 0 --> sid = 43
exp1_ratings <- subset(exp1_ratings,sid != 43)
exp1_ratings_wide <- subset(data_list_pilot$rating_wide,sid != 43)

data_list_pilot$rating <- subset(data_list_pilot$rating,sid != 43)
data_list_pilot$rating_wide <- subset(data_list_pilot$rating_wide,sid != 43)
```

```{r}
rrr <- data_list_pilot$memory
#table(rrr$sid)
rrr$reco_resp_num <- ifelse(rrr$reco_resp=="old",1,0)
rrr2 <- aggregate(rrr,reco_resp_num ~ sid, FUN = sd) # 0 x sd = 0
```

```{r}
#table(rrr$sid,rrr$source_mem) # 0 x sd = 0
```

## Results

### Evaluative ratings

```{r}
exp1_ratings$cs_sex <- ifelse(exp1_ratings$cs %in% c("f1","f2","f3","f4","f5","f6","f7","f8","f9","f10","f11","f12","f13","f14","f15","f16","f17","f18","f19","f20","f21","f22","f23","f24"),"female","male")
exp1_ratings$us_valence2 <- "no pairing"
exp1_ratings$us_valence2 <- ifelse(is.na(exp1_ratings$us_valence)==TRUE, "no pairing",as.character(exp1_ratings$us_valence))

exp1_ratings_agg <- aggregate(exp1_ratings,FUN=mean,evaluative_rating~sid*us_valence2)
library(reshape2)
exp1_ratings_agg2 <- dcast(exp1_ratings_agg,value.var="evaluative_rating",sid~us_valence2)

exp1_posneg <- apa_print(t.test(exp1_ratings_agg2$positive,exp1_ratings_agg2$negative,paired=TRUE,alternative="greater"))

mean_pos <- round(mean(exp1_ratings_agg2$positive),2)
sd_pos <- round(sd(exp1_ratings_agg2$positive),2)

mean_neg <- round(mean(exp1_ratings_agg2$negative),2)
sd_neg <- round(sd(exp1_ratings_agg2$negative),2)

mean_nop<- round(mean(exp1_ratings_agg2$`no pairing`),2)
sd_nop <- round(sd(exp1_ratings_agg2$`no pairing`),2)

exp1_posdist <- apa_print(t.test(exp1_ratings_agg2$positive,exp1_ratings_agg2$`no pairing`,paired=TRUE,alternative="greater"))
exp1_negdist <- apa_print(t.test(exp1_ratings_agg2$negative,exp1_ratings_agg2$`no pairing`,paired=TRUE,alternative="less"))

#effectsize::cohens_d(t.test(exp1_ratings_agg2$positive,exp1_ratings_agg2$negative,paired=TRUE,alternative="greater"))
```

The mean rating for positively paired CSs was higher than the mean rating for negatively paired CSs ($M_{positive}=$ `r mean_pos`, $SD_{positive}=$ `r sd_pos`; $M_{negative}=$ `r mean_neg`, $SD_{negative}=$ `r sd_neg`).
The mean difference (i.e., the mean EC effect) was small in size, $d_{Cohen}=.26$, and reached significance only in a one-tailed test, `r exp1_posneg$full_result`.

### MPT model analyses

```{r}
baseline_restrictions <- list(
  G = .25
)

hypothesis_restrictions_8b <- list(
  baseline = list()
  , D_eq0  = list(D = 0)
  , Cpos_eq0  = list(C_positive = 0)
  , Cneg_eq0  = list(C_negative = 0)
  , dpos_eq0  = list(d_positive = 0)
  , dneg_eq0  = list(d_negative = 0)
  , a_eq.5 = list(a = .5)
  , b_eq.5 = list(b = .5)
  , C_eq  = list(C = c("C_positive", "C_negative"))
  , d_eq  = list(d = c("d_positive", "d_negative"))
)

models_8b <- lapply(
  hypothesis_restrictions_8b
  , FUN = function(x) {
    fit_mpt(
      model = file.path(project_root, "model-equations", "wsw-8b.eqn")
      , data = subset(data_list_pilot$mpt_data_hierarchical,sid!=43)
      , restrictions = c(baseline_restrictions, x)
    )
  }
)
```

```{r}
baseline_restrictions <- list(
  G = .25
)

hypothesis_restrictions <- list(
  baseline = list()
  , D_eq0  = list(D = 0)
  , C_eq0  = list(C = 0)
  , d_eq0  = list(d = 0)
  , a_eq.5 = list(a = .5)
  , b_eq.5 = list(b = .5)
)

models <- lapply(
  hypothesis_restrictions
  , FUN = function(x) {
    fit_mpt(
      model = file.path(study_folders$wsw1, "WSW_pilot_hierarchical.eqn")
      , data = subset(data_list_pilot$mpt_data_hierarchical,sid!=43)
      , restrictions = c(baseline_restrictions, x)
    )
  }
)
apa_wsw6 <- apa_print(models$baseline)
apa_wsw_comps <- apa_print(compare(models))
```

```{r}
apa_wsw8b <- apa_print(models_8b$baseline)
apa_wsw8b_comps <- apa_print(compare(models_8b))
```

The 5-parameter model fit the data well, `r apa_wsw6$statistic$modelfit`.
Parameter estimates and confidence intervals are reported in Table 1.

The $D$ parameter estimate was significantly larger than zero, `r apa_wsw_comps$full_result$D_eq0`, indicating that participants recognized CSs as "old" (and distractor stimuli as "new") in $45.6$% of trials.
By implication, participants did not recognize CSs as "old" (and distractor stimuli as "new") in $54.4$% of trials.

The $C$ parameter estimate was significantly larger than zero, `r apa_wsw_comps$full_result$C_eq0`, indicating that participants had US identity memory for $67.4$% of CSs that had been recognized as "old".
This implies that US identity was absent for $32.6$% of CSs recognized as "old" (hereafter also referred to as "recognized" CSs).

The $d$ parameter estimate was significantly larger than zero, `r apa_wsw_comps$full_result$d_eq0`, indicating that in the subset of "recognized" CSs without US identity memory, participants possessed US valence memory in approx. $19.9$% of trials.
By implication, participants did not possess US valence memory for approx. $80.1$% of "recognized" CSs without US identity memory.

The $b$ parameter estimate was significantly lower than $.5$, `r apa_wsw_comps$full_result$b_eq_5`.
This suggests that, for CSs not recognized as "old" (as distractor stimuli not recognized as "new"), participants had an overall tendency towards selecting the "new" response.
Specifically, the size of the parameter estimate indicates that $14.8$% of CSs not recognized as "old" (and distractor stimuli not recognized as "new") were guessed as "old".
By implication, participants guessed "new" for $85.2$% of CSs not recognized as "old" (and distractor stimuli not recognized as "new").

The $a$ parameter estimate was significantly lower than $.5$, `r apa_wsw_comps$full_result$a_eq_5`.
This suggests that, in the absence of both US identity and US valence memory, participants had an overall tendency towards selecting a negative image in the US memory task.
Specifically, the size of the parameter estimate indicates that participants selected a positive (negative) US for $42.7$% ($57.3$%) of CSs and distractor stimuli guessed as "old" (and "recognized" CSs without US identity and US valence memory).

### Relationships between evaluative ratings and MPT parameters

```{r}
model <- readRDS(file.path(study_folders$wsw1, "model-objects", "treestan.rds"))
```

```{r warning = FALSE, fig.cap = "Experiment 1. Marginal EC effects as predicted by MPT parameters.", fig.width=12, fig.height = 9, out.width = "100%"}
par(mfrow = c(2, 3))
plot_regression(model, pars = c("D", "C", "d", "a", "b"))

# bayes_factors(model)
```


## Discussion

# Experiment 2

```{r}
data_list_exp2 <- readRDS(file.path(study_folders$wsw2_main, "data", "data.rds"))
#data_list_exp2$excluded_participants

n_final <- sum(length(unique(data_list_exp2$rating$sid)))
n_conditions <- split(data_list_exp2$rating, data_list_exp2$rating$task_order) |>
  lapply(function(x) {paste0("$n = ", length(unique(x$sid)), "$")})
n_conditions <- setNames(n_conditions, papaja::sanitize_terms(tolower(names(n_conditions))))

socio = read.csv(file.path(project_root, "/Paper/data/sociodemo.csv"))
socio = socio %>% filter(Status != "RETURNED")
```

## Method
Experiment 2 was pre-registered on the OSF.
The pre-registration, materials and data are publicly available at: https://osf.io/rqkvy/.
The data collection was conducted online.

### Design and participants
The experiment followed a 2 (US valence: positive vs. negative) $\times$ 2 (task order: evaluation task first vs. memory task first) mixed design.
The first factor varied within participants and the second factor varied between participants.

Participants were recruited through Prolific and received monetary compensation for their participation.
As in the first experiment, the sampling pool was restricted to English speakers who had not participated in our previous EC studies with at least 100 previous submissions and an approval rating of at least 90%.
We recruited 172 participants (50$\%$ female; $M_{age} =$ `r mean(as.numeric(socio$Age))`; $SD_{age} =$ `r sd(as.numeric(socio$Age))`).
Due an unknown technical error, the data from one participant was unavailable.
As preregistered, we excluded all participants who declared that they did not pay attention or that they did not take their responses seriously ($N=5$).
Though not preregistered, we decided to exclude four additional participants who gave the same response on all trials of the evaluation task.
Taken together, this resulted in a final sample size of `r n_final` participants (`r n_conditions$rating_first` in the evaluation task first condition and `r n_conditions$memory_first` in the memory task first condition).

The number of recruited participants was based on a power analysis for an EC effect as small as Cohen's $d = 0.2$. 
The power analysis was conducted with the R package *pwr* (version 1.3-0; Champely, 2020).
The test of the EC effect was implemented as a one-tailed paired-samples *t*-test with $\alpha=.05$ (IV: US valence; DV: evaluative ratings).
We found that 156 participants were required to achieve a statistical power of $1-\beta = .8$ to detect a significant EC effect.
We also found that a sample size of $N = 156$ provided statistical power of $1-\beta = .8$ to detect correlations $r\mathrm{s} \geq |.22|$ (e.g., between parameter estimates and evaluative ratings).
To avoid a final sample size smaller than $N = 156$ after applying the pre-registered exclusion criteria, we chose to recruit 172 participants (the required sample size increased by 10%).

### Materials
The experiment was programmed with *lab.js* [@henninger_labjs_2022] and exported to an HTTPS-protected website with JATOS [@lange_just_2015].

The CS pool comprised 54 nonwords made up of five to seven letters (e.g., *botsy*, *ikzunt*, *ampfong*).
The nonwords were taken from a previous EC study (Stahl & Bading, 2020).
For each participant, 12 randomly selected nonwords served as positively paired CSs, 12 randomly selected nonwords served as negatively paired CSs, and another 24 randomly selected nonwords served as distractor stimuli in the test phase.

As USs, we used the same 24 images as in Experiment 1.
For each participant, the 24 images were randomly assigned to the 24 CSs.

### Measures and procedures
We used JATOS [@lange_just_2015] to run the study online.
All verbal materials were presented in English.
The task instructions were similar to those provided in the first experiment (for details, see pre-registration).

#### Learning phase
Participants were told that they would see pairs of nonwords and images and were instructed to pay close attention to each pair.
The learning phase consisted of 72 trials.
The trials were separated by blank screens presented for 1,000 ms.
On each trial, a nonword (CS, center-left position) was presented together with a valenced image (US, center-right position).
CS and US appeared at the same time and remained on screen for 1,000 ms.
For each participant, trial order was randomized in sets of 24 trials.
In each set (three in total), each of the 24 CS-US pairs was presented once.

#### Test phase
After the learning phase, participants entered the test phase. 
In the test phase, participants performed two measurement tasks: an evaluation task and a memory task. 
Participants were randomly assigned to one of two task order conditions. 
In the "evaluation task first" condition, participants performed the evaluation task and then the memory task.
The task order was reversed in the "memory task first" condition.
The wording of the task instructions differed slightly between task order conditions (for details, see pre-registration).

##### Evaluation task
In the evaluation task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit. 
Participants rated how positive or negative they found each nonword on a 8-point Likert scale ranging from "very negative" (1) to "very positive" (8).
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

##### Memory task
In the memory task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit.
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

Each trial began with with the recognition memory task: participants were asked whether the nonword had been part of the nonword-image pairs presented in the learning phase.
To indicate their response, participants were presented with two buttons labeled "Yes (old)" and "No (new)".
If participants responded "No (new)", they proceeded to the next recognition memory trial.
If participants responded "Yes (old)", they proceeded to the associative memory task (in which they were asked to identify the previously paired US).
In this task, the nonword was presented together with eight images (all of which had been shown as USs during the learning phase).
The eight images were displayed in two rows of four images (with random assignment of images to positions).
For CSs that were correctly recognized as "old", the correct US was presented together with seven randomly selected distractors (3 images of the same valence as the correct US; 4 images of the opposite valence). 
For distractor stimuli incorrectly classified as "old", eight randomly selected images (4 $\times$ positive, 4 $\times$ negative) were presented.
Participants were instructed to guess the correct option if they could not remember the previously paired US.

#### Control measures and debriefing
After the test phase, participants were asked whether they paid attention to the nonwords and images presented throughout the experiment.
The response options ("Yes" vs. "No") were presented as two buttons.
Subsequently, participants were asked whether they took the requested responses seriously.
The response options ("I have taken the requested responses seriously" vs. "I have just clicked through, please discard my data") were again presented as two buttons.
Subsequently, participants were given the chance to comment on the study.
Finally, participants were debriefed about the purpose of the study and then redirected to Prolific.

### Data processing and statistical analysis

```{r}
exp2_ratings <- data_list_exp2$rating
exp2_ratings_sd <- aggregate(exp2_ratings,evaluative_rating ~ sid, FUN = sd) # 4 x sd = 0 --> sid = 49, 104, 138, 139
exp2_ratings_sd <- subset(exp2_ratings_sd, evaluative_rating > 0)
exp2_ratings <- subset(exp2_ratings,sid %in% exp2_ratings_sd$sid)
exp2_ratings_wide <- subset(data_list_exp2$rating_wide,sid %in% exp2_ratings_sd$sid)
```

```{r}
rrr <- data_list_exp2$memory
#table(rrr$sid)
rrr$reco_resp_num <- ifelse(rrr$reco_resp=="old",1,0)
rrr2 <- aggregate(rrr,reco_resp_num ~ sid, FUN = sd) # 1 x sd = 141 (always 'new')
#table(rrr$sid,rrr$source_mem) # 0 x sd = 0

```

## Results
### Evaluative ratings
```{r}
exp2_ratings <- subset(exp2_ratings,sid!=141)
exp2_ratings_wide <- subset(exp2_ratings_wide,sid!=141)

exp2_ratings_anova <- aov_ez(data = subset(exp2_ratings, us_valence %in% c("positive","negative")), id = "sid", dv = "evaluative_rating", within = c("us_valence"),between=c("task_order"))

exp2_ec <- t.test(exp2_ratings_wide$positive,exp2_ratings_wide$negative,paired=TRUE,alternative="greater")

effectsize::cohens_d(exp2_ec)
```

### MPT model

```{r}
baseline_restrictions <- list(
  G_x1 = .25
  , G_x2 = .25
  , C_x1 = "Cpos_x1=Cneg_x1"
  , C_x2 = "Cpos_x2=Cneg_x2"
  , d_x1 = "dpos_x1=dneg_x1"
  , d_x2 = "dpos_x2=dneg_x2"
)

hypothesis_restrictions <- list(
  baseline = list()
  , D_equal = list(D = c("D_x1", "D_x2"))
  , C_equal = list(C = c("C_x1", "C_x2"))
  , d_equal = list(d = c("d_x1", "d_x2"))
  # , Cx1_equal = list(C = c("Cpos_x1", "Cneg_x1"))
  # , Cx2_equal = list(C = c("Cpos_x2", "Cneg_x2"))
  # , dx1_equal = list(d = c("dpos_x1", "dneg_x1"))
  # , dx2_equal = list(d = c("dpos_x2", "dneg_x2"))
  , a_equal = list(a = c("a_x1", "a_x2"))
  , b_equal = list(b = c("b_x1", "b_x2"))
  , Deval0   = list(D_x1 = 0)
  , Dmem0   = list(D_x2 = 0)
  , Ceval0   = list(C_x1 = 0)
  , Cmem0   = list(C_x2 = 0)
  , deval0   = list(d_x1 = 0)
  , dmem0   = list(d_x2 = 0)
  , beval5   = list(b_x1 = 0.5)
  , bmem5   = list(b_x2 = 0.5)
  , aeval5   = list(a_x1 = 0.5)
  , amem5   = list(a_x2 = 0.5)
)

models <- lapply(
  hypothesis_restrictions
  , FUN = function(x) {
    fit_mpt(
      data = data_list_exp2$mpt_data
      , model = file.path(study_folders$wsw2_main, "WSW_8b_exp2.eqn")
      , restrictions = c(baseline_restrictions, x)
    )
  }
)
wesanderson::wes_palette("Zissou1", n = 3, type = "c") |> 
  palette()

# plot(
#   models$baseline
#   , factors = list("Task order" = c("Rating first" = "_x1", "Memory first" = "_x2"))
#   , parameters = c(
#     D   = expression(italic(D))
#     , Cpos = expression(italic(Cpos))
#     , Cneg = expression(italic(Cneg))
#     , dpos = expression(italic(dpos))
#     , dneg = expression(italic(dneg))
#     , a = expression(italic(a))
#     , b = expression(italic(b))
#   )
# )

```

```{r}
model_comparisons <- compare(models)
#model_comparisons
```

```{r}
baseline_restrictions <- list(
  G_x1 = .25
  , G_x2 = .25
)

hypothesis_restrictions <- list(
  baseline = list()
  , D_equal = list(D = c("D_x1", "D_x2"))
  , C_equal = list(C = c("C_x1", "C_x2"))
  , d_equal = list(d = c("d_x1", "d_x2"))
  , a_equal = list(a = c("a_x1", "a_x2"))
  , b_equal = list(b = c("b_x1", "b_x2"))
  , Dage0   = list(D_x1 = 0)
  , Dval0   = list(D_x2 = 0)
  , Cage0   = list(C_x1 = 0)
  , Cval0   = list(C_x2 = 0)
  , dage0   = list(d_x1 = 0)
  , dval0   = list(d_x2 = 0)
  , bage5   = list(b_x1 = 0.5)
  , bval5   = list(b_x2 = 0.5)
  , aage5   = list(a_x1 = 0.5)
  , aval5   = list(a_x2 = 0.5)
)

models <- lapply(
  hypothesis_restrictions
  , FUN = function(x) {
    fit_mpt(
      data = data_list_exp2$mpt_data
      , model = file.path(study_folders$wsw2_main, "WSW_exp2.eqn")
      , restrictions = c(baseline_restrictions, x)
    )
  }
)
wesanderson::wes_palette("Zissou1", n = 3, type = "c") |> 
  palette()

# plot(
#   models$baseline
#   , factors = list("Task order" = c("Rating first" = "_x1", "Memory first" = "_x2"))
#   , parameters = c(
#     D   = expression(italic(D))
#     , C = expression(italic(C))
#     , d = expression(italic(d))
#     , a = expression(italic(a))
#     , b = expression(italic(b))
#   )
# )

```

```{r}
model_comparisons <- compare(models)
#model_comparisons
```

### Relationships between evaluative ratings and MPT parameters

```{r}
model <- readRDS(file.path(study_folders$wsw2_main, "model-objects", "treestan.rds"))
```

```{r warning = FALSE, fig.cap = "Experiment 2. Marginal EC effects as predicted by MPT parameters.", fig.width=12, fig.height = 9, out.width = "100%"}
par(mfrow = c(2, 3))
plot_regression(
  model, pars = c("D", "C", "d", "a", "b")
  # , pt.bg = data_list_exp2$mpt_data_hierarchical$task_order
)

# bayes_factors(model)
```


## Discussion

# Experiment 3

```{r}
# from MB:
data_list <- readRDS(file.path(study_folders$wsw3_main, "data", "data.rds"))
data_list$excluded_participants                       # Participant IDs
#lapply(data_list$excluded_participants, FUN = length) # How many?

# data_list_pilot <- readRDS(file.path(study_folders$wsw3_p2, "data", "data.rds"))

# data_list_joint <- readRDS(file.path(study_folders$wsw3_joint, "data", "data.rds"))

# data_list_pilot$rating$study <- "pilot"
# data_list_pilot$rating_wide$study <- "pilot"

data_list$rating$study <- "main"
data_list$rating_wide$study <- "main"

```

## Method
The experiment was pre-registered on the OSF.
The pre-registration, materials and data are publicly available at: https://osf.io/rqkvy/.
The data collection was conducted online.

### Participants

```{r}

demographics <- read.csv(file.path(study_folders$wsw3_main, "data-raw", "demographics.csv")) |>
  subset(Status == "APPROVED") |>
  within({
    Age <- as.integer(Age)
    Sex <- factor(Sex, levels = c("Female", "Male"))
  })

n_sex <- table(demographics$Sex)
prop_sex <- as.list(proportions(n_sex)) |>
  lapply(function(x) {
    paste0("$",apa_num(x * 100), "\\%$")
  })

age <- with(demographics, {
  paste0(
    "$M_\\mathrm{age} = "
    , apa_num(mean(Age))
    , "$, $\\mathit{SD}_\\mathrm{age} = "
    , apa_num(sd(Age))
    , "$"
  )
})

n_conditions <- split(data_list$rating, data_list$rating$task_focus) |>
  lapply(function(x) {paste0("$n = ", length(unique(x$sid)), "$")})
n_conditions <- setNames(n_conditions, papaja::sanitize_terms(tolower(names(n_conditions))))



```

Participants were recruited through Prolific and received monetary compensation for their participation.
We collected a quota sample with approximately equal shares of male and female participants.
The sampling pool was restricted to Prolific users (1) whose first language is English, (2) who live in the USA or in the United Kingdom, and (3) who have at least 20 previous submissions and a Prolific approval rate of at least 90%.
Prolific users who participated in previous pretests and experiments (conducted by the members of the current team of authors) using the same materials were excluded from the sampling pool.

We recruited 142 participants (`r prop_sex$Female` female; `r age`).
As pre-registered,
we excluded `r length(unique(unlist(data_list$excluded_participants)))` participants
who failed at least one control measure (by selecting at least one physical activity, by reporting a lack of attention or by responding with "I have just clicked through, please discard my data").
Taken together, this resulted in a final sample size of `r sum(n_sex)` participants (`r n_conditions$valence` in the valence focus condition and `r n_conditions$age` in the age focus condition).

The number of recruited participants was based on a pre-registered sampling plan (see OSF repository).
We pre-registered a minimum sample size (per task focus condition), a maximum sample size (across task focus conditions), and two stopping criteria (determining the [dis-]continuation of data collection within the pre-registered sample size range).
The minimum sample size ($N=52$ participants per learning task focus condition) was based on a power analyses for the two-way interaction between US valence and learning task focus.
In this power analysis, we targeted a test power of $1-\beta=.9$ and used an $\alpha$-level of $.1$ to implement a one-tailed test of the two-way interaction (reflecting our directional prediction about the interaction pattern).
The effect size of the US valence $\times$ task focus interaction was set to $\eta^2_{p}\approx.079$ (corresponding, approximately, to one half of the effect size obtained in a pilot study).
In a first round of data collection, we recruited participants until the minimum sample size (after applying exclusion criteria) was reached.
Based on the available data, we then fitted the pre-registered MPT model (see below) and calculated Bayes factors comparing a nested version of the model with $d_\textrm{valence-focus}=d_\textrm{age-focus}$ to the baseline model with free $d_\textrm{valence-focus}$ and $d_\textrm{age-focus}$ (for details, see section "Data processing and statistical analyses").
The data collection was discontinued at this point because the first stopping criteria was met: the Bayes factor in favor of the baseline model exceeded 10 (i.e., we found strong evidence for the presence of a learning task focus effect on the $d$ parameter).

### Design
The experiment followed a 2 (US valence: positive vs. negative) $\times$ 2 (US age: young vs. old) $\times$ 2 (task focus: valence focus vs. age focus) mixed design.
The first factor varied within participants and the second factor varied between participants.

### Materials
The experiment was programmed with *lab.js* [@henninger_labjs_2022] and exported to an HTTPS-protected website with JATOS [@lange_just_2015].

The CS pool comprised 48 colored images of middle-aged human faces with neutral expressions (24 female faces, 24 male faces).
The images were taken from the FACES database (Ebner, Riediger, & Lindenberger, 2010) and optimized with an AI image optimization tool.

The US pool comprised 24 adjectives describing human traits, with six adjectives in each US valence $\times$ US age condition.
Based on a pilot study, we selected six adjectives describing traits that are positive and more typical for younger (than for older) people (*energetic*, *flexible*, *lively*, *open-minded*, *optimistic*, *strong* ), six adjectives describing traits that are positive and more typical for older (than for younger) people (*calm*, *dignified*, *nurturing*, *patient*, *realistic*, *wise*), six adjectives describing traits that are negative and more typical for younger (than for older) people (*careless*, *impulsive*, *naive*, *selfish*, *spoilt*, *self-absorbed*), and six adjectives describing traits that are negative and more typical for older (than for younger) people (*demented*, *feeble*, *frail*, *rigid*, *stubborn*, *self-weak*). 

For each participant, 12 randomly selected face images (50% female) served as positively paired CSs (i.e., they were paired with a positive US during the learning phase), 12 randomly selected face images (50% female) served as negatively paired CSs, and the remaining 24 face images served as distractor stimuli in the test phase.

For each participant, all 24 adjectives served as USs during the learning phase. 
The 24 adjectives were randomly assigned to the 24 CSs.
Six CSs (50% female) were paired with positive and "younger" USs, six CSs (50% female) were paired with positive and "older" USs, six CSs (50% female) were paired with negative and "younger" USs, and six CSs (50%) were paired with negative and "older" USs.

### Measures and procedures
All verbal materials were presented in English.
After providing informed consent and being asked to focus on the study,
participants were thanked for their participation and asked to carefully read all instructions and perform the tasks.
Subsequently, participants were presented with the instructions for the learning phase.

#### Learning phase
All participants read the following instructions:
"In the first part of the experiment you will be presented with photographs of faces (called 'faces' below) shown together with adjectives that denote human traits (called 'traits' below).
Each face will be paired with a single trait.
For each face-trait pair, the trait will be presented first. 
After a brief delay, the face will appear underneath.
Please pay close attention to all traits and faces.
Each face-trait pair will be presented for a limited time.
Your task will be to form an impression of each face-trait pair."

Next, depending on the learning task focus condition (valence focus or age focus), participants read:
"Specifically, you will have to indicate whether you think each face-trait pair is rather 'negative' ['typically old'] or rather 'positive' ['typically young'].
Press the spacebar to continue with the instructions."

The ordering of the two category labels in the presented sentence was randomly determined for each participant anew (valence focus condition: "positive first" vs. "negative first"; age focus condition: "typically old first" vs. "typically young first").
Note that the key assignment in the learning phase matched the ordering of the category labels in the instructions (e.g., when the "positive" label was mentioned first [i.e., further to the left in the sentence], the "positive" response was assigned to the left-hand key [A]).

The instructions continued as follows: 
"Next, you will be presented with the face-trait pairs.
Again, your task is to form an impression of each face-trait pair. You need to carefully look at both the faces and traits being presented.
Please indicate whether you think each pair is rather 'negative' ['typically old'] or rather 'positive' ['typically old'].
If you think the pair is rather 'negative' ['typically old'], press 'A' on your keyboard.
If you think the pair is rather 'positive' [typically young'], press 'L' on your keyboard. 
Although the presentation time is limited, 
please try your best to provide a response on each face-trait pair.
This part of the experiment will take about 8 minutes.
When you are ready, press the spacebar to start the task.
(This may take a few seconds.)"

Subsequently, participants worked through the learning task, consisting of 72 trials.
Each trial started with a US presented alone in green lower-case letters against a black background at the top of the screen. After 1,000 ms, the CS appeared right below the US. 
The CS-US pair remained on screen for 4,000 ms.
Participants had to respond within 3,000 ms after the onset of the US (by pressing the "A" or "L" key).
If participants responded in time, three hyphens appeared below the CS-US pair right after participants had entered a response (to indicate that the response was recorded).
If participants did not respond in time (i.e., within the 3,000 ms), the text "no response" appeared in red below the CS-US pair for 1,000 ms.
The CS-US pair then disappeared and the message "No response. Press the spacebar to continue." was displayed (in red) at the bottom of the screen (without time limit).
The trials were separated by empty screens presented for 2500 ms.
For each participant, trial order was randomized in sets of 24 trials.
In each set (three in total), each of the 24 CS-US pairs was presented once.

After the learning phase, the following text was displayed: "The first part of the experiment is now finished! You have now seen all face-trait pairs and may continue with the second part of the experiment. 
Press the spacebar to continue."

#### Test phase
After the learning phase, participants entered the test phase. 
In the test phase, participants performed two measurement tasks: an evaluation task followed by a memory task. 
The order of the two tasks (evaluation task first) was the same for all participants.
The task instructions can be found in the pre-registration (see OSF repository).

##### Evaluation task
In the evaluation task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit. 
Participants rated how positive or negative they found each face on a 8-point Likert scale ranging from "very negative" (1) to "very positive" (8).
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

##### Memory task
In the memory task, the 24 CSs and the 24 distractor stimuli were displayed individually without time limit.
The 48 trials were separated by blank screens presented for 500 ms.
Trial order was randomized for each participant anew.

Each trial began with with the recognition memory task: participants were asked whether the face had been shown as part of the face-image pairs presented in the learning phase.
To indicate their response, participants were presented with two buttons labeled "Shown" and "Not shown".
(Note that we used these response labels [instead of "old" and "new"] to avoid confusion with the response labels in the age focus task.)
If participants responded "Not shown", they proceeded to the next recognition memory trial.
If participants responded "Shown", they proceeded to the associative memory task (in which they were asked to identify the previously paired US).
In this task, the face image was presented together with 16 adjectives (four adjectives per US valence $\times$ US age condition).
Note that all presented adjectives had been shown as USs during the learning phase.
The 16 adjectives were displayed as buttons organized in three rows (with six buttons in the upper two rows and four buttons in the bottom row).
For truly "shown" faces, the correct US was presented on a randomly selected button, while the remaining buttons were filled with 15 randomly selected USs that have been paired with other CSs.
For new faces (erroneously classified as "shown"), the buttons will be filled with 16 randomly selected USs. 
After clicking on one of the 16 USs, participants were presented with a blank screen (500 ms) followed by the next trial (i.e., they were presented with a screen showing another face, the recognition task question, and the two response options).

#### Control measures and debriefing
After the test phase, participants were presented with a total of three control measures (all of which were used were as exclusion criteria).
First, participants were presented with a screen showing four long sentences in a single paragraph.
The first three sentences referred to attitude research and through length and writing style was meant to discourage participants from reading the whole text. 
In the very last sentence of the text, participants were instructed to ignore the upcoming question about their exercise habits (in order to demonstrate that they had read the entire passage). 
On the next screen, participants were presented with a list of seven physical activities and were asked to indicate which of these activities they perform regularly (by ticking a small box next to the respective activity). 
After having ticked all relevant boxes (or none at all), participants proceeded to the next screen by clicking on the "Continue" button displayed at the bottom of the screen.
Subsequently, participants were asked whether they paid attention to the nonwords and images presented throughout the experiment.
The response options ("Yes" vs. "No") were presented as two buttons.
Next, participants were asked whether they took the requested responses seriously [based on @aust_seriousness_2013].
The response options ("I have taken the requested responses seriously" vs. "I have just clicked through, please discard my data") were again presented as two buttons.
Afterwards, participants were given the chance to comment on the study.
Finally, participants were debriefed about the purpose of the study and then redirected to Prolific.

### Data processing and statistical analysis

## Results

```{r}
#model <- readRDS(file.path(study_folders$wsw3_joint, "model-objects", "treestan.rds"))
```

```{r warning = FALSE, fig.cap = "Experiment 3. Marginal EC effects as predicted by MPT parameters.", fig.width=12, fig.height = 9, out.width = "100%"}
par(mfrow = c(2, 3))
plot_regression(model, pars = c("D", "C", "d", "a", "b"))

# bayes_factors(model)
```

## Discussion 

# General discussion

# References

::: {#refs custom-style="Bibliography"}
:::


# (APPENDIX) Appendix {-}

```{r child = file.path(project_root, "Paper/appendix.rmd"), eval = TRUE}
```



